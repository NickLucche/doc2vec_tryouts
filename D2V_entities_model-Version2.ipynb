{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2: Model trained on text entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA imports\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec imports\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import gensim\n",
    "\n",
    "import json # to open our data file\n",
    "DATA_FILENAME = \"trend_analisys.json\"\n",
    "# open json file\n",
    "with open(DATA_FILENAME, \"r\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "# we're expecting a list now, since our json file is a json array\n",
    "assert type(json_data) is list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanitize data from duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates:  85\n",
      "New length:  208\n"
     ]
    }
   ],
   "source": [
    "unique_json = json_data\n",
    "counter = 0\n",
    "for i, dictionary in enumerate(unique_json):\n",
    "    try:\n",
    "        index = json_data.index(dictionary, i+1, len(json_data))\n",
    "        #print(\"Found a duplicate with index {0} from index {1}\".format(index, i))\n",
    "        del(unique_json[index])\n",
    "        counter = counter + 1\n",
    "    except ValueError:\n",
    "        None\n",
    "print(\"Number of duplicates: \", counter)\n",
    "print(\"New length: \", len(unique_json))\n",
    "#print(unique_json)\n",
    "json_data = unique_json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2-Training model with flattened_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument(['antico_egitto', 'archeologia', 'capitale_città', 'edificio', 'egitto', 'guerra', 'il_cairo', 'impero_romano', 'legge', 'menfi_egitto', 'politica', 'rito', 'sarcofago', 'storia_antica', 'sud', 'terme_romane', 'turismo'], [10])\n",
      "208\n"
     ]
    }
   ],
   "source": [
    "# we'll use every data at our disposal for training (we don't have that many)\n",
    "n_examples =  len(json_data)\n",
    "TRAIN_DATA_LENGTH = n_examples\n",
    "FLATTENED_ENTITIES_FIELD = 'flattened_entities'\n",
    "\n",
    "# TODO: Randomize selection of examples, don't just take the first ones\n",
    "# build training corpus: take the flattened_entities, preprocess them (tokenize, delete spaces..)\n",
    "# and create the TaggedDocument needed for training\n",
    "train_corpus = [gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(\n",
    "    d[FLATTENED_ENTITIES_FIELD]), [i]) for i, d in enumerate(json_data)]\n",
    "\n",
    "print(train_corpus[10])\n",
    "print(len(json_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 2 models at once - PV-DBOW and PV-DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow,d100,n5,mc2,t4)\n",
      "Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t4)\n",
      "Vocabulary created!\n",
      "Training Doc2Vec(dbow,d100,n5,mc2,t4)\n",
      "CPU times: user 260 ms, sys: 0 ns, total: 260 ms\n",
      "Wall time: 354 ms\n",
      "Training Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t4)\n",
      "CPU times: user 412 ms, sys: 4 ms, total: 416 ms\n",
      "Wall time: 485 ms\n",
      "Models Saved\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "\n",
    "# let's try training two models at once: Paragraph Vector - Distributed Memory (PV-DM), just like CBOW to W2V\n",
    "# and Paragraph Vector - Distributed Bag of Words (PV-DBOW), analogous to W2V Skip-gram\n",
    "epochs = 40\n",
    "vec_size = 100\n",
    "alpha = 0.05  # TODO: TRY HIGHER ALPHA\n",
    "MODEL_NAME = \"Models/d2v_TA_f_entities\"\n",
    "\n",
    "models = [\n",
    "    # PV-DBOW plain (with default alpha)\n",
    "    Doc2Vec(dm=0, vector_size=vec_size, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=epochs, workers=cores),\n",
    "    # PV-DM w/ default averaging; w/ higher alpha\n",
    "    Doc2Vec(dm=1, vector_size = vec_size, window=10, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs = epochs, workers=cores, alpha = alpha, comment='alpha=0.05'),\n",
    "]\n",
    "\n",
    "# build our vocabulary of words (all the unique words encountered inside our corpus, needed for training)\n",
    "for model in models:\n",
    "    print(model)\n",
    "    model.build_vocab(train_corpus)\n",
    "print(\"Vocabulary created!\")\n",
    "\n",
    "# train the models on the given data!\n",
    "counter = 0\n",
    "for model in models:\n",
    "    print(\"Training %s\" % model)\n",
    "    %time model.train(train_corpus, total_examples=len(train_corpus), epochs=model.epochs)\n",
    "    model.save(MODEL_NAME+str(counter)+'.model')\n",
    "    counter = counter + 1\n",
    "print(\"Models Saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization (using PCA and Plotly libs)\n",
    "all credits in the other file d2v_abstract+title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~D4nt3/34.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "tls.set_credentials_file(username='D4nt3', api_key='FdMB4O6qCfciGDOnLvdQ')\n",
    "\n",
    "COMPONENT_ONE = \"principal component 1\"\n",
    "COMPONENT_TWO = \"principal component 2\"\n",
    "\n",
    "# load model to visualize\n",
    "model_number = 1\n",
    "model = Doc2Vec.load(MODEL_NAME+str(model_number)+'.model')\n",
    "\n",
    "docs_vecs = []\n",
    "# docvecs (list of Doc2VecKeyedVectors) \n",
    "# – Vector representations of the documents in the corpus. Each vector has size == vector_size\n",
    "for doc in iter(range(0, len(model.docvecs))):\n",
    "    docs_vecs.append(model.docvecs[doc])\n",
    "    \n",
    "# loading dataset into Pandas DataFrame\n",
    "df = pd.DataFrame.from_records(docs_vecs)    \n",
    "\n",
    "    ## PCA dimensionality-reduction ##\n",
    "# PCA is effected by scale so you need to scale the features in your data before applying PCA. \n",
    "features = [i for i in range(vec_size)]\n",
    "x = df.loc[:, features].values # get features values\n",
    "#print(x)\n",
    "# standardize data\n",
    "x = StandardScaler().fit_transform(x) # scale data (especially in case different measures are used)\n",
    "    \n",
    "# build PCA model in 2D\n",
    "pca = PCA(n_components=2) # The new components are just the two main dimensions of variation.\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents, \n",
    "                           columns = [COMPONENT_ONE, COMPONENT_TWO])\n",
    "    \n",
    "# we'll draw a scatter graph with labels\n",
    "traces = []\n",
    "# let's get the labels\n",
    "titles = [dictionary['title'] for dictionary in json_data]\n",
    "#print(print(titles))\n",
    "finalDf = principalDf\n",
    "\n",
    "# double check to be sure we got labels just right\n",
    "#sample_title = training_set[10]['title']\n",
    "#sample_text = training_set[10]['flattened_entities']\n",
    "#inferred_vector = model.infer_vector(gensim.utils.simple_preprocess(sample_text))\n",
    "#print(gensim.utils.simple_preprocess(sample_text))\n",
    "# pca sample\n",
    "#x = np.array(inferred_vector)\n",
    "#pca = PCA(n_components=2)\n",
    "#pca_result = pca.fit_transform(x)\n",
    "#trace_sample = go.Scatter(\n",
    "#        x = pca_result[0],\n",
    "#        y = pca_result[1],\n",
    "#        mode = 'markers',\n",
    "#            #name = 'blue markers',\n",
    "#        marker = dict(\n",
    "#            size = 7,\n",
    "#            color = 'green',\n",
    "#        ),\n",
    "#        text = str(sample_title)\n",
    "#    )\n",
    "#traces.append(trace_sample)\n",
    "\n",
    "\n",
    "    # each trace will represent a point (squeezed vector from higher dimensions),\n",
    "    # and each point will have the title of the news assigned\n",
    "for i in range(len(finalDf)):\n",
    "    color = 'rgba(0, 0, 110, .8)'\n",
    "    if 'Apple' in titles[i]:\n",
    "            color = 'red'\n",
    "    elif 'Amazon' in titles[i]:\n",
    "        color = 'yellow'\n",
    "    elif 'Facebook' in titles[i] or 'Instagram' in titles[i]:\n",
    "        color = 'green'\n",
    "    elif 'spazio' in json_data[i][FLATTENED_ENTITIES_FIELD]:\n",
    "        color = 'black'\n",
    "    \n",
    "    trace0 = go.Scatter(\n",
    "        x = finalDf.loc[i:i, \"principal component 1\"],\n",
    "        y = finalDf.loc[i:i, \"principal component 2\"],\n",
    "        mode = 'markers',\n",
    "            #name = 'blue markers',\n",
    "        marker = dict(\n",
    "            size = 7,\n",
    "            color = color,\n",
    "        ),\n",
    "        text = str(titles[i])\n",
    "    )\n",
    "    traces.append(trace0)\n",
    "\n",
    "data = traces \n",
    "layout = dict(title = 'PCA Representantion of D2V on Flattened Entities',\n",
    "            hovermode= 'closest',\n",
    "            xaxis= dict(\n",
    "                title= 'first component',\n",
    "                ticklen= 5,\n",
    "                gridwidth= 2,\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                title= 'second component',\n",
    "                ticklen= 5,\n",
    "                gridwidth= 2,\n",
    "            ),\n",
    "            showlegend = False\n",
    "        )\n",
    "# Plot and embed in ipython notebook!\n",
    "    \n",
    "fig = dict(data = data, layout = layout)\n",
    "py.iplot(fig, filename='TA_model_flattened_entities-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means on PCA-reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=600,\n",
       "    n_clusters=5, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try another way for clustering data: K-Mean, an even more popular algorithm,\n",
    "# which I know from the introductory course on AI, so it might be better \n",
    "# to utilize algorithms which I know and can talk about in the presentation\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# build k-means model\n",
    "kmeans = KMeans(n_clusters = 5, max_iter=600, algorithm = 'auto', verbose=0,\n",
    "               init='k-means++', n_init=10) \n",
    "\n",
    "kmeans.fit(principalComponents) # data, as vectors of documents (in 2D)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize centroids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~D4nt3/22.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "tls.set_credentials_file(username='D4nt3', api_key='FdMB4O6qCfciGDOnLvdQ')\n",
    "\n",
    "COMPONENT_ONE = \"principal component 1\"\n",
    "COMPONENT_TWO = \"principal component 2\"\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# each trace will represent a point (squeezed vector from higher dimensions),\n",
    "# and each point will have the title of the news assigned\n",
    "for i in range(len(finalDf)):\n",
    "    # assign a color to each point belonging to a specific cluster\n",
    "    # computing distance from centroid\n",
    "    x = finalDf.loc[i:i, \"principal component 1\"]\n",
    "    y = finalDf.loc[i:i, \"principal component 2\"]\n",
    "    color = 'rgba(0, 0, 180, 0.8)'\n",
    "    min_d = 10000\n",
    "    closest_centroid = []\n",
    "    for centroid in centroids:\n",
    "        dist = np.linalg.norm(centroid-np.array(x, y))\n",
    "        if dist<min_d:\n",
    "            min_d = dist\n",
    "            closest_centroid = centroid\n",
    "    #print(\"Prediction: \",closest_centroid)\n",
    "    if np.array_equal(closest_centroid, centroids[0]):\n",
    "        color = 'blue'\n",
    "    elif np.array_equal(closest_centroid, centroids[1]):\n",
    "        color = 'pink'\n",
    "    elif np.array_equal(closest_centroid, centroids[2]):\n",
    "        color = 'yellow'\n",
    "    elif np.array_equal(closest_centroid, centroids[3]):\n",
    "        color = 'green'\n",
    "    else:\n",
    "        color = 'black'\n",
    "        \n",
    "    trace0 = go.Scatter(\n",
    "        x = x, \n",
    "        y = y,\n",
    "        mode = 'markers',\n",
    "            #name = 'blue markers',\n",
    "        marker = dict(\n",
    "            size = 7,\n",
    "            color = color,\n",
    "        ),\n",
    "        text = str(titles[i])\n",
    "    )\n",
    "    traces.append(trace0)\n",
    "\n",
    "# draw centroids\n",
    "c_trace = go.Scatter(\n",
    "    x = centroids[:, 0],\n",
    "    y = centroids[:, 1],\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        size = 9,\n",
    "        color = 'red',\n",
    "    ),\n",
    "    text = 'centroid'\n",
    ")\n",
    "traces.append(c_trace)\n",
    "\n",
    "data = traces \n",
    "layout = dict(title = 'PCA Representantion of DocVectors',\n",
    "            hovermode= 'closest',\n",
    "            xaxis= dict(\n",
    "                title= 'first component',\n",
    "                ticklen= 5,\n",
    "                gridwidth= 2,\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                title= 'second component',\n",
    "                ticklen= 5,\n",
    "                gridwidth= 2,\n",
    "            ),\n",
    "            showlegend = False\n",
    "        )\n",
    "# Plot and embed in ipython notebook!\n",
    "    \n",
    "fig = dict(data = data, layout = layout)\n",
    "py.iplot(fig, filename='TA_model_entities_kmeans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
