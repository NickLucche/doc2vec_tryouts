{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2: Model trained on text entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA imports\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec imports\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import gensim\n",
    "import json\n",
    "\n",
    "\n",
    "DATA_FILENAME = \"dump_solr.json\"\n",
    "DATA_FILENAME2 = \"trend_analisys.json\"\n",
    "# open json file\n",
    "with open(DATA_FILENAME, \"r\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "# open 'old' json file\n",
    "with open(DATA_FILENAME2, \"r\") as json_file:\n",
    "    json_data_old = json.load(json_file)\n",
    "\n",
    "# we're expecting a dictionary now, since our json file is a json object\n",
    "assert type(json_data) is dict\n",
    "\n",
    "# we're expecting a list this time, for the way it is formatted \n",
    "assert type(json_data_old) is list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in new json:  1377\n",
      "Number of documents in old json:  293\n",
      "New length:  1670\n",
      "[{'fonte_dati': ['trend_analisys'], 'id': 'https://www.ilpost.it/internet/page/3/', 'ta_id': [1, 2, 3], 'title': ['Internet - Pagina 3 di 94 - Il Post'], 'abstract': [\"  Soprattutto in Cina, alcuni impiegati dell'azienda avrebbero preso soldi dai venditori per fornire dati sugli utenti, eliminare recensioni negative e avvantaggiarli nei risultati    Un balletto da fare per strada, vicino a una macchina e su una canzone di Drake è diventato virale nelle ultime settimane, costringendo la polizia a prendere precauzioni in diversi paesi    Per errore un registro interno all'azienda salvava le password senza nasonderle: non sono state trovate prove di violazioni, ma Twitter consiglia ugualmente di intervenire  \"], 'url': ['https://www.ilpost.it/internet/page/3/'], 'website': ['ilpost.it'], 'timestamp': [1528360812000], 'publication_date': ['2018-06-07T08:40:12Z'], 'flattened_entities': ['azienda cina drake_rapper password twitter'], 'result_entities': ['Cina', 'Azienda', 'Drake (rapper)', 'Password', 'Twitter'], '_version_': 1613294439957004288}]\n"
     ]
    }
   ],
   "source": [
    "## let's now retrieve the meaningful part of the json document\n",
    "# response{}--->docs[]\n",
    "\n",
    "docs = json_data['response']['docs']\n",
    "print(\"Number of documents in new json: \",len(docs))\n",
    "print(\"Number of documents in old json: \",len(json_data_old))\n",
    "\n",
    "# let's use both data dumps, make a single list\n",
    "\n",
    "docs = docs + json_data_old\n",
    "\n",
    "print(\"New length: \", len(docs))\n",
    "print(docs[:1])\n",
    "\n",
    "for i, dictionary in enumerate(docs):\n",
    "    for field in ['title', 'abstract', 'flattened_entities']:\n",
    "        if isinstance(dictionary[field], list):\n",
    "            # re-format data to hold string instead of single-list item\n",
    "            docs[i][field] = dictionary[field][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract test corpus from the whole data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New data length: 1270, test set data length: 400\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(docs)\n",
    "test_corpus_length = 400\n",
    "# take first examples\n",
    "test_corpus = docs[:test_corpus_length]\n",
    "docs = docs[test_corpus_length:]\n",
    "\n",
    "filename = 'TOWL_f_entitites_test_corpus.json'\n",
    "# save test file to json\n",
    "with open(filename, 'w') as file:\n",
    "    json.dump(test_corpus,file)\n",
    "print(\"New data length: {0}, test set data length: {1}\".format(len(docs), len(test_corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't remove duplicates in this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates:  85\n",
      "New length:  208\n"
     ]
    }
   ],
   "source": [
    "\"\"\"unique_json = json_data\n",
    "counter = 0\n",
    "for i, dictionary in enumerate(unique_json):\n",
    "    try:\n",
    "        index = json_data.index(dictionary, i+1, len(json_data))\n",
    "        #print(\"Found a duplicate with index {0} from index {1}\".format(index, i))\n",
    "        del(unique_json[index])\n",
    "        counter = counter + 1\n",
    "    except ValueError:\n",
    "        None\n",
    "print(\"Number of duplicates: \", counter)\n",
    "print(\"New length: \", len(unique_json))\n",
    "#print(unique_json)\n",
    "json_data = unique_json\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instead, re-inforce entites 'short document type' by duplicating all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New length:  5080\n"
     ]
    }
   ],
   "source": [
    "docs = docs + docs + docs + docs\n",
    "import random\n",
    "# shuffle new list\n",
    "random.shuffle(docs)\n",
    "print(\"New length: \", len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2-Training model with flattened_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument(['adecco', 'automobile', 'blockchain', 'energia', 'fastweb', 'italia', 'microsoft', 'milano', 'nespresso', 'ossimoro', 'sky_italia', 'sole', 'tecnologia', 'wired'], [10])\n"
     ]
    }
   ],
   "source": [
    "# we'll use every data at our disposal for training\n",
    "n_examples =  len(json_data)\n",
    "TRAIN_DATA_LENGTH = n_examples\n",
    "FLATTENED_ENTITIES_FIELD = 'flattened_entities'\n",
    "\n",
    "# build training corpus: take the flattened_entities, preprocess them (tokenize, delete spaces..)\n",
    "# and create the TaggedDocument needed for training\n",
    "train_corpus = [gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(\n",
    "    d[FLATTENED_ENTITIES_FIELD]), [i]) for i, d in enumerate(docs)]\n",
    "\n",
    "print(train_corpus[10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 2 models at once - PV-DBOW and PV-DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow,d100,n5,t4)\n",
      "Doc2Vec(\"alpha=0.2\",dm/m,d100,n5,w10,t4)\n",
      "Vocabulary created!\n",
      "Training Doc2Vec(dbow,d100,n5,t4)\n",
      "CPU times: user 17.8 s, sys: 1.09 s, total: 18.8 s\n",
      "Wall time: 10.6 s\n",
      "Training Doc2Vec(\"alpha=0.2\",dm/m,d100,n5,w10,t4)\n",
      "CPU times: user 23.9 s, sys: 2.62 s, total: 26.5 s\n",
      "Wall time: 14.9 s\n",
      "Models Saved\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "\n",
    "# let's try training two models at once: Paragraph Vector - Distributed Memory (PV-DM), just like CBOW to W2V\n",
    "# and Paragraph Vector - Distributed Bag of Words (PV-DBOW), analogous to W2V Skip-gram\n",
    "epochs = 50\n",
    "vec_size = 100\n",
    "alpha = 0.2 # TODO: TRY HIGHER ALPHA\n",
    "MODEL_NAME = \"Models/d2v_TA_f_entities\"\n",
    "\n",
    "models = [\n",
    "    # min_count = 1; don't discard any word at all, we're using entites, meaningful words by default\n",
    "    # PV-DBOW plain (with default alpha)\n",
    "    Doc2Vec(dm=0, vector_size=vec_size, negative=5, hs=0, min_count=1, sample=0, \n",
    "            epochs=epochs, workers=cores),\n",
    "    # PV-DM w/ higher alpha\n",
    "    Doc2Vec(dm=1, vector_size = vec_size, window=10, negative=5, hs=0, min_count=1, sample=0, \n",
    "            epochs = epochs, workers=cores, alpha = alpha, comment='alpha=0.2'),\n",
    "]\n",
    "\n",
    "# build our vocabulary of words (all the unique words encountered inside our corpus, needed for training)\n",
    "for model in models:\n",
    "    print(model)\n",
    "    model.build_vocab(train_corpus)\n",
    "print(\"Vocabulary created!\")\n",
    "\n",
    "# train the models on the given data!\n",
    "counter = 0\n",
    "for model in models:\n",
    "    print(\"Training %s\" % model)\n",
    "    %time model.train(train_corpus, total_examples=len(train_corpus), epochs=model.epochs)\n",
    "    model.save(MODEL_NAME+str(counter)+'.model')\n",
    "    counter = counter + 1\n",
    "print(\"Models Saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization (using PCA and Plotly libs)\n",
    "all credits in the other file d2v_abstract+title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~D4nt3/34.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "tls.set_credentials_file(username='D4nt3', api_key='FdMB4O6qCfciGDOnLvdQ')\n",
    "\n",
    "COMPONENT_ONE = \"principal component 1\"\n",
    "COMPONENT_TWO = \"principal component 2\"\n",
    "\n",
    "# load model to visualize\n",
    "model_number = 1\n",
    "model = Doc2Vec.load(MODEL_NAME+str(model_number)+'.model')\n",
    "\n",
    "docs_vecs = []\n",
    "# docvecs (list of Doc2VecKeyedVectors) \n",
    "# – Vector representations of the documents in the corpus. Each vector has size == vector_size\n",
    "for doc in iter(range(0, len(model.docvecs))):\n",
    "    docs_vecs.append(model.docvecs[doc])\n",
    "    \n",
    "# loading dataset into Pandas DataFrame\n",
    "df = pd.DataFrame.from_records(docs_vecs)    \n",
    "\n",
    "    ## PCA dimensionality-reduction ##\n",
    "# PCA is effected by scale so you need to scale the features in your data before applying PCA. \n",
    "features = [i for i in range(vec_size)]\n",
    "x = df.loc[:, features].values # get features values\n",
    "#print(x)\n",
    "# standardize data\n",
    "x = StandardScaler().fit_transform(x) # scale data (especially in case different measures are used)\n",
    "    \n",
    "# build PCA model in 2D\n",
    "pca = PCA(n_components=2) # The new components are just the two main dimensions of variation.\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents, \n",
    "                           columns = [COMPONENT_ONE, COMPONENT_TWO])\n",
    "    \n",
    "# we'll draw a scatter graph with labels\n",
    "traces = []\n",
    "# let's get the labels\n",
    "titles = [dictionary['title'] for dictionary in json_data]\n",
    "#print(print(titles))\n",
    "finalDf = principalDf\n",
    "\n",
    "# double check to be sure we got labels just right\n",
    "#sample_title = training_set[10]['title']\n",
    "#sample_text = training_set[10]['flattened_entities']\n",
    "#inferred_vector = model.infer_vector(gensim.utils.simple_preprocess(sample_text))\n",
    "#print(gensim.utils.simple_preprocess(sample_text))\n",
    "# pca sample\n",
    "#x = np.array(inferred_vector)\n",
    "#pca = PCA(n_components=2)\n",
    "#pca_result = pca.fit_transform(x)\n",
    "#trace_sample = go.Scatter(\n",
    "#        x = pca_result[0],\n",
    "#        y = pca_result[1],\n",
    "#        mode = 'markers',\n",
    "#            #name = 'blue markers',\n",
    "#        marker = dict(\n",
    "#            size = 7,\n",
    "#            color = 'green',\n",
    "#        ),\n",
    "#        text = str(sample_title)\n",
    "#    )\n",
    "#traces.append(trace_sample)\n",
    "\n",
    "\n",
    "    # each trace will represent a point (squeezed vector from higher dimensions),\n",
    "    # and each point will have the title of the news assigned\n",
    "for i in range(len(finalDf)):\n",
    "    color = 'rgba(0, 0, 110, .8)'\n",
    "    if 'Apple' in titles[i]:\n",
    "            color = 'red'\n",
    "    elif 'Amazon' in titles[i]:\n",
    "        color = 'yellow'\n",
    "    elif 'Facebook' in titles[i] or 'Instagram' in titles[i]:\n",
    "        color = 'green'\n",
    "    elif 'spazio' in json_data[i][FLATTENED_ENTITIES_FIELD]:\n",
    "        color = 'black'\n",
    "    \n",
    "    trace0 = go.Scatter(\n",
    "        x = finalDf.loc[i:i, \"principal component 1\"],\n",
    "        y = finalDf.loc[i:i, \"principal component 2\"],\n",
    "        mode = 'markers',\n",
    "            #name = 'blue markers',\n",
    "        marker = dict(\n",
    "            size = 7,\n",
    "            color = color,\n",
    "        ),\n",
    "        text = str(titles[i])\n",
    "    )\n",
    "    traces.append(trace0)\n",
    "\n",
    "data = traces \n",
    "layout = dict(title = 'PCA Representantion of D2V on Flattened Entities',\n",
    "            hovermode= 'closest',\n",
    "            xaxis= dict(\n",
    "                title= 'first component',\n",
    "                ticklen= 5,\n",
    "                gridwidth= 2,\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                title= 'second component',\n",
    "                ticklen= 5,\n",
    "                gridwidth= 2,\n",
    "            ),\n",
    "            showlegend = False\n",
    "        )\n",
    "# Plot and embed in ipython notebook!\n",
    "    \n",
    "fig = dict(data = data, layout = layout)\n",
    "py.iplot(fig, filename='TA_model_flattened_entities-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means on PCA-reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=600,\n",
       "    n_clusters=5, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try another way for clustering data: K-Mean, an even more popular algorithm,\n",
    "# which I know from the introductory course on AI, so it might be better \n",
    "# to utilize algorithms which I know and can talk about in the presentation\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# build k-means model\n",
    "kmeans = KMeans(n_clusters = 5, max_iter=600, algorithm = 'auto', verbose=0,\n",
    "               init='k-means++', n_init=10) \n",
    "\n",
    "kmeans.fit(principalComponents) # data, as vectors of documents (in 2D)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize centroids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~D4nt3/22.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "tls.set_credentials_file(username='D4nt3', api_key='FdMB4O6qCfciGDOnLvdQ')\n",
    "\n",
    "COMPONENT_ONE = \"principal component 1\"\n",
    "COMPONENT_TWO = \"principal component 2\"\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# each trace will represent a point (squeezed vector from higher dimensions),\n",
    "# and each point will have the title of the news assigned\n",
    "for i in range(len(finalDf)):\n",
    "    # assign a color to each point belonging to a specific cluster\n",
    "    # computing distance from centroid\n",
    "    x = finalDf.loc[i:i, \"principal component 1\"]\n",
    "    y = finalDf.loc[i:i, \"principal component 2\"]\n",
    "    color = 'rgba(0, 0, 180, 0.8)'\n",
    "    min_d = 10000\n",
    "    closest_centroid = []\n",
    "    for centroid in centroids:\n",
    "        dist = np.linalg.norm(centroid-np.array(x, y))\n",
    "        if dist<min_d:\n",
    "            min_d = dist\n",
    "            closest_centroid = centroid\n",
    "    #print(\"Prediction: \",closest_centroid)\n",
    "    if np.array_equal(closest_centroid, centroids[0]):\n",
    "        color = 'blue'\n",
    "    elif np.array_equal(closest_centroid, centroids[1]):\n",
    "        color = 'pink'\n",
    "    elif np.array_equal(closest_centroid, centroids[2]):\n",
    "        color = 'yellow'\n",
    "    elif np.array_equal(closest_centroid, centroids[3]):\n",
    "        color = 'green'\n",
    "    else:\n",
    "        color = 'black'\n",
    "        \n",
    "    trace0 = go.Scatter(\n",
    "        x = x, \n",
    "        y = y,\n",
    "        mode = 'markers',\n",
    "            #name = 'blue markers',\n",
    "        marker = dict(\n",
    "            size = 7,\n",
    "            color = color,\n",
    "        ),\n",
    "        text = str(titles[i])\n",
    "    )\n",
    "    traces.append(trace0)\n",
    "\n",
    "# draw centroids\n",
    "c_trace = go.Scatter(\n",
    "    x = centroids[:, 0],\n",
    "    y = centroids[:, 1],\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        size = 9,\n",
    "        color = 'red',\n",
    "    ),\n",
    "    text = 'centroid'\n",
    ")\n",
    "traces.append(c_trace)\n",
    "\n",
    "data = traces \n",
    "layout = dict(title = 'PCA Representantion of DocVectors',\n",
    "            hovermode= 'closest',\n",
    "            xaxis= dict(\n",
    "                title= 'first component',\n",
    "                ticklen= 5,\n",
    "                gridwidth= 2,\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                title= 'second component',\n",
    "                ticklen= 5,\n",
    "                gridwidth= 2,\n",
    "            ),\n",
    "            showlegend = False\n",
    "        )\n",
    "# Plot and embed in ipython notebook!\n",
    "    \n",
    "fig = dict(data = data, layout = layout)\n",
    "py.iplot(fig, filename='TA_model_entities_kmeans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word 're' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7fe6848aa0d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# print out dimension of the vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m're'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'donna'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'uomo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1420\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m                 )\n\u001b[0;32m-> 1422\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m   1395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m         \"\"\"\n\u001b[0;32m-> 1397\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.wmdistance() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 're' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import  gensim\n",
    "\n",
    "MODEL_NAME = 'Models/d2v_TA_f_entities0.model'\n",
    "MODEL_TWO = 'Models/d2v_TA_f_entities1.model'\n",
    "#model = Doc2Vec.load(MODEL_ONE)\n",
    "model = Doc2Vec.load(MODEL_TWO)\n",
    "inferred_vectors = []\n",
    "# print out dimension of the vocabulary \n",
    "print(len(model.wv.vocab))\n",
    "#print(model.most_similar(positive=['re', 'donna'], negative=['uomo']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def perform_dbscan(eps = 0.4, min_samples = 4, metric = 'euclidean', algorithm = 'auto', data = None, verbose = True\n",
    "                  , titles = None):\n",
    "    \"\"\"perform DBSCAN over given data, using given parametrs. Returns dbscan object.\"\"\"\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples, metric=metric, algorithm=algorithm).fit(data)\n",
    "\n",
    "    #print(\"Core samples: \")\n",
    "    #for i in db.core_sample_indices_ :\n",
    "    #    print(titles[i]+\"\\n\")\n",
    "\n",
    "    # labels will print out the number of the cluster each example belongs to;\n",
    "    # -1 if the vector is considered noise (not belonging to any cluster)\n",
    "    #print(\"Labels: \", db.labels_)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"##Clusters##\")\n",
    "        cluster = [[]]\n",
    "        noise = []\n",
    "        noise_r = []\n",
    "        for i, label in enumerate(db.labels_):\n",
    "            if label != -1:\n",
    "                try:\n",
    "                    cluster[label].append(titles[i])\n",
    "                except Exception as e:\n",
    "                    cluster.append([titles[i]])\n",
    "            else:\n",
    "                noise.append(titles[i])\n",
    "                noise_r.append(i)\n",
    "        for list_ in cluster:\n",
    "            print(\"Cluster:\", list_)\n",
    "        print(\"Noise: \", noise)\n",
    "\n",
    "        print(\"DBSCAN finished.\\n\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents:  400\n",
      "[['africa', 'classe_sociale', 'economia', 'ibm', 'multinazionale', 'politica', 'tempo', 'wi', 'fi'], ['playstation', 'sport']]\n"
     ]
    }
   ],
   "source": [
    "## load test-corpus\n",
    "import json\n",
    "import gensim\n",
    "\n",
    "with open('TOWL_f_entities_test_corpus.json', 'r') as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "# we're expecting a list\n",
    "assert isinstance(json_data, list)\n",
    "titles = [dictionary['title'] for dictionary in json_data]\n",
    "test_corpus = [gensim.utils.simple_preprocess(d['flattened_entities']) for d in json_data]\n",
    "print(\"Number of documents: \", len(test_corpus))\n",
    "print(test_corpus[:2])\n",
    "\n",
    "inferred_vectors = [model.infer_vector(doc) for doc in test_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0 -1  0  0  0  0  0 -1  0 -1  0  0  0  0  0  0  0  0  0  1  0\n",
      "  0 -1 -1  0  0  0  0  0 -1 -1 -1  0  0  2 -1  0 -1  0 -1  0  0  0 -1  0\n",
      "  0  0  0  0 -1  0 -1 -1  0  0 -1 -1  0 -1  0 -1 -1 -1  0 -1  0  0 -1  0\n",
      "  3  0  0  0 -1  0 -1  0  0  1  0 -1 -1 -1  4  0  0 -1  0 -1 -1  0  0  0\n",
      "  0 -1  0  0 -1  0  0  0  0  0  5  0  5  0  0 -1 -1  0  0  0  0  0 -1  0\n",
      "  0 -1  0 -1  0  0 -1  0 -1 -1  0  0 -1 -1  0  0  0 -1  0  0  0 -1  0  0\n",
      "  0  0 -1  0  0  0  0  0  0  0  0  0 -1 -1 -1 -1 -1  0  0  0 -1 -1 -1  2\n",
      " -1  0 -1  0  0  0  0  0  0  0  0 -1  0 -1 -1 -1  0  0 -1  0  0  0  0 -1\n",
      "  0  0  0 -1  0  0  6 -1  0  0  0 -1  0  0  0 -1 -1 -1  0  0 -1  0 -1 -1\n",
      "  0  0  0 -1  0  0  0 -1  0  0  0  0  0 -1  0  0  0  0  0 -1  0  0  0  0\n",
      " -1 -1  0 -1 -1 -1  0  0  0 -1 -1  0  0  0 -1 -1  0  0 -1 -1 -1  0  0 -1\n",
      "  0 -1 -1  0  0  3 -1 -1 -1 -1 -1 -1  0  0  0 -1 -1  0  0  0  0  0  0  0\n",
      "  0  0 -1 -1  0  0  0  0 -1  0  0  0  0  0 -1  0  0  0  0 -1  0 -1 -1 -1\n",
      "  0 -1  6  0 -1  0  0  0  0  0  0  0  0  0  0 -1 -1 -1  0  0  0  0  0  0\n",
      "  0 -1  0  0  7 -1  0  0  0  0 -1  0  0  0  0  0 -1  0  0  0 -1  0 -1  0\n",
      "  0 -1  0 -1  0 -1  0  0  0 -1  0 -1  0  0 -1  0  0 -1  0  0  4 -1 -1  0\n",
      "  0 -1 -1 -1 -1  0  7  0 -1  0  0  0  0 -1  0 -1]\n"
     ]
    }
   ],
   "source": [
    "db = perform_dbscan(eps = 0.48, min_samples = 2, metric = 'cosine', algorithm = 'auto',\n",
    "                    data = inferred_vectors, verbose = False, titles = titles)\n",
    "print(db.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Clusters##\n",
      "Cluster: ['IoT: Notizie del giorno - News online su Punto Informatico', 'TMux | Punto Informatico', 'Il coding senza età: una donna ha creato un’app a 81 anni - Corriere.it', 'Facebook: ex moderatrice fa causa, traumatizzata da immagini - Hi-tech - ANSA.it', 'Tecnologia - Pagina 3 di 73 - Il Post']\n",
      "Noise:  ['Tech - Pagina 6 - Wired', 'Redstone 5 sarà Windows 10 October 2018 Update', 'Ecco cosa scatena il sonno incontrollabile nei pazienti narcolettici - Repubblica.it', 'Machine learning - Wired', 'Futuri robot con pelle che sente il vento - Hi-tech - ANSA.it']\n",
      "DBSCAN finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's take a few documents, randomly chosen from the inferred vectors\n",
    "import random\n",
    "subsample_size = 10\n",
    "subsamples = []\n",
    "titles2 = []\n",
    "for i in range(subsample_size):\n",
    "    index = random.randint(0, len(inferred_vectors)-1)\n",
    "    subsamples.append(inferred_vectors[index])\n",
    "    titles2.append(titles[index])\n",
    "    \n",
    "db = perform_dbscan(eps = 0.4, min_samples = 2, metric = 'cosine', algorithm = 'auto',\n",
    "                    data = subsamples, verbose = True, titles = titles2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
