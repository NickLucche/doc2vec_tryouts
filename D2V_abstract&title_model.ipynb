{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA imports\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec imports\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import gensim\n",
    "\n",
    "import json # to open our data file\n",
    "DATA_FILENAME = \"trend_analisys.json\"\n",
    "# open json file\n",
    "with open(DATA_FILENAME, \"r\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "# we're expecting a list now, since our json file is a json array\n",
    "assert type(json_data) is list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicates from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates:  85\n",
      "New length:  208\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i, dictionary in enumerate(json_data):\n",
    "    try:\n",
    "        index = json_data.index(dictionary, i+1, len(json_data))\n",
    "        #print(\"Found a duplicate with index {0} from index {1}\".format(index, i))\n",
    "        del(json_data[index])\n",
    "        counter = counter + 1\n",
    "    except ValueError:\n",
    "        None\n",
    "print(\"Number of duplicates: \", counter)\n",
    "print(\"New length: \", len(json_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1 - Training Model with Abstract field (whole text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 208, number of train examples: 187, number of test examples: 21\n"
     ]
    }
   ],
   "source": [
    "# we have our json data now, let's go ahead and divide into training and test set\n",
    "n_examples =  len(json_data)\n",
    "# how much of the data we're going to be using for training and for testing\n",
    "# default values: 80% train, 20% test\n",
    "TRAIN_DATA_LENGTH = 9 * n_examples // 10\n",
    "TEST_DATA_LENGTH = n_examples - TRAIN_DATA_LENGTH\n",
    "ABSTRACT_FIELD_NAME = 'abstract'\n",
    "TITLE_FIELD_NAME = 'title'\n",
    "\n",
    "print(\"Total examples: {0}, number of train examples: {1}, number of test examples: {2}\".format(n_examples,TRAIN_DATA_LENGTH, TEST_DATA_LENGTH))\n",
    "\n",
    "# TODO: Randomize selection of examples, don't just take the first ones\n",
    "# build training corpus: take the needed abstract, preprocess them (tokenize, delete spaces..)\n",
    "# and create the TaggedDocument needed for training\n",
    "# also added title to it \n",
    "train_corpus = [gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(\n",
    "    d[TITLE_FIELD_NAME]+d[ABSTRACT_FIELD_NAME]), [i]) for i, d in enumerate(json_data) if i<TRAIN_DATA_LENGTH]\n",
    "\n",
    "test_corpus = [gensim.utils.simple_preprocess(\n",
    "    d[TITLE_FIELD_NAME]+d[ABSTRACT_FIELD_NAME]) for i, d in enumerate(json_data) if i>TRAIN_DATA_LENGTH]\n",
    "assert len(train_corpus)==TRAIN_DATA_LENGTH\n",
    "#print(train_corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train model using Skip-Gram training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 780 ms, sys: 8 ms, total: 788 ms\n",
      "Wall time: 315 ms\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "# create the doc2vec model\n",
    "# TODO: tune this parameters (personally, I think we could use a bigger vec_size, like 50)\n",
    "max_epochs = 40\n",
    "vec_size = 50\n",
    "alpha = 0.030\n",
    "MODEL_NAME = \"Models/d2v_TA_abstract&title.model\"\n",
    "\n",
    "model = Doc2Vec(vector_size=vec_size,\n",
    "                min_count=2, # words that appear less than twice in the corpus are ignored\n",
    "                dm=1) # Skip-Gram\n",
    "                \n",
    "# build our vocabulary of words (all the unique words encountered inside our corpus, needed for training)\n",
    "model.build_vocab(train_corpus)\n",
    "\n",
    "# train the model on the given data!\n",
    "%time model.train(train_corpus, total_examples = model.corpus_count, epochs = model.epochs)\n",
    "\n",
    "model.save(MODEL_NAME)\n",
    "print(\"Model Saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Data\n",
    "credits: https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.119387</td>\n",
       "      <td>-0.346534</td>\n",
       "      <td>-0.273014</td>\n",
       "      <td>-1.118393</td>\n",
       "      <td>-0.581692</td>\n",
       "      <td>0.611214</td>\n",
       "      <td>0.127722</td>\n",
       "      <td>-0.202269</td>\n",
       "      <td>0.153298</td>\n",
       "      <td>-1.062154</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108225</td>\n",
       "      <td>-0.207567</td>\n",
       "      <td>0.727272</td>\n",
       "      <td>0.585349</td>\n",
       "      <td>0.222956</td>\n",
       "      <td>0.875400</td>\n",
       "      <td>0.151211</td>\n",
       "      <td>0.468302</td>\n",
       "      <td>-1.184027</td>\n",
       "      <td>-0.635066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.157088</td>\n",
       "      <td>-0.485685</td>\n",
       "      <td>-0.351238</td>\n",
       "      <td>-1.503304</td>\n",
       "      <td>-0.765628</td>\n",
       "      <td>0.824385</td>\n",
       "      <td>0.154027</td>\n",
       "      <td>-0.271140</td>\n",
       "      <td>0.219725</td>\n",
       "      <td>-1.413674</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137261</td>\n",
       "      <td>-0.271600</td>\n",
       "      <td>0.996338</td>\n",
       "      <td>0.800607</td>\n",
       "      <td>0.303293</td>\n",
       "      <td>1.172230</td>\n",
       "      <td>0.214158</td>\n",
       "      <td>0.628052</td>\n",
       "      <td>-1.598117</td>\n",
       "      <td>-0.848087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.065053</td>\n",
       "      <td>-0.219481</td>\n",
       "      <td>-0.163035</td>\n",
       "      <td>-0.659115</td>\n",
       "      <td>-0.348154</td>\n",
       "      <td>0.363078</td>\n",
       "      <td>0.066665</td>\n",
       "      <td>-0.117280</td>\n",
       "      <td>0.105474</td>\n",
       "      <td>-0.621904</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074848</td>\n",
       "      <td>-0.132332</td>\n",
       "      <td>0.437380</td>\n",
       "      <td>0.348238</td>\n",
       "      <td>0.134807</td>\n",
       "      <td>0.510593</td>\n",
       "      <td>0.081707</td>\n",
       "      <td>0.281237</td>\n",
       "      <td>-0.704783</td>\n",
       "      <td>-0.378905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.053392</td>\n",
       "      <td>-0.160366</td>\n",
       "      <td>-0.137524</td>\n",
       "      <td>-0.514732</td>\n",
       "      <td>-0.276063</td>\n",
       "      <td>0.295708</td>\n",
       "      <td>0.057035</td>\n",
       "      <td>-0.082042</td>\n",
       "      <td>0.073855</td>\n",
       "      <td>-0.485232</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053983</td>\n",
       "      <td>-0.096164</td>\n",
       "      <td>0.350960</td>\n",
       "      <td>0.277355</td>\n",
       "      <td>0.106803</td>\n",
       "      <td>0.417250</td>\n",
       "      <td>0.065572</td>\n",
       "      <td>0.212934</td>\n",
       "      <td>-0.555022</td>\n",
       "      <td>-0.293616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.067006</td>\n",
       "      <td>-0.206694</td>\n",
       "      <td>-0.158040</td>\n",
       "      <td>-0.636199</td>\n",
       "      <td>-0.326443</td>\n",
       "      <td>0.336898</td>\n",
       "      <td>0.075776</td>\n",
       "      <td>-0.118137</td>\n",
       "      <td>0.083223</td>\n",
       "      <td>-0.589575</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061092</td>\n",
       "      <td>-0.124880</td>\n",
       "      <td>0.408338</td>\n",
       "      <td>0.342582</td>\n",
       "      <td>0.135183</td>\n",
       "      <td>0.483082</td>\n",
       "      <td>0.088697</td>\n",
       "      <td>0.266961</td>\n",
       "      <td>-0.668985</td>\n",
       "      <td>-0.364236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.119387 -0.346534 -0.273014 -1.118393 -0.581692  0.611214  0.127722   \n",
       "1  0.157088 -0.485685 -0.351238 -1.503304 -0.765628  0.824385  0.154027   \n",
       "2  0.065053 -0.219481 -0.163035 -0.659115 -0.348154  0.363078  0.066665   \n",
       "3  0.053392 -0.160366 -0.137524 -0.514732 -0.276063  0.295708  0.057035   \n",
       "4  0.067006 -0.206694 -0.158040 -0.636199 -0.326443  0.336898  0.075776   \n",
       "\n",
       "         7         8         9     ...           40        41        42  \\\n",
       "0 -0.202269  0.153298 -1.062154    ...    -0.108225 -0.207567  0.727272   \n",
       "1 -0.271140  0.219725 -1.413674    ...    -0.137261 -0.271600  0.996338   \n",
       "2 -0.117280  0.105474 -0.621904    ...    -0.074848 -0.132332  0.437380   \n",
       "3 -0.082042  0.073855 -0.485232    ...    -0.053983 -0.096164  0.350960   \n",
       "4 -0.118137  0.083223 -0.589575    ...    -0.061092 -0.124880  0.408338   \n",
       "\n",
       "         43        44        45        46        47        48        49  \n",
       "0  0.585349  0.222956  0.875400  0.151211  0.468302 -1.184027 -0.635066  \n",
       "1  0.800607  0.303293  1.172230  0.214158  0.628052 -1.598117 -0.848087  \n",
       "2  0.348238  0.134807  0.510593  0.081707  0.281237 -0.704783 -0.378905  \n",
       "3  0.277355  0.106803  0.417250  0.065572  0.212934 -0.555022 -0.293616  \n",
       "4  0.342582  0.135183  0.483082  0.088697  0.266961 -0.668985 -0.364236  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try to visualize all document_vectors\n",
    "# get all vectors of documents we created from model training\n",
    "docs_vecs = []\n",
    "# docvecs (list of Doc2VecKeyedVectors) \n",
    "# – Vector representations of the documents in the corpus. Each vector has size == vector_size\n",
    "for doc in iter(range(0, len(model.docvecs))):\n",
    "    docs_vecs.append(model.docvecs[doc])\n",
    "\n",
    "# loading dataset into Pandas DataFrame\n",
    "df = pd.DataFrame.from_records(docs_vecs)\n",
    "#df.head()\n",
    "\n",
    "#df[['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.11938673 -0.34653431 -0.2730135  ...  0.46830156 -1.18402719\n",
      "  -0.63506573]\n",
      " [ 0.15708816 -0.48568472 -0.35123813 ...  0.62805247 -1.59811747\n",
      "  -0.84808689]\n",
      " [ 0.06505304 -0.21948071 -0.1630352  ...  0.28123662 -0.7047835\n",
      "  -0.37890479]\n",
      " ...\n",
      " [ 0.06400771 -0.20057003 -0.15090545 ...  0.26194432 -0.66003615\n",
      "  -0.34150797]\n",
      " [ 0.07232197 -0.17000496 -0.12539493 ...  0.22952157 -0.57459134\n",
      "  -0.30948418]\n",
      " [ 0.10433161 -0.26795733 -0.21095505 ...  0.36031994 -0.9214952\n",
      "  -0.48799506]]\n"
     ]
    }
   ],
   "source": [
    "# PCA is effected by scale so you need to scale the features in your data before applying PCA. \n",
    "features = [i for i in range(vec_size)]\n",
    "\n",
    "x = df.loc[:, features].values # get features values\n",
    "#print(x)\n",
    "# we don't have target here y = df.loc[:,['target']].values # get target values (guess kind of flower/Iris)\n",
    "\n",
    "# standardize data\n",
    "x = StandardScaler().fit_transform(x) # scale data (especially in case different measures are used)\n",
    "# pd.DataFrame(data = x, columns = features).head() # show first data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Projection with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build PCA model in 2D\n",
    "pca = PCA(n_components=2) # The new components are just the two main dimensions of variation.\n",
    "\n",
    "principalComponents = pca.fit_transform(x)\n",
    "\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "principalDf.head()\n",
    "# these components drawn don't hold a lot of information 'per-se', they're just the result \n",
    "# of dimension-reduction\n",
    "\n",
    "finalDf = principalDf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Data interactively with Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "tls.set_credentials_file(username='D4nt3', api_key='FdMB4O6qCfciGDOnLvdQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~D4nt3/6.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll draw a scatter graph with labels\n",
    "traces = []\n",
    "# let's get the labels\n",
    "titles = [dictionary[TITLE_FIELD_NAME] for i, dictionary in enumerate(json_data) if i<TRAIN_DATA_LENGTH]\n",
    "# double check to be sure we got labels just right\n",
    "#i = 50\n",
    "#print(\"TITLE: {0}\\n <<{1}>>\".format(titles[i], train_corpus[i][:1]))\n",
    "\n",
    "# each trace will represent a point (squeezed vector from higher dimensions),\n",
    "# and each point will have the title of the news assigned\n",
    "for i in range(len(finalDf)):\n",
    "    trace0 = go.Scatter(\n",
    "        x = finalDf.loc[i:i, \"principal component 1\"],\n",
    "        y = finalDf.loc[i:i, \"principal component 2\"],\n",
    "        mode = 'markers',\n",
    "        #name = 'blue markers',\n",
    "        marker = dict(\n",
    "            size = 7,\n",
    "            color = 'rgba(0, 0, 110, .8)',\n",
    "        ),\n",
    "        text = str(titles[i])\n",
    "    )\n",
    "    traces.append(trace0)\n",
    "\n",
    "data = traces \n",
    "layout = dict(title = 'PCA Representantion of DocVectors',\n",
    "        hovermode= 'closest',\n",
    "        xaxis= dict(\n",
    "            title= 'first component',\n",
    "            ticklen= 5,\n",
    "            gridwidth= 2,\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title= 'second component',\n",
    "            ticklen= 5,\n",
    "            gridwidth= 2,\n",
    "        ),\n",
    "        showlegend = False\n",
    "    )\n",
    "# Plot and embed in ipython notebook!\n",
    "fig = dict(data = data, layout = layout)\n",
    "py.iplot(fig, filename='TA_model-scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some (basic) testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (181): «iphone milano tutti in fila per nuovi modelli tlc ansa itda questa mattina anche in italia possibile acquistare nuovi iphone xs xs max per occasione lo store apple di piazza liberty ha aperto le porte al pubblico già partire dalle primi clienti hanno così potuto vedere per la prima volta dal vivo nuovi modelli presentati dall azienda di cupertino anche se il sistema di pre ordine online ha evitato che si creassero lunghe code all esterno chi come vitaly giovane studente russo all università bocconi di milano che si posizionato di fronte all ingresso già ieri pomeriggio assicurandosi così di essere il primo tra clienti senza prenotazione ad accedere al negozio una lunga attesa mitigata dal caffè caldo servito tutti clienti che pazientemente attendono il loro turno»\n",
      "\n",
      "\n",
      "Similar Doc-->(doctag:6,score:0.9995049238204956):<<Cinque cose da fare per gestire bene la sicurezza degli account social aziendali - Il Sole 24 ORE>>\n",
      "\n",
      "Similar Doc-->(doctag:179,score:0.9994415044784546):<<Wsj, Facebook ha trattato con le banche per i dati degli utenti - Internet e Social - ANSA.it>>\n",
      "\n",
      "Similar Doc-->(doctag:22,score:0.999431848526001):<<Spotify rende la vita più semplice agli artisti indipendenti - La Stampa>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning:\n",
      "\n",
      "Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's check if the model is at least decent,\n",
    "# which means: is it able to at least recognize news/documents\n",
    "# it has seen in training?\n",
    "import random\n",
    "\n",
    "# Pick a random document from the train corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(train_corpus) - 1)\n",
    "\n",
    "inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "similar_docs = model.docvecs.most_similar([inferred_vector], topn=3)\n",
    "\n",
    "# show the 3 most similar document titles\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "for doc_tag, similarity in similar_docs:\n",
    "    print(\"\\nSimilar Doc-->(doctag:{0},score:{1}):<<{2}>>\".format(doc_tag, similarity, titles[doc_tag]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some more basic testing on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document : «le auto guida autonoma migliorano il flusso del trafficol introduzione di autoveicoli guida autonoma nel traffico può contribuire migliorarne il flusso diminuire il consumo di carburante questo secondo una ricerca della rutgers university camden recentemente presentata washington una ricerca la national science foundation un agenzia governativa statunitense che si occupa della promozione del progresso scientifico dell avanzamento della salute prosperità welfare all interno del paese ha invitato gli scienziati discutere del proprio lavoro con leader dell industria automobilistica gli ufficiali governativi in occasione del washington auto show tenutosi gennaio un team di ricercatori esperti in teoria del flusso di traffico teoria del controllo robotica sistemi cyber fisici ingegneria dei trasporti hanno mostrato ai rappresentanti politici presenti all evento come le automobili guida autonoma possano aiutare prevenire gli ingorghi stradali farli scomparire una volta formati meccanismi del traffico gli automobilisti tendono creare un traffico di tipo stop and go ad esempio quando cambiano corsia confluiscono in una strada ancora causa delle naturali oscillazioni della guida umana gli studiosi hanno dimostrato che controllando il ritmo delle auto guida indipendente questi veicoli contribuiscono regolarizzare il flusso di traffico dissolvere le ondate stop and go durante il washington auto show gli scienziati hanno permesso ai visitatori di osservare il loro esperimento attraverso la realtà virtuale guardando una in cui un unica auto guida autonoma girava in una pista in maniera continua insieme ad altre venti automobili guidate da persone impatto delle automobili guida autonoma ricercatori hanno determinato che anche una piccola percentuale di auto guida autonoma il può avere un impatto significativo nell eliminazione dell oscillazione del traffico nella diminuzione del consumo totale di carburante degli eventi di frenata benedetto piccoli ricercatore della rutgers university camden autore dell articolo che descrive lo studio ha commentato molti politici produttori di auto venditori altre persone con cui abbiamo parlato erano molto impressionati dai risultati della ricerca hanno mostrato sentimenti positivi nei riguardi dei veicoli autonomi sono tutti accordo che impatto sull economia del traffico reale su quello ambientale potrebbero essere di grande importanza la mobilità sostenibile invece il tema dell articolo le alternative per la mobilità sostenibile il caso dell idrogeno di lorenzo battisti luciano celi che potrete leggere acquistando ultimo numero di sapere»\n",
      "\n",
      "\n",
      "Similar Doc-->(doctag:84,score:0.9999004602432251):<<Ufo, Alieni, Extraterrestri, Alexander Oparin, cellule prebiotiche, Cufom, Cun, Extremamente, Luis Elizondo, Tom DeLonge>>\n",
      "\n",
      "Similar Doc-->(doctag:35,score:0.9998975992202759):<<Nuovi dispositivi in arrivo da Amazon, c'è anche un forno a microonde - Repubblica.it>>\n",
      "\n",
      "Similar Doc-->(doctag:121,score:0.999893844127655):<<Mai più batterie che prendono fuoco>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning:\n",
      "\n",
      "Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take the first example in the test set,\n",
    "# and see which document is the most similar\n",
    "\n",
    "inferred_vector = model.infer_vector(test_corpus[5])\n",
    "similar_docs = model.docvecs.most_similar([inferred_vector], topn=3)\n",
    "\n",
    "# show the 3 most similar document titles\n",
    "print('Test Document : «{}»\\n'.format( ' '.join(test_corpus[5])))\n",
    "for doc_tag, similarity in similar_docs:\n",
    "    print(\"\\nSimilar Doc-->(doctag:{0},score:{1}):<<{2}>>\".format(doc_tag, similarity, titles[doc_tag]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
