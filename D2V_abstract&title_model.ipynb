{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA imports\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec imports\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import gensim\n",
    "\n",
    "import json # to open our data file\n",
    "DATA_FILENAME = \"trend_analisys.json\"\n",
    "# open json file\n",
    "with open(DATA_FILENAME, \"r\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "# we're expecting a list now, since our json file is a json array\n",
    "assert type(json_data) is list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicates from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates:  85\n",
      "New length:  208\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i, dictionary in enumerate(json_data):\n",
    "    try:\n",
    "        index = json_data.index(dictionary, i+1, len(json_data))\n",
    "        #print(\"Found a duplicate with index {0} from index {1}\".format(index, i))\n",
    "        del(json_data[index])\n",
    "        counter = counter + 1\n",
    "    except ValueError:\n",
    "        None\n",
    "print(\"Number of duplicates: \", counter)\n",
    "print(\"New length: \", len(json_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1 - Training Model with Abstract field (whole text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 208, number of train examples: 208, number of test examples: 0\n"
     ]
    }
   ],
   "source": [
    "# we have our json data now, let's go ahead and divide into training and test set\n",
    "n_examples =  len(json_data)\n",
    "# how much of the data we're going to be using for training and for testing\n",
    "# default values: 80% train, 20% test\n",
    "#TRAIN_DATA_LENGTH = 9 * n_examples // 10\n",
    "## UPDATE: let's train on the whole dataset\n",
    "TRAIN_DATA_LENGTH = n_examples\n",
    "TEST_DATA_LENGTH = n_examples - TRAIN_DATA_LENGTH\n",
    "ABSTRACT_FIELD_NAME = 'abstract'\n",
    "TITLE_FIELD_NAME = 'title'\n",
    "\n",
    "print(\"Total examples: {0}, number of train examples: {1}, number of test examples: {2}\".format(n_examples,TRAIN_DATA_LENGTH, TEST_DATA_LENGTH))\n",
    "\n",
    "# TODO: Randomize selection of examples, don't just take the first ones\n",
    "# build training corpus: take the needed abstract, preprocess them (tokenize, delete spaces..)\n",
    "# and create the TaggedDocument needed for training\n",
    "# also added title to it \n",
    "train_corpus = [gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(\n",
    "    d[TITLE_FIELD_NAME]+d[ABSTRACT_FIELD_NAME]), [i]) for i, d in enumerate(json_data) if i<TRAIN_DATA_LENGTH]\n",
    "\n",
    "#test_corpus = [gensim.utils.simple_preprocess(\n",
    "#    d[TITLE_FIELD_NAME]+d[ABSTRACT_FIELD_NAME]) for i, d in enumerate(json_data) if i>TRAIN_DATA_LENGTH]\n",
    "assert len(train_corpus)==TRAIN_DATA_LENGTH\n",
    "#print(train_corpus[:1])\n",
    "\n",
    "## let's also shuffle train set\n",
    "from random import shuffle\n",
    "#train_corpus = shuffle(train_corpus[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train model using Skip-Gram training\n",
    "credits for model improvements: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow,d100,n5,mc2,t4)\n",
      "Doc2Vec(\"alpha=0.1\",dm/m,d100,n5,w10,mc2,t4)\n",
      "Vocabulary created!\n",
      "Training Doc2Vec(dbow,d100,n5,mc2,t4)\n",
      "CPU times: user 7.31 s, sys: 80 ms, total: 7.39 s\n",
      "Wall time: 2.56 s\n",
      "Training Doc2Vec(\"alpha=0.1\",dm/m,d100,n5,w10,mc2,t4)\n",
      "CPU times: user 12 s, sys: 104 ms, total: 12.1 s\n",
      "Wall time: 4.37 s\n",
      "Models Saved\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "\n",
    "# let's try training two models at once: Paragraph Vector - Distributed Memory (PV-DM), just like CBOW to W2V\n",
    "# and Paragraph Vector - Distributed Bag of Words (PV-DBOW), analogous to W2V Skip-gram\n",
    "epochs = 40\n",
    "vec_size = 100\n",
    "alpha = 0.10  # default= 0.030\n",
    "MODEL_NAME = \"Models/d2v_TA_abstract&title\"\n",
    "\n",
    "models = [\n",
    "    # PV-DBOW plain\n",
    "    Doc2Vec(dm=0, vector_size=vec_size, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=epochs, workers=cores),\n",
    "    # PV-DM w/ default averaging; a higher starting alpha may improve CBOW/PV-DM modes\n",
    "    Doc2Vec(dm=1, vector_size= vec_size, window=10, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs= epochs, workers=cores, alpha= alpha, comment='alpha=0.1'),\n",
    "]\n",
    "\n",
    "# build our vocabulary of words (all the unique words encountered inside our corpus, needed for training)\n",
    "for model in models:\n",
    "    print(model)\n",
    "    model.build_vocab(train_corpus)\n",
    "print(\"Vocabulary created!\")\n",
    "\n",
    "# train the models on the given data!\n",
    "counter = 0\n",
    "for model in models:\n",
    "    print(\"Training %s\" % model)\n",
    "    %time model.train(train_corpus, total_examples=len(train_corpus), epochs=model.epochs)\n",
    "    model.save(MODEL_NAME+str(counter)+'.model')\n",
    "    counter = counter + 1\n",
    "print(\"Models Saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Data\n",
    "credits: https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try to visualize all document_vectors\n",
    "# get all vectors of documents we created from model training\n",
    "docs_vecs = []\n",
    "model = models[0]\n",
    "# docvecs (list of Doc2VecKeyedVectors) \n",
    "# – Vector representations of the documents in the corpus. Each vector has size == vector_size\n",
    "for doc in iter(range(0, len(model.docvecs))):\n",
    "    docs_vecs.append(model.docvecs[doc])\n",
    "\n",
    "# loading dataset into Pandas DataFrame\n",
    "df = pd.DataFrame.from_records(docs_vecs)\n",
    "#df.head()\n",
    "\n",
    "#df[['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA is effected by scale so you need to scale the features in your data before applying PCA. \n",
    "features = [i for i in range(vec_size)]\n",
    "\n",
    "x = df.loc[:, features].values # get features values\n",
    "#print(x)\n",
    "# we don't have target here y = df.loc[:,['target']].values # get target values (guess kind of flower/Iris)\n",
    "\n",
    "# standardize data\n",
    "x = StandardScaler().fit_transform(x) # scale data (especially in case different measures are used)\n",
    "# pd.DataFrame(data = x, columns = features).head() # show first data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Projection with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build PCA model in 2D\n",
    "pca = PCA(n_components=2) # The new components are just the two main dimensions of variation.\n",
    "\n",
    "principalComponents = pca.fit_transform(x)\n",
    "\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "principalDf.head()\n",
    "# these components drawn don't hold a lot of information 'per-se', they're just the result \n",
    "# of dimension-reduction\n",
    "\n",
    "finalDf = principalDf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Data interactively with Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "tls.set_credentials_file(username='D4nt3', api_key='FdMB4O6qCfciGDOnLvdQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~D4nt3/30.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll draw a scatter graph with labels\n",
    "traces = []\n",
    "# let's get the labels\n",
    "titles = [dictionary[TITLE_FIELD_NAME] for i, dictionary in enumerate(json_data) if i<TRAIN_DATA_LENGTH]\n",
    "# double check to be sure we got labels just right\n",
    "#i = 50\n",
    "#print(\"TITLE: {0}\\n <<{1}>>\".format(titles[i], train_corpus[i][:1]))\n",
    "\n",
    "# each trace will represent a point (squeezed vector from higher dimensions),\n",
    "# and each point will have the title of the news assigned\n",
    "for i in range(len(finalDf)):\n",
    "    trace0 = go.Scatter(\n",
    "        x = finalDf.loc[i:i, \"principal component 1\"],\n",
    "        y = finalDf.loc[i:i, \"principal component 2\"],\n",
    "        mode = 'markers',\n",
    "        #name = 'blue markers',\n",
    "        marker = dict(\n",
    "            size = 7,\n",
    "            color = 'rgba(0, 0, 110, .8)',\n",
    "        ),\n",
    "        text = str(titles[i])\n",
    "    )\n",
    "    traces.append(trace0)\n",
    "\n",
    "data = traces \n",
    "layout = dict(title = 'PCA Representantion of DocVectors',\n",
    "        hovermode= 'closest',\n",
    "        xaxis= dict(\n",
    "            title= 'first component',\n",
    "            ticklen= 5,\n",
    "            gridwidth= 2,\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title= 'second component',\n",
    "            ticklen= 5,\n",
    "            gridwidth= 2,\n",
    "        ),\n",
    "        showlegend = False\n",
    "    )\n",
    "# Plot and embed in ipython notebook!\n",
    "fig = dict(data = data, layout = layout)\n",
    "py.iplot(fig, filename='TA_model_title&abstract_NOKMEANS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means clustering on PCA reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=600,\n",
       "    n_clusters=5, n_init=15, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try another way for clustering data: K-Mean, an even more popular algorithm,\n",
    "# which I know from the introductory course on AI, so it might be better \n",
    "# to utilize algorithms which I know and can talk about in the presentation\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# build k-means model\n",
    "kmeans = KMeans(n_clusters = 5, max_iter=600, algorithm = 'auto', verbose=0,\n",
    "               init='k-means++', n_init=15) \n",
    "\n",
    "kmeans.fit(principalComponents) # data, as vectors of documents (in 2D)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing K-Means results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~D4nt3/28.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "from scipy.spatial import distance\n",
    "\n",
    "tls.set_credentials_file(username='D4nt3', api_key='FdMB4O6qCfciGDOnLvdQ')\n",
    "\n",
    "COMPONENT_ONE = \"principal component 1\"\n",
    "COMPONENT_TWO = \"principal component 2\"\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# each trace will represent a point (squeezed vector from higher dimensions),\n",
    "# and each point will have the title of the news assigned\n",
    "for i in range(len(finalDf)):\n",
    "    # assign a color to each point belonging to a specific cluster\n",
    "    # computing distance from centroid\n",
    "    x = finalDf.loc[i:i, \"principal component 1\"]\n",
    "    y = finalDf.loc[i:i, \"principal component 2\"]\n",
    "    color = 'rgba(0, 0, 180, 0.8)'\n",
    "    min_d = 10000\n",
    "    closest_centroid = np.array([])\n",
    "    \n",
    "    for centroid in centroids:\n",
    "        #dist = np.linalg.norm(centroid-np.array(x, y))\n",
    "        dist = distance.euclidean(np.array(x,y), centroid)\n",
    "        #print(\"Distance from centroid: \",dist)\n",
    "        if dist<min_d:\n",
    "            min_d = dist\n",
    "            closest_centroid = centroid\n",
    "    #print(\"Prediction: \",closest_centroid)\n",
    "    if np.array_equal(closest_centroid, centroids[0]):\n",
    "        color = 'blue'\n",
    "    elif np.array_equal(closest_centroid, centroids[1]):\n",
    "        color = 'pink'\n",
    "    elif np.array_equal(closest_centroid, centroids[2]):\n",
    "        color = 'yellow'\n",
    "    elif np.array_equal(closest_centroid, centroids[3]):\n",
    "        color = 'green'\n",
    "    else:\n",
    "        color = 'black'\n",
    "        \n",
    "    trace0 = go.Scatter(\n",
    "        x = x, \n",
    "        y = y,\n",
    "        mode = 'markers',\n",
    "            #name = 'blue markers',\n",
    "        marker = dict(\n",
    "            size = 7,\n",
    "            color = color,\n",
    "        ),\n",
    "        text = str(titles[i])\n",
    "    )\n",
    "    traces.append(trace0)\n",
    "\n",
    "# draw centroids\n",
    "c_trace = go.Scatter(\n",
    "    x = centroids[:, 0],\n",
    "    y = centroids[:, 1],\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        size = 9,\n",
    "        color = 'red',\n",
    "    ),\n",
    "    text = 'centroid'\n",
    ")\n",
    "traces.append(c_trace)\n",
    "\n",
    "data = traces \n",
    "layout = dict(title = 'PCA Representantion of DocVectors',\n",
    "            hovermode= 'closest',\n",
    "            xaxis= dict(\n",
    "                title= 'first component',\n",
    "                ticklen= 5,\n",
    "                gridwidth= 2,\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                title= 'second component',\n",
    "                ticklen= 5,\n",
    "                gridwidth= 2,\n",
    "            ),\n",
    "            showlegend = False\n",
    "        )\n",
    "# Plot and embed in ipython notebook!\n",
    "    \n",
    "fig = dict(data = data, layout = layout)\n",
    "py.iplot(fig, filename='TA_model_title&abstract_kmeans_model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.17454237 -0.70935933]\n",
      " [-0.56029901 -3.30869754]\n",
      " [ 3.49757356 -0.83890783]\n",
      " [ 1.59265357  2.13376192]\n",
      " [-2.57751057  1.48418982]]\n"
     ]
    }
   ],
   "source": [
    "print(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some (basic) testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (3): «tempesta su titano le immagini di cassini repubblica itsembra il deserto del sahara nel bel mezzo di una tempesta di sabbia invece siamo su titano nubi di polvere spazzano equatore della più grande luna di saturno nelle immagini catturate dalla sonda cassini che più di un anno dalla sua uscita di scena continua regalare indimenticabili cartoline dallo spazio secondo lo studio pubblicato nature geoscience firmato dal team dell università paris diderot coordinato da sebastien rodriguez la somiglianza con la terra non solo in apparenza visto che dai dati raccolti titano risulta geologicamente attivo le sue dune sono simili alle nostre quelle di marte oltre essere una luna molto attiva spiega rodriguez titano risulta simile marte anche per il ciclo della polvere cumuli spostati dal vento su grandi distanze fanno sì che si formino dune che restituiscono un panorama già visto sulle terre più aride comprese quelle del pianeta rosso grazie alle immagini infrarossi scattate dalla sonda nel ricercatori hanno scoperto la tempesta di polvere che ha reso titano il terzo corpo celeste del sistema solare in cui sono state osservati fenomeni del genere»\n",
      "\n",
      "\n",
      "Similar Doc-->(doctag:3,score:0.9677678346633911):<<Tempesta su Titano: le immagini di Cassini - Repubblica.it>>\n",
      "\n",
      "Similar Doc-->(doctag:123,score:0.7422125935554504):<<La più grande luna di Saturno sotto una tempesta di polvere - Spazio & Astronomia - ANSA.it>>\n",
      "\n",
      "Similar Doc-->(doctag:148,score:0.6271712779998779):<<Surface Hub 2 al debutto dal 2019, con una seconda versione nel 2020>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning:\n",
      "\n",
      "Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's check if the model is at least decent,\n",
    "# which means: is it able to at least recognize news/documents\n",
    "# it has seen in training?\n",
    "import random\n",
    "\n",
    "# Pick a random document from the train corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(train_corpus) - 1)\n",
    "\n",
    "inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "similar_docs = model.docvecs.most_similar([inferred_vector], topn=3)\n",
    "\n",
    "# show the 3 most similar document titles\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "for doc_tag, similarity in similar_docs:\n",
    "    print(\"\\nSimilar Doc-->(doctag:{0},score:{1}):<<{2}>>\".format(doc_tag, similarity, titles[doc_tag]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some more basic testing on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-e8e75f7752f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# and see which document is the most similar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0minferred_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msimilar_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minferred_vector\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "# take the first example in the test set,\n",
    "# and see which document is the most similar\n",
    "\n",
    "inferred_vector = model.infer_vector(test_corpus[5])\n",
    "similar_docs = model.docvecs.most_similar([inferred_vector], topn=3)\n",
    "\n",
    "# show the 3 most similar document titles\n",
    "print('Test Document : «{}»\\n'.format( ' '.join(test_corpus[5])))\n",
    "for doc_tag, similarity in similar_docs:\n",
    "    print(\"\\nSimilar Doc-->(doctag:{0},score:{1}):<<{2}>>\".format(doc_tag, similarity, titles[doc_tag]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
