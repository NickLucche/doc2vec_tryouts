{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try different models-compare clustering results over eval. set\n",
    "TODO: try different eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Number of clusters: 2\n",
      "##Cluster length: ['3', '20']\n",
      "### Doc length:  [10558, 3702, 2439, 6075, 1886, 19646, 4117, 1547, 2014, 6584, 10788, 2512, 2375, 3694, 4614, 2758, 5397, 5287, 9861, 3475, 4289, 4561, 6182]\n"
     ]
    }
   ],
   "source": [
    "# load eval.set (duplicates free)\n",
    "import json\n",
    "filename = 'kmeans_negative_test_0noise.json'\n",
    "#filename = '/home/nick/anaconda3/bin/Tirocinio/doc2vec_tryouts/EvaluateModels/english_4_clusters.json'\n",
    "#filename = '/home/nick/anaconda3/bin/Tirocinio/doc2vec_tryouts/EvaluateModels/pre-clustered_docs_harder.json'\n",
    "with open(filename, 'r') as file:\n",
    "    cdocs = json.load(file)\n",
    "print(\"#Number of clusters:\",len(cdocs))\n",
    "print(\"##Cluster length:\",[str(len(cluster)) for cluster in cdocs])\n",
    "print(\"### Doc length: \",[len(doc['headline']+doc['bodyText']) for cluster in cdocs for doc in cluster \n",
    "                          if not(doc['bodyText'] is None)])\n",
    "#print(\"### Doc length: \",[len(doc['title']+doc['abstract']) for cluster in cdocs for doc in cluster])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title+abstract models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(\"dm=0, vec = 500\",dbow,d500,n5,mc2,t4) -VocabSize: 1875\n",
      "Doc2Vec(\"dm=0, vec = 800\",dbow,d800,n5,mc2,t4) -VocabSize: 1875\n",
      "Doc2Vec(\"dm=0, vec = 800, negative=12\",dbow,d800,n12,mc2,t4) -VocabSize: 1875\n",
      "Vocabulary created!\n",
      "Training Doc2Vec(\"dm=0, vec = 500\",dbow,d500,n5,mc2,t4)\n",
      "CPU times: user 3 s, sys: 0 ns, total: 3 s\n",
      "Wall time: 1.61 s\n",
      "Training Doc2Vec(\"dm=0, vec = 800\",dbow,d800,n5,mc2,t4)\n",
      "CPU times: user 4.08 s, sys: 0 ns, total: 4.08 s\n",
      "Wall time: 2.1 s\n",
      "Training Doc2Vec(\"dm=0, vec = 800, negative=12\",dbow,d800,n12,mc2,t4)\n",
      "CPU times: user 8.28 s, sys: 0 ns, total: 8.28 s\n",
      "Wall time: 4.1 s\n"
     ]
    }
   ],
   "source": [
    "# train different models\n",
    "from gensim.utils import simple_preprocess as sp\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import multiprocessing\n",
    "\n",
    "unwrapped_docs = [doc for cluster in cdocs for doc in cluster]\n",
    "# title + abstract models\n",
    "train_corpus_ta = [ TaggedDocument(sp(doc['headline'] + doc['bodyText']) ,[i]) for i, doc in enumerate(unwrapped_docs)]\n",
    "#train_corpus_ta = [ TaggedDocument(sp(doc['title'] + doc['abstract']) ,[i]) for i, doc in enumerate(unwrapped_docs)]\n",
    "epochs = 45\n",
    "vec_size = 100\n",
    "models = [\n",
    "    # dm = 0, simple SG, simpler model, most of the time efficient and accurate\n",
    "    Doc2Vec(dm=0, vector_size=500, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=45, workers=multiprocessing.cpu_count(), comment='dm=0, vec = 500'),\n",
    "    Doc2Vec(dm=0, vector_size=800, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=45, workers=multiprocessing.cpu_count(), comment='dm=0, vec = 800'),\n",
    "    Doc2Vec(dm=0, vector_size=800, negative=12, hs=0, min_count=2, sample=0, \n",
    "            epochs=40, workers=multiprocessing.cpu_count(), comment='dm=0, vec = 800, negative=12'),\n",
    "]\n",
    "\n",
    "# build our vocabulary of words (all the unique words encountered inside our corpus)\n",
    "for model in models:\n",
    "    model.build_vocab(train_corpus_ta)\n",
    "    print(model, \"-VocabSize:\", len(model.wv.vocab))\n",
    "print(\"Vocabulary created!\")\n",
    "\n",
    "# train the models on the given data!\n",
    "counter = 0\n",
    "for model in models:\n",
    "    print(\"Training %s\" % model)\n",
    "    %time model.train(train_corpus_ta, total_examples=len(train_corpus_ta), epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans\n",
    "## Choosing K (highlight the right k in red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~D4nt3_/12.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "titles = [doc['headline'] for doc in unwrapped_docs]\n",
    "#titles = [doc['title'] for doc in unwrapped_docs]\n",
    "# let's try Kmeans with K between 2 and 20\n",
    "def run_KMeans_K_times(model, K):\n",
    "    doc_vecs = [model.docvecs[j] for j in range(len(model.docvecs))]\n",
    "    sse = []\n",
    "    kmeans_results = []\n",
    "    # run k means with k between 2 and K\n",
    "    for i in range(2, K+1):\n",
    "        kmeans = KMeans(n_clusters=i, init='k-means++', # smart-way of choosing starting point\n",
    "               n_init= 50, # run k-means 100 times and get the best result in terms of SSE\n",
    "                n_jobs = -1 # use every core\n",
    "        )\n",
    "        # compute sse for this kmeans result\n",
    "        kmeans.fit(doc_vecs)\n",
    "        sse.append(kmeans.inertia_)\n",
    "        \n",
    "        # get 'visual' clustering, by saving a list of titles\n",
    "        clusters = {label: [] for label in kmeans.labels_}\n",
    "        for label, list_ in clusters.items():\n",
    "            for i, l in enumerate(kmeans.labels_):\n",
    "                if label==l:\n",
    "                    clusters[label].append(titles[i])\n",
    "        \n",
    "        kmeans_results.append(clusters)\n",
    "    return kmeans_results, sse\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "# using my api-key\n",
    "tls.set_credentials_file(username='D4nt3_', api_key='4O71urldgOueVtcApOdX')\n",
    "graph_name = 'kmeans_choosing_k'\n",
    "\n",
    "kmeans_results, sse = run_KMeans_K_times(models[1], 23)\n",
    "K = 20 # max_k\n",
    "# plot results of K and SSE\n",
    "trace = go.Scatter(\n",
    "    x = [k for k in range(2, K)],\n",
    "    y = sse,\n",
    "    mode = 'lines+markers'\n",
    ")\n",
    "layout = dict(title = 'Choosing K in Kmeans',\n",
    "              xaxis = dict(title = 'K value'),\n",
    "              yaxis = dict(title = 'SSE'),\n",
    "              )\n",
    "\n",
    "data = [trace]\n",
    "fig = dict(data=data, layout=layout)\n",
    "py.iplot(fig, filename = graph_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Kmeans\n",
    "On KMeans best conditions: 4 clusters, no-noise, equal-size in clusters, (approximately) same density, (unsure about shape).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results occurences(correct guess, cluster index):  [(3, 0), (16, 1)]\n",
      "Accuracy (Precision) over each cluster:  [42.857142857142854, 100.0]\n",
      "Accuracy (Recall) over each cluster:  [100.0, 80.0]\n",
      "Precision score: 71.42857142857143, Recall score: 90.0\n",
      "#Number of clusters found: 2, against number of pre-computed clusters: 2#\n",
      "\n",
      "Results occurences(correct guess, cluster index):  [(2, 0), (18, 0)]\n",
      "Accuracy (Precision) over each cluster:  [10.0, 90.0]\n",
      "Accuracy (Recall) over each cluster:  [66.66666666666667, 90.0]\n",
      "Precision score: 50.0, Recall score: 78.33333333333334\n",
      "#Number of clusters found: 2, against number of pre-computed clusters: 2#\n",
      "\n",
      "Results occurences(correct guess, cluster index):  [(3, 0), (11, 0)]\n",
      "Accuracy (Precision) over each cluster:  [21.428571428571427, 78.57142857142857]\n",
      "Accuracy (Recall) over each cluster:  [100.0, 55.0]\n",
      "Precision score: 50.0, Recall score: 77.5\n",
      "#Number of clusters found: 2, against number of pre-computed clusters: 2#\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import my_dbscan\n",
    "import model_evaluation as me\n",
    "\n",
    "clusters = []\n",
    "models_precision_kmeans = []\n",
    "models_recall_kmeans = []\n",
    "expected_clusters = []\n",
    "for docs in cdocs:\n",
    "    #expected_clusters.append([doc['title'] for doc in docs])\n",
    "    expected_clusters.append([doc['headline'] for doc in docs])\n",
    "        \n",
    "for model in models:\n",
    "    # K already chosen before, with value 4\n",
    "    cluster_results, sse = run_KMeans_K_times(model, 2)\n",
    "    #cluster_results, sse = run_KMeans_K_times(model, 10)\n",
    "    cluster_results = cluster_results[-1] # get 4-means results\n",
    "    clist = []\n",
    "    for key, cluster in cluster_results.items():\n",
    "        clist.append(cluster)\n",
    "    \n",
    "    clusters.append(clist)\n",
    "    \n",
    "     # evaluate clustering\n",
    "    precision, recall = me.compute_clustering_accuracy(clist, expected_clusters)\n",
    "    print('Precision score: %s, Recall score: %s'%(precision, recall))\n",
    "    \n",
    "    models_precision_kmeans.append(precision)\n",
    "    models_recall_kmeans.append(recall)\n",
    "    # last check: we want to penalize models that simply cluster all docs together (that's not a valid result)\n",
    "    # that's way we have precision score\n",
    "    print(\"#Number of clusters found: {0}, against number of pre-computed clusters: {1}#\\n\".format(\n",
    "        len(clist), len(cdocs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['First woman: Smithsonian Air and Space director looks from the moon to Mars', 'The first human on Mars should be a woman – we deserve stardust too', 'Forgotten heroes of the first world war', 'Overlord review – nasty second world war action-horror fantasy', 'Reflections on the first world war and Armistice Day', 'Thousands of Ypres sculptures to commemorate world war one dead', 'Black soldiers’ role in the first world war', 'Second world war winners and the appeal of Dad’s Army', 'Transcription by Kate Atkinson review – second world war spying hijinks', 'Bells will ring out: world to mark end of first world war, 100 years on', 'Second world war pilot Mary Ellis dies aged 101', 'London Sinfonietta/George Benjamin\\xa0review – austere first world war meditation', 'Long-lost photo album of first world war soldier given to his family', 'Empire Cinemas rejects first world war short film by Bible Society', 'When heroes of the first world war made playing fields out of battlefields', 'Relief and reckoning: the first world war was over and Australia counted the cost', '11-11: Memories Retold review – a first world war game in which no shots are fired', 'Poppy appeal has raised £1 every second since first world war', 'The animal victims of the first world war are a stain on our conscience', \"'It was very hard for him': relatives remember first world war survivors\"], [\"Trump's moon shot might be steered by a woman, says Nasa chief\", \"'Very aggressive': Trump suggests Montenegro could cause world war three\", 'Trump to meet Putin and Erdoğan during first world war ceremonies']]\n"
     ]
    }
   ],
   "source": [
    "print(clusters[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Clustering DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results occurences(correct guess, cluster index):  [(0, 3), (3, 3)]\n",
      "Accuracy (Precision) over each cluster:  [0, 100.0]\n",
      "Accuracy (Recall) over each cluster:  [0, 15.0]\n",
      "Precision score: 50.0, Recall score: 7.5\n",
      "#Number of clusters found: 4, against number of pre-computed clusters: 2#\n",
      "\n",
      "Results occurences(correct guess, cluster index):  [(3, 0), (11, 1)]\n",
      "Accuracy (Precision) over each cluster:  [100.0, 100.0]\n",
      "Accuracy (Recall) over each cluster:  [100.0, 55.0]\n",
      "Precision score: 100.0, Recall score: 77.5\n",
      "#Number of clusters found: 4, against number of pre-computed clusters: 2#\n",
      "\n",
      "Results occurences(correct guess, cluster index):  [(2, 2), (7, 3)]\n",
      "Accuracy (Precision) over each cluster:  [100.0, 100.0]\n",
      "Accuracy (Recall) over each cluster:  [66.66666666666667, 35.0]\n",
      "Precision score: 100.0, Recall score: 50.833333333333336\n",
      "#Number of clusters found: 4, against number of pre-computed clusters: 2#\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import my_dbscan\n",
    "import model_evaluation as me\n",
    "\n",
    "models_recall = []\n",
    "models_precision = []\n",
    "clusters_found = [] # keep clusters results, they're useful later on\n",
    "min_s = 2 # min_samples\n",
    "titles = [doc['headline'] for doc in unwrapped_docs]\n",
    "urls = titles\n",
    "expected_clusters = []\n",
    "for docs in cdocs:\n",
    "    expected_clusters.append([doc['headline'] for doc in docs])\n",
    "#urls = [doc['url'] for doc in unwrapped_docs]\n",
    "for k, model in enumerate(models):\n",
    "    # try different eps for some model\n",
    "    if k == 1:\n",
    "        eps = 0.41\n",
    "        eps_increment = 0.13\n",
    "        min_s = 2\n",
    "    elif k==6:\n",
    "        eps = 0.11\n",
    "        eps_increment = 0.13\n",
    "        min_s = 3\n",
    "    else:\n",
    "        eps = 0.11\n",
    "        eps_increment = 0.13\n",
    "        min_s = 2\n",
    "    # get list of document vectors\n",
    "    doc_vecs = [model.docvecs[j] for j in range(len(model.docvecs))]\n",
    "    \n",
    "    # apply dbscan clustering to these vectors\n",
    "    titles_clusters = my_dbscan.apply_dbscan(doc_vecs = doc_vecs, titles = titles, \n",
    "                                               urls = urls, subset_length = len(titles),\n",
    "                                                 eps = eps, eps_increment = eps_increment,\n",
    "                                               n_iterations = 3, verbose = False, min_samples = min_s)\n",
    "    # get clusters as list of titles\n",
    "    #titles_clusters = utils.getDocTitleFromUrl(unwrapped_docs, urls_cluster_list)\n",
    "    clusters_found.append(titles_clusters)\n",
    "    \n",
    "    # evaluate clustering\n",
    "\n",
    "    precision, recall = me.compute_clustering_accuracy(titles_clusters, expected_clusters)\n",
    "    print('Precision score: %s, Recall score: %s'%(precision, recall))\n",
    "    \n",
    "    models_precision.append(precision)\n",
    "    models_recall.append(recall)\n",
    "    # last check: we want to penalize models that simply cluster all docs together (that's not a valid result)\n",
    "    # that's way we have precision score\n",
    "    print(\"#Number of clusters found: {0}, against number of pre-computed clusters: {1}#\\n\".format(\n",
    "        len(titles_clusters), len(cdocs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First woman: Smithsonian Air and Space director looks from the moon to Mars', 'The first human on Mars should be a woman – we deserve stardust too', \"Trump's moon shot might be steered by a woman, says Nasa chief\"]\n",
      "['Forgotten heroes of the first world war', 'Overlord review – nasty second world war action-horror fantasy', 'Reflections on the first world war and Armistice Day', 'Black soldiers’ role in the first world war', 'Second world war winners and the appeal of Dad’s Army', 'Transcription by Kate Atkinson review – second world war spying hijinks', 'Second world war pilot Mary Ellis dies aged 101', 'London Sinfonietta/George Benjamin\\xa0review – austere first world war meditation', 'Trump to meet Putin and Erdoğan during first world war ceremonies', 'Relief and reckoning: the first world war was over and Australia counted the cost', '11-11: Memories Retold review – a first world war game in which no shots are fired']\n",
      "['Thousands of Ypres sculptures to commemorate world war one dead', 'Bells will ring out: world to mark end of first world war, 100 years on', 'Long-lost photo album of first world war soldier given to his family', 'When heroes of the first world war made playing fields out of battlefields', 'Poppy appeal has raised £1 every second since first world war', 'The animal victims of the first world war are a stain on our conscience', \"'It was very hard for him': relatives remember first world war survivors\"]\n",
      "[\"'Very aggressive': Trump suggests Montenegro could cause world war three\", 'Empire Cinemas rejects first world war short film by Bible Society']\n"
     ]
    }
   ],
   "source": [
    "for cluster in clusters_found[1]:\n",
    "    print(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~D4nt3_/14.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "#doc_vecs = [models[1].docvecs[j] for j in range(len(models[1].docvecs))]\n",
    "# finally, call eps-estimate function\n",
    "title_dist_tuples = utils.choose_eps(2, doc_vecs, titles)\n",
    "data = utils.visualize_eps_graph(title_dist_tuples=title_dist_tuples)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "tls.set_credentials_file(username='D4nt3_', api_key='4O71urldgOueVtcApOdX')\n",
    "py.iplot(data, filename='step-graph-eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize results graphically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~D4nt3/76.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "# using my api-key\n",
    "tls.set_credentials_file(username='D4nt3', api_key='FdMB4O6qCfciGDOnLvdQ')\n",
    "graph_name = 'model_eval_results'\n",
    "\n",
    "model_descr = [model.comment for model in models]\n",
    "# plot test-accuracy results, plus silhoutte scores (times 100, since all the values are in between 0-1)\n",
    "trace0 = go.Bar(\n",
    "    x = model_descr + ['loaded model'],\n",
    "    y = models_precision,\n",
    "    name='Precision',\n",
    "    marker=dict(\n",
    "        color='rgb(49,130,189)'\n",
    "    )\n",
    ")\n",
    "trace1 = go.Bar(\n",
    "    x = model_descr + ['loaded model'],\n",
    "    y = [s*100 for s in s_scores] + [0],\n",
    "    name='Silhoutte score',\n",
    "    marker=dict(\n",
    "        color='rgb(204,50,100)',\n",
    "    )\n",
    "    \n",
    ")\n",
    "\n",
    "trace2 = go.Bar(\n",
    "    x = model_descr + ['loaded model'],\n",
    "    y = models_recall,\n",
    "    name='Recall',\n",
    "    marker=dict(\n",
    "        color='rgb(155, 244, 66)',\n",
    "    )\n",
    "    \n",
    ")\n",
    "# precision, recall, silhoutte\n",
    "data = [trace0, trace2,  trace1]\n",
    "layout = go.Layout(\n",
    "    title = 'Clustering Accuracy Results',\n",
    "    xaxis=dict(\n",
    "        tickfont=dict(\n",
    "            size=10,\n",
    "            color='rgb(107, 107, 107)',\n",
    "            \n",
    "        ),\n",
    "        tickangle = -45\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Clustering accuracy (%)',\n",
    "        titlefont=dict(\n",
    "            size=16,\n",
    "            color='rgb(107, 107, 107)'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            size=14,\n",
    "            color='rgb(107, 107, 107)'\n",
    "        )\n",
    "    ),\n",
    "   \n",
    "    barmode='group',\n",
    "    bargap=0.2,\n",
    "    bargroupgap=0.1\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename=graph_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silhoutte score\n",
    "Silhouette coefficient combines ideas of both cohesion and separation, \n",
    "but for individual points, as well as clusters and \n",
    "clusterings; the Silhouette Coefficient is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample.\n",
    "The Silhoutte score is merely the average of each silhoutte coefficient, computed over each sample.\n",
    "It's a measure of the goodness of clustering, by assuming the fact that a cluster X is defined good if both every sample inside it is close to each other, and far from any other relatively-near cluster Y.\n",
    "This is not necessarily true for every shape of cluster.\n",
    "\n",
    "    The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12614417\n",
      "0.19104731\n",
      "0.1858033\n",
      "0.15527193\n",
      "0.1609296\n",
      "-0.023030862\n",
      "0.17550677\n",
      "0.19581276\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "s_scores = []\n",
    "for model in models:\n",
    "    # get clusters for each model\n",
    "    doc_vecs = [model.docvecs[j] for j in range(len(model.docvecs))]\n",
    "    titles = [doc['title'] for doc in unwrapped_docs]\n",
    "    urls = [doc['url'] for doc in unwrapped_docs]\n",
    "    # apply dbscan clustering to these vectors\n",
    "    urls_cluster_list = my_dbscan.apply_dbscan(doc_vecs = doc_vecs, titles = titles, \n",
    "                                               urls = urls, subset_length = len(titles),\n",
    "                                                 eps = 0.11, eps_increment = 0.13, n_iterations = 3, verbose = False)\n",
    "    \n",
    "    # get cluster labels, mantaining original docs ordering\n",
    "    labels = []\n",
    "    for doc in unwrapped_docs:\n",
    "        noise = True\n",
    "        for i, url_list in enumerate(urls_cluster_list):\n",
    "            if(doc['url'] in url_list):\n",
    "                labels.append(i) # keep cluster id\n",
    "                noise = False\n",
    "        if noise:\n",
    "            labels.append(-1)\n",
    "    # make sure they have the same size\n",
    "    assert len(labels) == len(doc_vecs)\n",
    "    ss = silhouette_score(doc_vecs, labels , metric='cosine')\n",
    "    s_scores.append(ss)\n",
    "    print(ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions: \n",
    "A really low silhoutte score helps us identify models that tend to have very few cluster, hence not really recognizing differences between docs. This is fundamental, since test rules used so far prevent us from recognizing these kinds of models.\n",
    "Since silhoutte score is an average of silhoutte coefficients, smaller changes of values here may mean greater differences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
