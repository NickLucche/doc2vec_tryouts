{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test models on live-like version documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in new json:  96\n",
      "Number of documents in new json:  604\n"
     ]
    }
   ],
   "source": [
    "# fetch data\n",
    "import json\n",
    "filenames = ['blockchain.json', 'industria_4.0.json']\n",
    "\n",
    "with open(filenames[0], 'r') as outfile:\n",
    "    json_data = json.load(outfile)\n",
    "#print(\"Length of the json file: {0}, type: {1}\".format(len(json_data), type(json_data)))\n",
    "\n",
    "## let's now retrieve the meaningful part of the json document\n",
    "# response{}--->docs[]\n",
    "\n",
    "docs = json_data['response']['docs']\n",
    "print(\"Number of documents in new json: \",len(docs))\n",
    "\n",
    "# open file 2 and do the same things\n",
    "with open(filenames[1], 'r') as outfile:\n",
    "    json_data = json.load(outfile)\n",
    "\n",
    "docs = docs + json_data['response']['docs']\n",
    "print(\"Number of documents in new json: \",len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New length after removing docs:  412\n"
     ]
    }
   ],
   "source": [
    "## many documents have a failed abstract, let's remove them\n",
    "to_check = ' Questo sito web utilizza cookie tecnici e, previo Suo consenso, cookie di profilazione,'\n",
    "docs = [doc for i, doc in enumerate(docs) if not(to_check.strip() in doc['abstract'][0].strip())]\n",
    "\n",
    "print(\"New length after removing docs: \", len(docs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Length:  384\n",
      "['Una roadmap per rilanciare Impresa 4.0, nove tappe che disegnano una via per rilanciare il Piano Impresa 4.0 e supportare più efficacemente i processi di trasformazione digitale delle imprese italiane. Realizzata dal Digital Transformation Institute, la Roadmap Impresa 4.0 ha l’obiettivo di guidare la riflessione sulla strategia da adottare nei prossimi mesi e ribadire che non bisogna abbassare la guardia, perché il nostro sistema imprenditoriale ha necessità di essere supportato in questo complesso processo di cambiamento.\\n“Ma perché il cambiamento avvenga e riesca a produrre un effetto positivo, è necessario che le imprese, le istituzioni, tutti abbiano consapevolezza delle caratteristiche e delle dinamiche della rivoluzione in corso – spiega Stefano Epifani, presidente dell’Istituto e docente di Internet Studies alla Sapienza, Università di Roma – È necessaria la consapevolezza degli impatti, delle opportunità e delle minacce che comportano i nuovi scenari aperti dai processi trasformativi attuati da elementi come i big data, l’intelligenza artificiale, l’internet of things, i social media, la sharing economy, Blockchain, Industry 4.0 e tanti altri concetti. Ma soprattutto consapevolezza del reale senso della dinamica trasformativa della rivoluzione in corso. Parlare di trasformazione digitale, infatti, non può e non deve ridursi a parlare di digitalizzazione”.\\nLa roadmap, alla realizzazione della quale hanno contribuito tanto i membri dell’istituto quanto un panel esterno di esperti di riferimento del settore, parte dalla consapevolezza del fatto che “il Piano Impresa 4.0 ha aperto una strada importante nella direzione del supporto allo sviluppo dei processi di trasformazione digitale, avviando un complesso percorso di collaborazione inter istituzionale e favorendo la creazione di una rete di supporto all’innovazione” come afferma Alberto Marinelli, professore di Media Digitali alla Sapienza e socio fondatore dell’Istituto.\\n“Per questo motivo – continua Kurt Hilgenberg, Professore di Internet Studies al Politecnico di Berlino – è importante dare continuità all’azione avviata, portando tanto la dotazione finanziaria quanto l’impatto dell’azione in termini di coinvolgimento delle Istituzioni ai livelli dei best performer europei”\\nTuttavia, sostiene Gianfranco Vento – anch’egli tra i fondatori del Digital Transformation Institute e professore di Economia alla Regent’s University di Londra – “la sfida del digitale si vince, prima di tutto, con le competenze e con la consapevolezza nelle imprese. Creazione di consapevolezza e sviluppo di competenze sono due strade obbligate da percorrere. Per questo motivo è importante sviluppare programmi finalizzati alla costruzione di competenze tanto nei cittadini quanto nelle aziende e nell’Amministrazione”.\\nCosa si può fare quindi nel concreto, valorizzando quanto è già stato realizzato? “In primo luogo, promuovere percorsi finalizzati a sviluppare una profonda consapevolezza sul ruolo del digitale nel business e nell’economia da parte di imprese ed istituzioni – dice Roberto Lippi, coordinatore di UN Habitat, il programma delle Nazioni Unite che si occupa di sviluppo urbano sostenibile –  Ma non solo. È necessario supportare con più convinzione la trasformazione digitale nei giovani, attraverso un sistema di formazione professionale duale, che preveda un percorso di reale alternanza tra teoria e pratica che faccia comprendere davvero logiche, processi e modelli del mondo del lavoro: creare consapevolezza è l’unico modo di promuovere un percorso di sviluppo sostenibile, che produca reale benessere nella popolazione”.\\nConsapevolezza dunque, ma senza dimenticare la normativa e le infrastrutture di rete. L’ampliamento della banda larga, tanto nei centri urbani che nelle zone remote, è fondamentale per favorire l’inclusione sociale ed economica, sviluppare i servizi, rendere il nostro Paese competitivo a livello globale. Così come disegnare norme flessibili, che non siano di ostacolo allo sviluppo, ma siano capaci di supportare i nuovi attori ed i nuovi modelli senza penalizzare quelli esistenti.\\nCosa migliorare del Piano Impresa 4.0? Di certo, semplificare lo schema d’azione. Punti Impresa Digitale, Centri di Competenza, Centri di Trasferimento Tecnologico, Digital Innovation Hub: troppi operatori a supporto dell’innovazione con compiti differenziati e funzioni alle volte sovrapposte. Poi bisogna ripartire dalla natura delle imprese del nostro Paese, che non è solo industriale. Inoltre, come sottolinea Alfonso Fuggetta, direttore del Cefriel, “focalizzare su velocità, certezza e semplicità dei provvedimenti. Servono strumenti automatici che consentano alle aziende di mettere in campo innovazione pianificando le azioni in maniera veloce e rapida, senza ostacoli da parte della burocrazia o complessi processi di selezione”. Qualsiasi strategia si metta in campo deve fornire un set di strumenti e servizi che supportino efficacemente anche il variegato e complesso ecosistema delle PMI nel rispondere alla sfida di Impresa 4.0. Infine, fare sistema in questo momento appare indispensabile. Per cogliere la scommessa di Impresa 4.0 e sviluppare un Network di Impresa 4.0 inclusivo rispetto a università, impresa e scuola, la collaborazione è una priorità se si vuole rendere la trasformazione digitale un’efficace azione di sistema.']\n"
     ]
    }
   ],
   "source": [
    "## Adjust data format\n",
    "for i, dictionary in enumerate(docs):\n",
    "    for field in ['title', 'abstract', 'flattened_entities']:\n",
    "        if isinstance(dictionary[field], list):\n",
    "            # re-format data to hold string instead of single-list item\n",
    "            docs[i][field] = dictionary[field][0]\n",
    "# remove duplicates\n",
    "for i, doc in enumerate(docs):\n",
    "    if \"Industry 4.0 (o industria 4.0): cos'è, notizie, normative, casi studio - I4T\" in doc['title']:\n",
    "        del(docs[i])\n",
    "\"\"\"\n",
    "duplicates_indeces = []\n",
    "for i, doc in enumerate(docs):\n",
    "    for j in range(i+1, len(docs)):\n",
    "        if docs[j]['title'] == doc['title']:\n",
    "            duplicates_indeces.append(j)\n",
    "print(\"Number of duplicates: \", len(duplicates_indeces))\n",
    "docs = [doc for i, doc in enumerate(docs) if not(i in duplicates_indeces)]\n",
    "\"\"\"\n",
    "print(\"New Length: \", len(docs))\n",
    "\n",
    "## randomize everything by shuffling the documents around\n",
    "import random\n",
    "random.shuffle(docs)\n",
    "\n",
    "for doc in docs:\n",
    "    if to_check.strip() in doc['abstract'].strip():\n",
    "        print(\"cookie doc found\")\n",
    "print([d['abstract'] for d in docs[:1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's first try to infer vector from model; if that doesn't work much, let's train another model with this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9031\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import  gensim\n",
    "\n",
    "MODEL_NAME = 'TestModels/d2v_TA_abstract&title0.model'\n",
    "MODEL_TWO = 'Models/d2v_TA_abstract&title0.model'\n",
    "model = Doc2Vec.load(MODEL_NAME)\n",
    "#model = Doc2Vec.load(MODEL_TWO)\n",
    "inferred_vectors = []\n",
    "# print out dimension of the vocabulary \n",
    "print(len(model.wv.vocab))\n",
    "#print(model.most_similar(positive=['re', 'donna'], negative=['uomo']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "# infer vectors from data\n",
    "test_corpus = [gensim.utils.simple_preprocess(d['title']+d['abstract']) for d in docs]\n",
    "print(len(test_corpus))\n",
    "inferred_vectors = [model.infer_vector(doc) for doc in test_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def perform_dbscan(eps = 0.4, min_samples = 4, metric = 'euclidean', algorithm = 'auto', data = None, verbose = True\n",
    "                  , titles = None, print_noise = True):\n",
    "    \"\"\"perform DBSCAN over given data, using given parametrs. Returns dbscan object.\"\"\"\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples, metric=metric, algorithm=algorithm).fit(data)\n",
    "\n",
    "    #print(\"Core samples: \")\n",
    "    #for i in db.core_sample_indices_ :\n",
    "    #    print(titles[i]+\"\\n\")\n",
    "\n",
    "    # labels will print out the number of the cluster each example belongs to;\n",
    "    # -1 if the vector is considered noise (not belonging to any cluster)\n",
    "    #print(\"Labels: \", db.labels_)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"##Clusters##\")\n",
    "        cluster = [[]]\n",
    "        noise = []\n",
    "        noise_r = []\n",
    "        for i, label in enumerate(db.labels_):\n",
    "            if label != -1:\n",
    "                try:\n",
    "                    cluster[label].append(titles[i])\n",
    "                except Exception as e:\n",
    "                    cluster.append([titles[i]])\n",
    "            else:\n",
    "                noise.append(titles[i])\n",
    "                noise_r.append(i)\n",
    "        for list_ in cluster:\n",
    "            print(\"Cluster:\", list_)\n",
    "        if print_noise:\n",
    "            print(\"Noise: \", noise)\n",
    "\n",
    "        print(\"DBSCAN finished.\\n\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Impresa 4.0, roadmap in nove tappe. Ma la priorità sono le Pmi - CorCom', 'Industria 4.0, primi risultati dopo 4 anni di evangelizzazione - Industry4Business', \"Blockchain: cos'è, come funziona ed applicazioni nell'IoT - I4T\", 'Industria 4.0, Sacconi: \"Serve un welfare 4.0 per spingere l\\'occupazione\" - CorCom', \"Cos'è, come fare ed esempi concreti di Industria 4.0 - Internet4Things\"]\n"
     ]
    }
   ],
   "source": [
    "# get docs titles\n",
    "titles = [doc['title'] for doc in docs]\n",
    "print(titles[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental DBSCAN over small subset\n",
    "## TODO: check if noise is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Clusters##\n",
      "Cluster: ['Skill e competenze per la digital servitization - Industry4Business', \"Telecom: 5G, sicurezza e piattaforme per l'IoT\", \"Che cos'è TrustedChain e perché può cambiare la logica di gestione delle transazioni grazie alla Blockchain - Blockchain 4innovation\", \"Cos'è, come fare ed esempi concreti di Industria 4.0 - Internet4Things\", 'Abbanoa punta sulla blockchain per certificare la lettura dei contatori - Blockchain 4innovation', 'Blockchain e Industry 4.0: Sap Leonardo porta la Data Intelligence nella Smart Factory - Blockchain 4innovation', 'SpidChain, identità digitale 4.0 per PA e aziende - Blockchain 4innovation', \"Industry 4.0 (o industria 4.0): cos'è, notizie, normative, casi studio - I4T\", 'Ecco tutte le tecnologie Industry 4.0 prorogate dalla legge di Stabilità', 'Legal e blockchain - Blockchain 4innovation', \"Industry 4.0 (o industria 4.0): cos'è, notizie, normative, casi studio - I4T\", \"Industry 4.0 (o industria 4.0): cos'è, notizie, normative, casi studio - I4T\", \"Dallo Smart Manufacturing all'Industria 4.0: la nuova era per il manifatturiero parte dall'Industrial IoT - Industry4Business\", 'Osservatorio Industria 4.0: Industrial IoT, Analytics e Cloud Manufacturing spingono il mercato a 2,4 Mld con un +30% - Industry4Business', \"Tech Data entra nell'alleanza globale per l'IoT\", \"Industry 4.0 (o industria 4.0): cos'è, notizie, normative, casi studio - I4T\"]\n",
      "Noise:  [\"Industria 4.0, si rafforza l'asse Confindustria-Ubi Banca - CorCom\", 'Deloitte-Dnv GL, nasce la certificazione blockchain: “È solo l’inizio” - Blockchain 4innovation', 'Paradigma Industry X.0: parte da Modena la nuova rivoluzione del made in Italy - CorCom', 'La blockchain piace al Web, ma dietro l’angolo c\\'è il pericolo \"bolla\" - Blockchain 4innovation', 'Blockchain & Bitcoin: una guida per capire e per orientarsi dedicata ai lettori di MilanoFinanza - Blockchain 4innovation', 'Moreschini (Microsoft): Blockchain e Cloud accoppiata vincente per la PA - Blockchain 4innovation', 'Industria 4.0, così finisce il \"diritto pesante\" del lavoro - CorCom', 'Sharp Europe punta su Iot e intelligenza artificiale: Bob Ishida nuovo Ceo - CorCom', 'Tassa sui robot, Bentivogli: \"Per l\\'Italia sarebbe un boomerang\" - CorCom']\n",
      "DBSCAN finished.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# let's try and find other clusters in the noise data, with higher eps\\nnoise_data = [subset[i] for i, label in enumerate(db.labels_) if label==-1]\\nnoise_titles = [subset_titles[i] for i, label in enumerate(db.labels_) if label==-1]\\n\\ndb = perform_dbscan(eps = eps + eps_increment, min_samples = 2, metric = 'cosine', algorithm = 'auto',\\n                    data = noise_data, verbose = True, titles = noise_titles, print_noise = False)\\n\\nnoise_data = [subset[i] for i, label in enumerate(db.labels_) if label==-1]\\nnoise_titles = [subset_titles[i] for i, label in enumerate(db.labels_) if label==-1]\\n\\ndb = perform_dbscan(eps = eps + eps_increment + eps_increment, min_samples = 2, metric = 'cosine', algorithm = 'auto',\\n                    data = noise_data, verbose = True, titles = noise_titles)\""
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_length = 25\n",
    "# subset of docs vectors \n",
    "subset = inferred_vectors[:subset_length]\n",
    "subset_titles = titles[:subset_length]\n",
    "\n",
    "eps = 0.25\n",
    "eps_increment = .15\n",
    "db = perform_dbscan(eps = 0.45, min_samples = 2, metric = 'cosine', algorithm = 'auto',\n",
    "                    data = subset, verbose = True, titles = subset_titles, print_noise = True)\n",
    "\"\"\"\n",
    "# let's try and find other clusters in the noise data, with higher eps\n",
    "noise_data = [subset[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "noise_titles = [subset_titles[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "\n",
    "db = perform_dbscan(eps = eps + eps_increment, min_samples = 2, metric = 'cosine', algorithm = 'auto',\n",
    "                    data = noise_data, verbose = True, titles = noise_titles, print_noise = False)\n",
    "\n",
    "noise_data = [subset[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "noise_titles = [subset_titles[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "\n",
    "db = perform_dbscan(eps = eps + eps_increment + eps_increment, min_samples = 2, metric = 'cosine', algorithm = 'auto',\n",
    "                    data = noise_data, verbose = True, titles = noise_titles)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Clusters##\n",
      "Cluster: ['Industria 4.0, Sap: \"Pmi italiane sulla via della digitalizzazione\" - CorCom', \"Dalle parole ai fatti, ecco la via italiana all'industria 4.0 - CorCom\", \"Industria 4.0, si rafforza l'asse Confindustria-Ubi Banca - CorCom\", 'Industria 4.0, al via la sfida della Campania - CorCom']\n",
      "Cluster: ['Italtel e Rold uniscono le forze per affermarsi nell’Industria 4.0', 'Industria 4.0 e blockchain - Pagina 4 di 5 - Blockchain 4innovation']\n",
      "Cluster: [\"EY: ambiti applicativi e roadmap per l'industria 4.0 in Italia\", 'Cisco, Alascom, Fanuc: una robotica collaborativa basata (anche) sul linguaggio naturale - Industry4Business']\n",
      "Noise:  ['Innovazione 4.0, competence center e digital innovation hub', \"Almaviva rilancia su Industria 4.0: nel Polo Meccatronica la nuova sede per l'R&S - CorCom\", 'Internet of Things alla prova degli utenti, serve più AI per renderlo \"infallibile\" - CorCom', 'IoT, Migliarina (Vodafone): \"Così le telco andranno oltre la connettività\" - CorCom', 'Se Blockchain fa rima con supply chain: la killer application che il mercato aspettava? - Blockchain 4innovation', \"Olivetti, IoT per l'ambiente in mostra al G7 di Taormina - CorCom\", \"Dall'IoT ai big data, la svolta 4.0 di Black&Decker - CorCom\", \"Industry 4.0 (o industria 4.0): cos'è, notizie, normative, casi studio - I4T\", 'Ansaldo Energia lancia la call \"Digital X Factory\" - CorCom', 'MecSpe 2018, in mostra a Parma la via italiana a Industria 4.0', \"Arriva da LorenzoVinci.it il food in chiave Blockchain di Genuino. Con ICO all'orizzonte - Blockchain 4innovation\", \"Taglio del nastro per Co+Factory, l'hub 4.0 per startup e Pmi - CorCom\"]\n",
      "DBSCAN finished.\n",
      "\n",
      "##Clusters##\n",
      "Cluster: ['Innovazione 4.0, competence center e digital innovation hub', \"Almaviva rilancia su Industria 4.0: nel Polo Meccatronica la nuova sede per l'R&S - CorCom\", 'IoT, Migliarina (Vodafone): \"Così le telco andranno oltre la connettività\" - CorCom', 'Se Blockchain fa rima con supply chain: la killer application che il mercato aspettava? - Blockchain 4innovation', \"Olivetti, IoT per l'ambiente in mostra al G7 di Taormina - CorCom\", \"Dall'IoT ai big data, la svolta 4.0 di Black&Decker - CorCom\", \"Industry 4.0 (o industria 4.0): cos'è, notizie, normative, casi studio - I4T\", 'Ansaldo Energia lancia la call \"Digital X Factory\" - CorCom', 'MecSpe 2018, in mostra a Parma la via italiana a Industria 4.0', \"Arriva da LorenzoVinci.it il food in chiave Blockchain di Genuino. Con ICO all'orizzonte - Blockchain 4innovation\", \"Taglio del nastro per Co+Factory, l'hub 4.0 per startup e Pmi - CorCom\"]\n",
      "Noise:  ['Internet of Things alla prova degli utenti, serve più AI per renderlo \"infallibile\" - CorCom']\n",
      "DBSCAN finished.\n",
      "\n",
      "##Clusters##\n",
      "Cluster: []\n",
      "Noise:  ['Internet of Things alla prova degli utenti, serve più AI per renderlo \"infallibile\" - CorCom']\n",
      "DBSCAN finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subset_length = 20\n",
    "# subset of docs vectors \n",
    "subset = inferred_vectors[:subset_length]\n",
    "subset_titles = titles[:subset_length]\n",
    "\n",
    "eps = 0.25\n",
    "eps_increment = 0.13\n",
    "# starting eps will be the sum of eps + eps_increment \n",
    "for i in range(3):\n",
    "    eps = eps + eps_increment\n",
    "    # decrease eps_increment a bit \n",
    "    #eps_increment = eps_increment - .02\n",
    "    db = perform_dbscan(eps = eps, min_samples = 2, metric = 'cosine', algorithm = 'auto',\n",
    "                        data = subset, verbose = True, titles = subset_titles, print_noise = True)\n",
    "\n",
    "    # let's try and find other clusters in the noise data, with higher eps\n",
    "    subset = [subset[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "    subset_titles = [subset_titles[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "    if subset is None:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model Approach\n",
    "other idea, use entities as tags!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train corpus:  351\n",
      "Doc2Vec(\"live data\",dbow,d100,n5,mc2,t4)\n",
      "Doc2Vec(\"alpha=0.1-live data\",dm/m,d100,n5,w10,mc2,t4)\n",
      "Vocabulary created!\n",
      "Training Doc2Vec(\"live data\",dbow,d100,n5,mc2,t4)\n",
      "CPU times: user 33.4 s, sys: 220 ms, total: 33.6 s\n",
      "Wall time: 11.5 s\n",
      "Training Doc2Vec(\"alpha=0.1-live data\",dm/m,d100,n5,w10,mc2,t4)\n",
      "CPU times: user 51.9 s, sys: 204 ms, total: 52.1 s\n",
      "Wall time: 15.9 s\n",
      "Models Saved\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import  gensim\n",
    "# get train corpus\n",
    "train_corpus = [gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(\n",
    "    d['title']+d['abstract']), [i]) for i, d in enumerate(docs) ]\n",
    "print(\"Length of train corpus: \",len(train_corpus))\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "\n",
    "# let's try training two models at once: Paragraph Vector - Distributed Memory (PV-DM), just like CBOW to W2V\n",
    "# and Paragraph Vector - Distributed Bag of Words (PV-DBOW), analogous to W2V Skip-gram\n",
    "epochs = 45\n",
    "vec_size = 100\n",
    "alpha = 0.10  # default= 0.030\n",
    "MODEL_NAME = \"Models_Live_Test/d2v_abstract&title\"\n",
    "\n",
    "models = [\n",
    "    # PV-DBOW plain\n",
    "    Doc2Vec(dm=0, vector_size=vec_size, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=epochs, workers=cores, comment='live data'),\n",
    "    # PV-DM w/ default averaging; a higher starting alpha may improve CBOW/PV-DM modes\n",
    "    Doc2Vec(dm=1, vector_size= vec_size, window=10, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs= epochs, workers=cores, alpha= alpha, comment='alpha=0.1-live data'),\n",
    "]\n",
    "\n",
    "# build our vocabulary of words (all the unique words encountered inside our corpus, needed for training)\n",
    "for model in models:\n",
    "    print(model)\n",
    "    model.build_vocab(train_corpus)\n",
    "print(\"Vocabulary created!\")\n",
    "\n",
    "# train the models on the given data!\n",
    "counter = 0\n",
    "for model in models:\n",
    "    print(\"Training %s\" % model)\n",
    "    %time model.train(train_corpus, total_examples=len(train_corpus), epochs=model.epochs)\n",
    "    model.save(MODEL_NAME+str(counter)+'.model')\n",
    "    counter = counter + 1\n",
    "print(\"Models Saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental DBSCAN over model vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"tag '351' not seen in training corpus/invalid\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-db2ce726b5e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msubset_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdocvecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvec\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# inferred vectors should result in the same vec as above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#inferred_vectors  = [model.infer_vector(doc.words) for i, doc in enumerate(train_corpus) if i<subset_length]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-db2ce726b5e4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msubset_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdocvecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvec\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# inferred vectors should result in the same vec as above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#inferred_vectors  = [model.infer_vector(doc.words) for i, doc in enumerate(train_corpus) if i<subset_length]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m   1529\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_int_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoctags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_rawint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1531\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tag '%s' not seen in training corpus/invalid\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"tag '351' not seen in training corpus/invalid\""
     ]
    }
   ],
   "source": [
    "# load model\n",
    "modelname = 'Models_Live_Test/d2v_abstract&title0.model'\n",
    "model = Doc2Vec.load(modelname)\n",
    "\n",
    "subset_length = 25\n",
    "print(len(model.docvecs))\n",
    "docvecs = [vec for vec in model.docvecs]\n",
    "# inferred vectors should result in the same vec as above\n",
    "#inferred_vectors  = [model.infer_vector(doc.words) for i, doc in enumerate(train_corpus) if i<subset_length]\n",
    "\n",
    "# subset of docs vectors \n",
    "subset = docvecs[:subset_length]\n",
    "subset_titles = titles[:subset_length]\n",
    "\n",
    "eps = 0.35\n",
    "eps_increment = .15\n",
    "db = perform_dbscan(eps = eps, min_samples = 2, metric = 'cosine', algorithm = 'auto',\n",
    "                    data = subset, verbose = True, titles = subset_titles)\n",
    "\n",
    "# let's try and find other clusters in the noise data, with higher eps\n",
    "noise_data = [subset[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "noise_titles = [subset_titles[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "\n",
    "db = perform_dbscan(eps = eps + eps_increment, min_samples = 2, metric = 'cosine', algorithm = 'auto',\n",
    "                    data = noise_data, verbose = True, titles = noise_titles)\n",
    "\n",
    "# let's try and find other clusters in the noise data, with higher eps\n",
    "noise_data = [subset[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "noise_titles = [subset_titles[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "\n",
    "db = perform_dbscan(eps = eps + eps_increment + 0.1, min_samples = 2, metric = 'cosine', algorithm = 'auto',\n",
    "                    data = noise_data, verbose = True, titles = noise_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize clusters over whole data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA imports\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline\n",
    "\n",
    "# load model\n",
    "MODEL_NAME = 'TestModels/d2v_TA_abstract&title0.model'\n",
    "MODEL_TWO = 'Models/d2v_TA_abstract&title0.model'\n",
    "#model = Doc2Vec.load(MODEL_NAME)\n",
    "model = Doc2Vec.load(MODEL_TWO)\n",
    "\n",
    "inferred_vectors = [model.infer_vector(doc) for doc in test_corpus]\n",
    "# loading dataset into Pandas DataFrame\n",
    "df = pd.DataFrame.from_records(inferred_vectors)\n",
    "\n",
    "# PCA is effected by scale so you need to scale the features in your data before applying PCA. \n",
    "vec_size = 100\n",
    "features = [i for i in range(vec_size)]\n",
    "\n",
    "x = df.loc[:, features].values # get features values\n",
    "\n",
    "# standardize data\n",
    "x = StandardScaler().fit_transform(x) # scale data (especially in case different measures are used)\n",
    "# build PCA model in 2D\n",
    "pca = PCA(n_components=2) # The new components are just the two main dimensions of variation.\n",
    "\n",
    "principalComponents = pca.fit_transform(x)\n",
    "\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "finalDf = principalDf \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~D4nt3/44.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "from scipy.spatial import distance\n",
    "\n",
    "tls.set_credentials_file(username='D4nt3', api_key='FdMB4O6qCfciGDOnLvdQ')\n",
    "\n",
    "COMPONENT_ONE = \"principal component 1\"\n",
    "COMPONENT_TWO = \"principal component 2\"\n",
    "#centroids = kmeans.cluster_centers_\n",
    "titles = [dictionary['title'] for dictionary in docs]\n",
    "traces = []\n",
    "\n",
    "# each trace will represent a point (squeezed vector from higher dimensions),\n",
    "# and each point will have the title of the news assigned\n",
    "for i in range(len(finalDf)):\n",
    "    # assign a color to each point belonging to a specific cluster\n",
    "    # computing distance from centroid\n",
    "    #x = finalDf.loc[i:i, \"principal component 1\"]\n",
    "    #y = finalDf.loc[i:i, \"principal component 2\"]\n",
    "    x , y = finalDf.iat[i, 0], finalDf.iat[i, 1]\n",
    "    color = 'rgba(0, 0, 180, 0.8)'\n",
    "    \"\"\"\n",
    "    centroid_index = kmeans.predict([[x, y]])\n",
    "    closest_centroid = centroids[centroid_index]\n",
    "    #print(closest_centroid, centroids[0])\n",
    "    if np.array_equal(closest_centroid, [centroids[0]]):\n",
    "        color = 'blue'\n",
    "    elif np.array_equal(closest_centroid, [centroids[1]]):\n",
    "        color = 'pink'\n",
    "    elif np.array_equal(closest_centroid, [centroids[2]]):\n",
    "        color = 'yellow'\n",
    "    elif np.array_equal(closest_centroid, [centroids[3]]):\n",
    "        color = 'green'\n",
    "    else:\n",
    "        color = 'black'\n",
    "    \"\"\"\n",
    "    \n",
    "    trace0 = go.Scatter(\n",
    "        x = [x], \n",
    "        y = [y],\n",
    "        mode = 'markers',\n",
    "            #name = 'blue markers',\n",
    "        marker = dict(\n",
    "            size = 7,\n",
    "            color = color,\n",
    "        ),\n",
    "        text = str(titles[i])\n",
    "    )\n",
    "    traces.append(trace0)\n",
    "\n",
    "# draw centroids\n",
    "\"\"\"\n",
    "c_colors = ['blue', 'pink', 'yellow', 'green', 'black']\n",
    "for i in range(len(centroids)):\n",
    "    c_trace = go.Scatter(\n",
    "        x = [centroids[i, 0]],\n",
    "        y = [centroids[i, 1]],\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 9,\n",
    "            color = 'red',\n",
    "        ),\n",
    "        text = c_colors[i]\n",
    "    )\n",
    "    traces.append(c_trace)\n",
    "\"\"\"\n",
    "data = traces \n",
    "layout = dict(title = 'PCA Representantion of D2V on Title+Abstract',\n",
    "            hovermode= 'closest',\n",
    "            xaxis= dict(\n",
    "                title= 'first component',\n",
    "                ticklen= 5,\n",
    "                gridwidth= 2,\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                title= 'second component',\n",
    "                ticklen= 5,\n",
    "                gridwidth= 2,\n",
    "            ),\n",
    "            showlegend = False\n",
    "        )\n",
    "# Plot and embed in ipython notebook!\n",
    "    \n",
    "fig = dict(data = data, layout = layout)\n",
    "py.iplot(fig, filename='live-test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means using SSE as tool to evaluate clustering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
