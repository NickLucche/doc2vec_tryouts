{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try different models-compare clustering results over eval. set\n",
    "TODO: try different eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Number of clusters: 10\n",
      "##Cluster length: ['4', '3', '3', '2', '10', '9', '3', '4', '10', '6']\n",
      "### Doc length:  [2210, 468, 2675, 1694, 1947, 1761, 1575, 1812, 1967, 2672, 2588, 2933, 950, 2218, 5673, 3585, 9133, 941, 1467, 9253, 2409, 2247, 3021, 1083, 1003, 1564, 2256, 1198, 2015, 8371, 1183, 3088, 1240, 1200, 1172, 1869, 2315, 3287, 1997, 2162, 2764, 996, 930, 877, 4216, 2068, 1992, 11060, 1157, 1350, 1225, 1978, 5780, 7297]\n"
     ]
    }
   ],
   "source": [
    "# load eval.set (duplicates free)\n",
    "import json\n",
    "filename = 'pre-clustered_docs_harder.json'\n",
    "#filename = 'pre_clustered_10_english.json'\n",
    "with open(filename, 'r') as file:\n",
    "    cdocs = json.load(file)\n",
    "print(\"#Number of clusters:\",len(cdocs))\n",
    "print(\"##Cluster length:\",[str(len(cluster)) for cluster in cdocs])\n",
    "print(\"### Doc length: \",[len(doc['title']+doc['abstract']) for cluster in cdocs for doc in cluster \n",
    "                          if not(doc['abstract'] is None)])\n",
    "#print(\"### Doc length: \",[len(doc['headline']+doc['bodyText']) for cluster in cdocs for doc in cluster])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title+abstract models\n",
    "## TODO: lower case text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Doc2Vec(\"dm=0, vec = 100\",dbow,d100,n5,mc2,t4) -VocabSize: 2104\n",
      "Doc2Vec(\"dm=0, vec = 500, +Eps\",dbow,d500,n5,mc2,t4) -VocabSize: 2104\n",
      "Doc2Vec(\"dm=0, vec = 800\",dbow,d800,n5,mc2,t4) -VocabSize: 2104\n",
      "Doc2Vec(\"dm=0, vec = 800, negative=10\",dbow,d800,n10,mc2,t4) -VocabSize: 2104\n",
      "Doc2Vec(\"dm=1, vec=100, alpha=0.05\",dm/m,d100,n5,w10,mc2,t4) -VocabSize: 2104\n",
      "Doc2Vec(\"dm=1, vec=150, alpha=0.5, hs=1\",dm/m,d150,n5,w5,mc2,s0.001,t4) -VocabSize: 2104\n",
      "Doc2Vec(\"dm=1, vec=500, lower eps, min_pts=3\",dm/m,d500,n5,w10,mc2,t4) -VocabSize: 2104\n",
      "Doc2Vec(\"dm=1, vec=500, alpha=.05\",dm/m,d500,n5,w10,mc2,t4) -VocabSize: 2104\n",
      "Vocabulary created!\n",
      "Training Doc2Vec(\"dm=0, vec = 100\",dbow,d100,n5,mc2,t4)\n",
      "CPU times: user 1.62 s, sys: 0 ns, total: 1.62 s\n",
      "Wall time: 907 ms\n",
      "Training Doc2Vec(\"dm=0, vec = 500, +Eps\",dbow,d500,n5,mc2,t4)\n",
      "CPU times: user 3.24 s, sys: 16 ms, total: 3.25 s\n",
      "Wall time: 1.7 s\n",
      "Training Doc2Vec(\"dm=0, vec = 800\",dbow,d800,n5,mc2,t4)\n",
      "CPU times: user 5.5 s, sys: 40 ms, total: 5.54 s\n",
      "Wall time: 2.75 s\n",
      "Training Doc2Vec(\"dm=0, vec = 800, negative=10\",dbow,d800,n10,mc2,t4)\n",
      "CPU times: user 8.6 s, sys: 32 ms, total: 8.63 s\n",
      "Wall time: 4.19 s\n",
      "Training Doc2Vec(\"dm=1, vec=100, alpha=0.05\",dm/m,d100,n5,w10,mc2,t4)\n",
      "CPU times: user 2.77 s, sys: 16 ms, total: 2.78 s\n",
      "Wall time: 1.45 s\n",
      "Training Doc2Vec(\"dm=1, vec=150, alpha=0.5, hs=1\",dm/m,d150,n5,w5,mc2,s0.001,t4)\n",
      "CPU times: user 1.73 s, sys: 20 ms, total: 1.75 s\n",
      "Wall time: 962 ms\n",
      "Training Doc2Vec(\"dm=1, vec=500, lower eps, min_pts=3\",dm/m,d500,n5,w10,mc2,t4)\n",
      "CPU times: user 5.92 s, sys: 56 ms, total: 5.98 s\n",
      "Wall time: 2.92 s\n",
      "Training Doc2Vec(\"dm=1, vec=500, alpha=.05\",dm/m,d500,n5,w10,mc2,t4)\n",
      "CPU times: user 6.47 s, sys: 28 ms, total: 6.5 s\n",
      "Wall time: 3.17 s\n"
     ]
    }
   ],
   "source": [
    "# train different models\n",
    "from gensim.utils import simple_preprocess as sp\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import multiprocessing\n",
    "\n",
    "unwrapped_docs = [doc for cluster in cdocs for doc in cluster]\n",
    "# title + abstract models\n",
    "train_corpus_ta = [ TaggedDocument(sp(doc['title'] + doc['abstract']) ,[i]) for i, doc in enumerate(unwrapped_docs)]\n",
    "#train_corpus_ta = [ TaggedDocument(sp(doc['headline'] + doc['bodyText']) ,[i]) for i, doc in enumerate(unwrapped_docs)]\n",
    "epochs = 45\n",
    "vec_size = 100\n",
    "models = [\n",
    "    # dm = 0, simple SG, simpler model, most of the time efficient and accurate\n",
    "    Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=45, workers=multiprocessing.cpu_count(), comment='dm=0, vec = 100'),\n",
    "    Doc2Vec(dm=0, vector_size=500, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=45, workers=multiprocessing.cpu_count(), comment='dm=0, vec = 500, +Eps'),\n",
    "    Doc2Vec(dm=0, vector_size=800, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=45, workers=multiprocessing.cpu_count(), comment='dm=0, vec = 800'),\n",
    "    Doc2Vec(dm=0, vector_size=800, negative=10, hs=0, min_count=2, sample=0, \n",
    "            epochs=40, workers=multiprocessing.cpu_count(), comment='dm=0, vec = 800, negative=10'),\n",
    "    # dm = 1, CBOW equivalent \n",
    "    Doc2Vec(dm=1, vector_size= 100, window=10, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs = epochs, workers=multiprocessing.cpu_count(), alpha= 0.05, comment='dm=1, vec=100, alpha=0.05'),\n",
    "    Doc2Vec(dm=1, vector_size= 150, min_count=2,epochs = epochs, \n",
    "            workers=multiprocessing.cpu_count(), alpha= 0.5, comment='dm=1, vec=150, alpha=0.5, hs=1'),\n",
    "    Doc2Vec(dm=1, vector_size= 500, window=10, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs = epochs, workers=multiprocessing.cpu_count(), alpha= 0.05, comment='dm=1, vec=500'),\n",
    "    Doc2Vec(dm=1, vector_size= 500, window=10, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs = epochs, workers=multiprocessing.cpu_count(), alpha= 0.05, comment='dm=1, vec=500, alpha=.05')\n",
    "]\n",
    "# also evaluate result on currently used model\n",
    "loaded_model = Doc2Vec.load('/home/nick/anaconda3/bin/Tirocinio/doc2vec_tryouts/Models_Live_Test/d2v_abstract&title0.model')\n",
    "print(\"Model loaded\")\n",
    "\n",
    "# build our vocabulary of words (all the unique words encountered inside our corpus)\n",
    "for model in models:\n",
    "    model.build_vocab(train_corpus_ta)\n",
    "    print(model, \"-VocabSize:\", len(model.wv.vocab))\n",
    "print(\"Vocabulary created!\")\n",
    "\n",
    "# train the models on the given data!\n",
    "counter = 0\n",
    "for model in models:\n",
    "    print(\"Training %s\" % model)\n",
    "    %time model.train(train_corpus_ta, total_examples=len(train_corpus_ta), epochs=model.epochs)\n",
    "    #model.save(MODEL_NAME+str(counter)+'.model')\n",
    "    counter = counter + 1\n",
    "#print(\"Models Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Precision) over each cluster:  [53.33333333333333, 75.0, 60.0, 100.0, 100.0, 83.33333333333333, 32.5, 66.66666666666666, 66.66666666666666, 50.0]\n",
      "Accuracy (Recall) over each cluster:  [50.0, 100.0, 49.99999999999999, 100.0, 60.0, 25.925925925925924, 49.99999999999999, 50.0, 20.0, 33.33333333333332]\n",
      "Precision score: 68.74999999999999, Recall score: 53.925925925925924\n",
      "#Number of clusters found: 13, against number of pre-computed clusters: 10#\n",
      "\n",
      "Accuracy (Precision) over each cluster:  [25.0, 25.0, 100.0, 100.0, 64.28571428571428, 100.0, 15.476190476190474, 33.33333333333333, 54.232804232804234, 47.22222222222222]\n",
      "Accuracy (Recall) over each cluster:  [75.0, 100.0, 100.0, 100.0, 50.0, 100.0, 49.99999999999999, 100.0, 33.333333333333336, 41.66666666666666]\n",
      "Precision score: 56.45502645502645, Recall score: 75.0\n",
      "#Number of clusters found: 8, against number of pre-computed clusters: 10#\n",
      "\n",
      "Accuracy (Precision) over each cluster:  [100.0, 75.0, 100.0, 100.0, 73.33333333333333, 87.5, 32.5, 60.0, 47.5, 60.0]\n",
      "Accuracy (Recall) over each cluster:  [50.0, 100.0, 100.0, 100.0, 26.666666666666668, 25.0, 49.99999999999999, 50.0, 17.5, 33.33333333333332]\n",
      "Precision score: 73.58333333333333, Recall score: 55.250000000000014\n",
      "#Number of clusters found: 16, against number of pre-computed clusters: 10#\n",
      "\n",
      "Accuracy (Precision) over each cluster:  [35.0, 30.0, 62.5, 100.0, 77.77777777777777, 75.0, 26.666666666666664, 27.5, 62.5, 58.33333333333333]\n",
      "Accuracy (Recall) over each cluster:  [50.0, 100.0, 49.99999999999999, 100.0, 26.666666666666668, 44.44444444444445, 49.99999999999999, 50.0, 20.0, 24.999999999999993]\n",
      "Precision score: 55.527777777777786, Recall score: 51.61111111111111\n",
      "#Number of clusters found: 13, against number of pre-computed clusters: 10#\n",
      "\n",
      "Accuracy (Precision) over each cluster:  [26.666666666666668, 20.0, 53.333333333333336, 100.0, 61.11111111111111, 100.0, 13.333333333333334, 100.0, 55.55555555555555, 41.666666666666664]\n",
      "Accuracy (Recall) over each cluster:  [100.0, 100.0, 49.99999999999999, 100.0, 30.0, 29.62962962962963, 66.66666666666666, 100.0, 23.333333333333332, 41.66666666666666]\n",
      "Precision score: 57.166666666666664, Recall score: 64.12962962962963\n",
      "#Number of clusters found: 11, against number of pre-computed clusters: 10#\n",
      "\n",
      "Accuracy (Precision) over each cluster:  [7.317073170731707, 7.317073170731707, 34.55284552845528, 4.878048780487805, 19.51219512195122, 56.09756097560975, 7.317073170731707, 9.75609756097561, 25.203252032520325, 12.195121951219512]\n",
      "Accuracy (Recall) over each cluster:  [75.0, 100.0, 49.99999999999999, 100.0, 80.0, 38.888888888888886, 100.0, 100.0, 40.0, 83.33333333333334]\n",
      "Precision score: 18.414634146341463, Recall score: 76.72222222222223\n",
      "#Number of clusters found: 3, against number of pre-computed clusters: 10#\n",
      "\n",
      "Accuracy (Precision) over each cluster:  [22.22222222222222, 16.666666666666664, 16.666666666666664, 50.0, 38.888888888888886, 75.0, 22.22222222222222, 100.0, 43.75, 36.11111111111111]\n",
      "Accuracy (Recall) over each cluster:  [100.0, 100.0, 100.0, 100.0, 30.0, 44.44444444444444, 49.99999999999999, 100.0, 20.0, 33.33333333333332]\n",
      "Precision score: 42.15277777777777, Recall score: 67.77777777777779\n",
      "#Number of clusters found: 8, against number of pre-computed clusters: 10#\n",
      "\n",
      "Accuracy (Precision) over each cluster:  [40.0, 30.0, 55.0, 100.0, 87.5, 100.0, 20.0, 100.0, 51.85185185185185, 47.22222222222222]\n",
      "Accuracy (Recall) over each cluster:  [100.0, 100.0, 49.99999999999999, 100.0, 17.5, 38.888888888888886, 66.66666666666666, 100.0, 26.666666666666668, 49.99999999999999]\n",
      "Precision score: 63.157407407407405, Recall score: 64.97222222222221\n",
      "#Number of clusters found: 12, against number of pre-computed clusters: 10#\n",
      "\n",
      "Accuracy (Precision) over each cluster:  [10.0, 100.0, 25.0, 61.666666666666664, 10.0, 10.0, 25.0, 23.333333333333332]\n",
      "Accuracy (Recall) over each cluster:  [100.0, 100.0, 50.0, 50.0, 100.0, 75.0, 25.0, 33.33333333333333]\n",
      "Precision score: 33.125, Recall score: 66.66666666666667\n",
      "#Number of clusters found: 5, against number of pre-computed clusters: 10#\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import my_dbscan\n",
    "import model_evaluation as me\n",
    "#import importlib\n",
    "#importlib.reload(me)\n",
    "\n",
    "models_recall = []\n",
    "models_precision = []\n",
    "clusters_found = [] # keep clusters results, they're useful later on\n",
    "min_s = 2 # min_samples\n",
    "for k, model in enumerate(models):\n",
    "    # try different eps for some model\n",
    "    if k == 1:\n",
    "        eps = 0.24\n",
    "        eps_increment = 0.13\n",
    "        min_s = 2\n",
    "    elif k==6:\n",
    "        eps = 0.11\n",
    "        eps_increment = 0.13\n",
    "        min_s = 3\n",
    "    else:\n",
    "        eps = 0.11\n",
    "        eps_increment = 0.13\n",
    "        min_s = 2\n",
    "    # get list of document vectors\n",
    "    doc_vecs = [model.docvecs[j] for j in range(len(model.docvecs))]\n",
    "    titles = [doc['title'] for doc in unwrapped_docs]\n",
    "    urls = [doc['url'] for doc in unwrapped_docs]\n",
    "    # apply dbscan clustering to these vectors\n",
    "    urls_cluster_list = my_dbscan.apply_dbscan(doc_vecs = doc_vecs, titles = titles, \n",
    "                                               urls = urls, subset_length = len(titles),\n",
    "                                                 eps = eps, eps_increment = eps_increment,\n",
    "                                               n_iterations = 3, verbose = False, min_samples = min_s)\n",
    "    # get clusters as list of titles\n",
    "    titles_clusters = utils.getDocTitleFromUrl(unwrapped_docs, urls_cluster_list)\n",
    "    clusters_found.append(titles_clusters)\n",
    "    #data = utils.plot_clusters(titles_clusters)\n",
    "    \n",
    "    # evaluate clustering\n",
    "    expected_clusters = []\n",
    "    for docs in cdocs:\n",
    "        expected_clusters.append([doc['title'] for doc in docs])\n",
    "    precision, recall = me.compute_clustering_accuracy(titles_clusters, expected_clusters, alg='avg')\n",
    "    print('Precision score: %s, Recall score: %s'%(precision, recall))\n",
    "    \n",
    "    models_precision.append(precision)\n",
    "    models_recall.append(recall)\n",
    "    # last check: we want to penalize models that simply cluster all docs together (that's not a valid result)\n",
    "    # that's way we have precision score\n",
    "    print(\"#Number of clusters found: {0}, against number of pre-computed clusters: {1}#\\n\".format(\n",
    "        len([l for l in urls_cluster_list if len(l)>0] ), len(cdocs)))\n",
    "\n",
    "## loaded model\n",
    "inferred_vecs = [loaded_model.infer_vector(sp(doc['title']+doc['abstract'])) for doc in unwrapped_docs]\n",
    "urls_cluster_list = my_dbscan.apply_dbscan(doc_vecs = inferred_vecs, titles = titles, \n",
    "                                               urls = urls, subset_length = len(titles),\n",
    "                                                 eps = eps, eps_increment = eps_increment, n_iterations = 3, verbose = False)\n",
    "titles_clusters = utils.getDocTitleFromUrl(unwrapped_docs, urls_cluster_list)\n",
    "clusters_found.append(titles_clusters)\n",
    "# evaluate clustering\n",
    "expected_clusters = []\n",
    "for docs in cdocs:\n",
    "    expected_clusters.append([doc['title'] for doc in docs])\n",
    "precision, recall = me.compute_clustering_accuracy(titles_clusters, expected_clusters)\n",
    "print('Precision score: %s, Recall score: %s'%(precision, recall))\n",
    "models_precision.append(precision)\n",
    "models_recall.append(recall)\n",
    "print(\"#Number of clusters found: {0}, against number of pre-computed clusters: {1}#\\n\".format(\n",
    "    len(urls_cluster_list), len(cdocs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tesla sotto indagine per colpa dei tweet di Elon Musk: crollo in Borsa - Corriere.it', 'Elon Musk denuciato per truffa, Tesla crolla in borsa - Wired']\n",
      "[\"L'equinozio d'autunno non è il 21 settembre: quest'anno arriva il 23 - Repubblica.it\", \"E' l'equinozio d'autunno - Spazio & Astronomia - ANSA.it\"]\n",
      "['iPhone XS: perché Apple ha nascosto il notch?', 'Problemi per iPhone XS e XS Max: «Non si caricano se il cavo è collegato mentre lo schermo è spento» - Corriere.it']\n",
      "['iPhone XS appiana le rughe, protestano gli utenti', 'iPhone XS: proteste per la carica troppo lenta']\n",
      "['Fifa 19 contro Pes 2019: qual è il migliore quest’anno? Ecco la sfida giocata da noi - Corriere.it', 'Fifa 19, la prova in anteprima - Corriere.it']\n",
      "['FIFA 19 sui campi della Champions League', 'FIFA 19, annunciata la disponibilità della demo', 'I miglioramenti di FIFA 19 per Nintendo Switch', \"FIFA 19: L'ora dei campioni, trailer di lancio\", \"FIFA 19 si mostra nel trailer L'Ora dei Campioni\"]\n",
      "['A spasso attorno alla Luna, SpaceX annuncia il primo turista spaziale - Corriere.it', 'SpaceX, i viaggi sulla Luna possono attendere', 'SpaceX porterà un uomo in orbita attorno alla Luna', 'SpaceX manderà Yusaku Maezawa sulla Luna']\n",
      "['Marte, il sottosuolo può avere ospitato la vita - Spazio & Astronomia - ANSA.it', 'SpaceX: ecco come saranno le basi umane su Marte', 'NASA, dalla CO2 al glucosio su Marte', 'Elon Musk contro la NASA per terraformare Marte', 'Decolla Facebook Dating, il Tinder di Menlo Park: primi test in Colombia - Repubblica.it', 'Facebook: Tinder nel mirino, novità di Instagram e Whatsapp - Corriere.it', 'Facebook Dating: ecco come funziona l’anti-Tinder di Zuckerberg - Corriere.it', 'Tesla, Elon Musk lascia la presidenza', 'Social e bambini: YouTube assume nuovi moderatori e Facebook lancia Messenger Kids - Corriere.it', 'YouTube e i video con bambini «abusati» Google sotto accusa, ritirata la pubblicità - Corriere.it']\n",
      "[\"iPhone Xs Max tira 3-4 volte più dell'Xs - Hi-tech - ANSA.it\", 'Apple lancia iPhone Xs e la versione Max: sempre più grandi, gli smartphone sono la nuova Tv - Corriere.it']\n",
      "['Aspettando iPhone Xs: il giorno di iOS 12', 'SmartThings: Samsung presenta un Tracker LTE']\n",
      "['Samsung lancia il suo primo smartphone con tre fotocamere - La Stampa', 'Samsung, in arrivo uno smartphone con quattro fotocamere (e il primo con schermo pieghevole) - Corriere.it', 'Huawei P20 Pro: 3 fotocamere | Arrivano anche P20 e P20 Lite - Corriere.it', 'Huawei: recensioni e novità']\n",
      "['Samsung, i nuovi Galaxy J6+ e J4+ - Tlc - ANSA.it', 'Samsung Galaxy Note 9: la nuova S Pen', 'Notizie Samsung Galaxy Note 9', \"Samsung Bixby 2.0 supporterà anche l'italiano\", 'Samsung Galaxy Note 9', \"Huawei Mate 20 Lite: un midrange dall'ottima autonomia. La recensione\", 'Huawei P Smart Plus: per lanciarlo alleanza tra i cinesi e Amazon. Con Emis Killa come testimonial - Corriere.it', 'Huawei MateBook X Pro', 'Huawei Mate 20 lite']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for cluster in clusters_found[-2]:\n",
    "    print(cluster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize results graphically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~D4nt3_/16.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "# using my api-key\n",
    "tls.set_credentials_file(username='D4nt3_', api_key='4O71urldgOueVtcApOdX')\n",
    "graph_name = 'model_eval_results_avg'\n",
    "\n",
    "model_descr = [model.comment for model in models]\n",
    "# plot test-accuracy results, plus silhoutte scores (times 100, since all the values are in between 0-1)\n",
    "trace0 = go.Bar(\n",
    "    x = model_descr + ['loaded model'],\n",
    "    y = models_precision,\n",
    "    name='Precision',\n",
    "    marker=dict(\n",
    "        color='rgb(49,130,189)'\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "trace2 = go.Bar(\n",
    "    x = model_descr + ['loaded model'],\n",
    "    y = models_recall,\n",
    "    name='Recall',\n",
    "    marker=dict(\n",
    "        color='rgb(155, 244, 66)',\n",
    "    )\n",
    "    \n",
    ")\n",
    "# precision, recall\n",
    "data = [trace0, trace2]\n",
    "layout = go.Layout(\n",
    "    title = 'Clustering Accuracy Results (set-1)',\n",
    "    xaxis=dict(\n",
    "        tickfont=dict(\n",
    "            size=10,\n",
    "            color='rgb(107, 107, 107)',\n",
    "            \n",
    "        ),\n",
    "        tickangle = -45\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Clustering accuracy (%)',\n",
    "        titlefont=dict(\n",
    "            size=16,\n",
    "            color='rgb(107, 107, 107)'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            size=14,\n",
    "            color='rgb(107, 107, 107)'\n",
    "        )\n",
    "    ),\n",
    "   \n",
    "    barmode='group',\n",
    "    bargap=0.2,\n",
    "    bargroupgap=0.1\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename=graph_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silhoutte score\n",
    "Silhouette coefficient combines ideas of both cohesion and separation, \n",
    "but for individual points, as well as clusters and \n",
    "clusterings; the Silhouette Coefficient is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample.\n",
    "The Silhoutte score is merely the average of each silhoutte coefficient, computed over each sample.\n",
    "It's a measure of the goodness of clustering, by assuming the fact that a cluster X is defined good if both every sample inside it is close to each other, and far from any other relatively-near cluster Y.\n",
    "This is not necessarily true for every shape of cluster.\n",
    "\n",
    "    The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17489207\n",
      "0.18785714\n",
      "0.21962713\n",
      "0.14373659\n",
      "0.16597174\n",
      "-0.05085477\n",
      "0.21195403\n",
      "0.19973199\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "s_scores = []\n",
    "titles = [doc['title'] for doc in unwrapped_docs]\n",
    "urls = [doc['url'] for doc in unwrapped_docs]\n",
    "    \n",
    "for model in models:\n",
    "    # get clusters for each model\n",
    "    doc_vecs = [model.docvecs[j] for j in range(len(model.docvecs))]\n",
    "\n",
    "    # apply dbscan clustering to these vectors\n",
    "    urls_cluster_list = my_dbscan.apply_dbscan(doc_vecs = doc_vecs, titles = titles, \n",
    "                                               urls = urls, subset_length = len(titles),\n",
    "                                                 eps = 0.11, eps_increment = 0.13, n_iterations = 3, verbose = False)\n",
    "    \n",
    "    # get cluster labels, mantaining original docs ordering\n",
    "    labels = []\n",
    "    for doc in unwrapped_docs:\n",
    "        noise = True\n",
    "        for i, url_list in enumerate(urls_cluster_list):\n",
    "            if(doc['url'] in url_list):\n",
    "                labels.append(i) # keep cluster id\n",
    "                noise = False\n",
    "                break\n",
    "        if noise:\n",
    "            labels.append(-1)\n",
    "    # make sure they have the same size\n",
    "    assert len(labels) == len(doc_vecs)\n",
    "    ss = silhouette_score(doc_vecs, labels , metric='cosine')\n",
    "    s_scores.append(ss)\n",
    "    print(ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions: \n",
    "A really low silhoutte score helps us identify models that tend to have very few cluster, hence not really recognizing differences between docs. This is fundamental, since test rules used so far prevent us from recognizing these kinds of models.\n",
    "Since silhoutte score is an average of silhoutte coefficients, smaller changes of values here may mean greater differences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
