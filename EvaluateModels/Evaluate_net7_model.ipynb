{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Doc2Vec model on new data\n",
    "# Title + Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train corpus:  59\n",
      "Doc2Vec(\"live data\",dbow,d100,n5,mc2,t4)\n",
      "Doc2Vec(\"alpha=0.1-live data\",dm/m,d100,n5,w10,mc2,t4)\n",
      "Vocabulary created!\n",
      "Training Doc2Vec(\"live data\",dbow,d100,n5,mc2,t4)\n",
      "CPU times: user 1.33 s, sys: 12 ms, total: 1.34 s\n",
      "Wall time: 805 ms\n",
      "Training Doc2Vec(\"alpha=0.1-live data\",dm/m,d100,n5,w10,mc2,t4)\n",
      "CPU times: user 2.23 s, sys: 4 ms, total: 2.24 s\n",
      "Wall time: 1.22 s\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "#filename = '2_different_fb-spazio.json'\n",
    "filename = 'similar_documents_facebook.json'\n",
    "# create corpus\n",
    "with open(filename, 'r') as file:\n",
    "    docs = json.load(file)\n",
    "\n",
    "# find duplicates\n",
    "index_list = []\n",
    "for i, doc in enumerate(docs):\n",
    "    try:\n",
    "        index = docs.index(i+1, len(docs), doc)\n",
    "        index_list.append(index)\n",
    "    except:\n",
    "        None\n",
    "docs = [doc for j, doc in enumerate(docs) if not(j in index_list)]\n",
    "\n",
    "    \n",
    "#filenames = ['mixed_docs_mix1.json', 'mixed_docs_mix2.json', 'mixed_docs_mix3.json']\n",
    "#docs = []\n",
    "#for filename in filenames:\n",
    "#    with open(filename, 'r') as file:\n",
    "#        docs += json.load(file)\n",
    "\n",
    "# get train corpus\n",
    "train_corpus = [gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(\n",
    "    d['title']+d['abstract']), [i]) for i, d in enumerate(docs) ]\n",
    "print(\"Length of train corpus: \",len(train_corpus))\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "\n",
    "# let's try training two models at once: Paragraph Vector - Distributed Memory (PV-DM), just like CBOW to W2V\n",
    "# and Paragraph Vector - Distributed Bag of Words (PV-DBOW), analogous to W2V Skip-gram\n",
    "epochs = 45\n",
    "vec_size = 100\n",
    "alpha = 0.10  \n",
    "MODEL_NAME = \"Models_Live_Test/d2v_abstract&title\"\n",
    "\n",
    "models = [\n",
    "    # PV-DBOW plain\n",
    "    Doc2Vec(dm=0, vector_size=vec_size, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=epochs, workers=cores, comment='live data'),\n",
    "    # PV-DM w/ default averaging\n",
    "    Doc2Vec(dm=1, vector_size= vec_size, window=10, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs= epochs, workers=cores, alpha= alpha, comment='alpha=0.1-live data'),\n",
    "]\n",
    "\n",
    "# build our vocabulary of words (all the unique words encountered inside our corpus, needed for training)\n",
    "for model in models:\n",
    "    print(model)\n",
    "    model.build_vocab(train_corpus)\n",
    "print(\"Vocabulary created!\")\n",
    "\n",
    "# train the models on the given data!\n",
    "counter = 0\n",
    "for model in models:\n",
    "    print(\"Training %s\" % model)\n",
    "    %time model.train(train_corpus, total_examples=len(train_corpus), epochs=model.epochs)\n",
    "    #model.save(MODEL_NAME+str(counter)+'.model')\n",
    "    counter = counter + 1\n",
    "#print(\"Models Saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "##Clusters##\n",
      "Cluster: ['Facebook, i cofondatori di Instagram lasciano la società - Repubblica.it', 'Facebook, terremoto in Instagram: lasciano i due co-fondatori della app dopo scontro con Zuckerberg - Il Sole 24 ORE']\n",
      "Cluster: [\"Arriva Portal, l'assistente smart di Facebook - Repubblica.it\", 'Facebook, Portal in arrivo la prossima settimana?']\n",
      "Cluster: ['I fondatori di Instagram lasciano Facebook: ecco cosa faranno in futuro', 'Ecco perché i fondatori di Instagram hanno lasciato Facebook - Corriere.it']\n",
      "Cluster: ['Facebook: ex moderatrice fa causa, traumatizzata da immagini - Hi-tech - ANSA.it', 'Usa, causa contro Facebook per inserzioni discriminatorie - Internet e Social - ANSA.it', \"L'ex moderatrice di contenuti fa causa a Facebook: «Traumatizzata, ho filtrato post tossici» - Corriere.it\"]\n",
      "Cluster: ['Wsj, Facebook ha trattato con le banche per i dati degli utenti - Internet e Social - ANSA.it', 'Facebook chiede alle banche i dati dei clienti - Wired']\n",
      "Cluster: ['Russiagate, Facebook consegna al Congresso le 3mila pubblicità comprate dai russi: «Viste da 10 milioni di utenti» - Corriere.it', 'Russiagate, dalla pubblicità su Facebook alla fabbrica di troll: le ultime novità - Corriere.it']\n",
      "Cluster: ['Facebook, sono cinque milioni gli europei coinvolti nell’attacco hacker - Corriere.it', 'Facebook, dopo l’hack c’è il rischio multa']\n",
      "Cluster: ['Facebook annuncia Oculus Quest, nuovo visore VR', 'Facebook Manifold VR Camera, video di annuncio', 'Facebook, Manifold per filmare in realtà virtuale']\n",
      "DBSCAN finished.\n",
      "\n",
      "##Clusters##\n",
      "Cluster: ['Facebook, ex moderatrice denuncia: “Filtriamo contenuti scioccanti, siamo traumatizzati\" - Repubblica.it', 'VR: Oculus Quest, il visore standalone di Facebook', 'Facebook Cambridge Analytica, coinvolti 214 mila italiani - Corriere.it', 'Facebook: Tinder nel mirino, novità di Instagram e Whatsapp - Corriere.it', 'Sport, notizie e musica: così Facebook punta tutto sui video - Corriere.it', 'Facebook e il controllo dei contenuti: il Guardian rivela le regole segrete su sesso, terrorismo e violenza - Corriere.it', 'Facebook è diventato ufficialmente un editore di notizie: ecco i programmi per la sezione Watch - Corriere.it', 'Facebook Watch, la tv di Mark Zuckerberg arriva in tutto il mondo - Corriere.it', 'Brad Parscale: «Così i dipendenti di Facebook hanno aiutato Trump a vincere le elezioni» - Corriere.it', 'Facebook compra WhatsApp, operazione record da 14 miliardi di euro - Corriere.it', 'Social e bambini: YouTube assume nuovi moderatori e Facebook lancia Messenger Kids - Corriere.it', 'La prossima Champions League verrà trasmessa (anche) su Facebook - Corriere.it', 'Le «Storie» a scomparsa arrivano anche su Facebook - Corriere.it', 'Facebook Dating: ecco come funziona l’anti-Tinder di Zuckerberg - Corriere.it', 'Facebook vuole mettere la pubblicità su WhatsApp - Corriere.it']\n",
      "Cluster: ['Facebook e Google nel mirino di Trump - Internet e Social - ANSA.it', 'Russiagate, non solo Facebook e Twitter: gli hacker hanno usato anche Pokémon Go - Corriere.it']\n",
      "Cluster: ['Facebook: la Ue attacca, migliori la trasparenza o multe in arrivo - Internet e Social - ANSA.it', 'Fuga da Facebook: lo usa solo il 51% degli adolescenti americani - Corriere.it', 'Dopo il caso Facebook, il Congresso Usa indaga sui rapporti tra Google e Huawei - Corriere.it', 'In Europa calano gli utenti, crolla il titolo di Facebook - Wired']\n",
      "Cluster: ['Facebook, scoperta una grave vulnerabilità', 'Facebook, attacco hacker: colpiti utenti anche in Italia - Corriere.it', 'Facebook, attacco hacker: colpiti 50 milioni di account - Corriere.it', 'Facebook disconnette 90 milioni di iscritti']\n",
      "Cluster: ['Facebook ha usato miliardi di foto di Instagram per educare la sua intelligenza artificiale - Wired', 'Al Wired Next Fest Facebook spiega il futuro dell’intelligenza artificiale - Wired', \"Facebook apre l'Area 404 - Wired\", 'Facebook, priorità alle notizie locali - Wired']\n",
      "DBSCAN finished.\n",
      "\n",
      "##Clusters##\n",
      "Cluster: ['Dating, il servizio di Facebook per trovare l’anima gemella - La Stampa', 'Decolla Facebook Dating, il Tinder di Menlo Park: primi test in Colombia - Repubblica.it', 'Tra cinque anni su Facebook ci saranno solo video - Corriere.it']\n",
      "Cluster: [\"Facebook: l'advertising cerca il tuo profilo ombra\", 'Facebook, numero di telefono agli inserzionisti']\n",
      "Cluster: ['Il fondatore di Oculus, Palmer Luckey abbandona Facebook - Corriere.it', 'Facebook e la caccia al tesoro delle opzioni per tutelare i nostri dati - Wired', 'Dopo il caso Facebook fuga di clienti: Cambridge Analytica chiude - Wired']\n",
      "Noise:  ['Zuckerberg prende il controllo di Instagram per salvare Facebook - Il Sole 24 ORE', 'Moderatore contro Facebook: siamo troppo esposti', 'Facebook contro le armi stampate in 3D', 'Facebook: iscrizione, come funziona']\n",
      "DBSCAN finished.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~D4nt3/72.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'S-G' model finds many more clusters ( higher precision) -model[1]\n",
    "# CBOW model finds fewer clusters with more docs in them\n",
    "\n",
    "import my_dbscan\n",
    "model = models[1]\n",
    "print(len(model.docvecs))\n",
    "doc_vecs = [model.docvecs[j] for j in range(len(model.docvecs))]\n",
    "titles = [doc['title'] for doc in docs]\n",
    "urls = [doc['url'] for doc in docs]\n",
    "urls_cluster_list = my_dbscan.apply_dbscan(doc_vecs = doc_vecs, titles = titles, \n",
    "                                           urls = urls, subset_length = 70,\n",
    "                                             eps = 0.27, eps_increment = 0.1, n_iterations = 3, verbose = False)\n",
    "# visualize clustering\n",
    "import plotly.plotly as py\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "import random\n",
    "import numpy as np\n",
    "import utils\n",
    "# using my api-key\n",
    "tls.set_credentials_file(username='D4nt3', api_key='FdMB4O6qCfciGDOnLvdQ')\n",
    "i = 0\n",
    "cluster_titles = utils.getDocTitleFromUrl(docs, urls_cluster_list)\n",
    "fig = utils.plot_clusters(cluster_titles)\n",
    "py.iplot(fig, filename='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose Eps for DBSCAN\n",
    "Choose eps value for dbscan alg, by plotting a mixed corpus of data (taken as sample).\n",
    "## Important assumption: no-duplicates\n",
    "Duplicates docs in more trained model are not a problem, but here they may spoil the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~D4nt3/72.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\"\"\"\n",
    "filenames = ['mixed_docs_mix1.json', 'mixed_docs_mix2.json']\n",
    "docs = []\n",
    "for filename in filenames:\n",
    "    with open(filenames[0], 'r') as file:\n",
    "        docs += json.load(file)\n",
    "\n",
    "train_corpus = [gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(\n",
    "    d['title']+d['abstract']), [i]) for i, d in enumerate(docs) ]\n",
    "model = Doc2Vec(dm=0, vector_size=vec_size, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=epochs, workers=4, comment='live data')\n",
    "model.build_vocab(train_corpus)\n",
    "%time model.train(train_corpus, total_examples=len(train_corpus), epochs=model.epochs)\n",
    "\"\"\"\n",
    "\n",
    "# plot the 'eps-value' graph (needed to choose the best eps using DBSCAN)\n",
    "\n",
    "\n",
    "model = models[1]\n",
    "doc_vectors = [model.docvecs[i] for i in range(len(docs))]\n",
    "titles = [doc['title'] for doc in docs]\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "import random\n",
    "import numpy as np\n",
    "# using my api-key\n",
    "tls.set_credentials_file(username='D4nt3', api_key='FdMB4O6qCfciGDOnLvdQ')\n",
    "import utils\n",
    "\n",
    "# let's first try with 2 as min_count\n",
    "# we get back a list of (doc_title, dist_from_kth_neighbour)\n",
    "title_dist_tuples = utils.choose_eps(2, doc_vectors, titles)\n",
    "data = utils.visualize_eps_graph(title_dist_tuples)    \n",
    "py.iplot(data, filename='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 69\n",
      "New length 69\n"
     ]
    }
   ],
   "source": [
    "filename = '2_different_fb-spazio.json'\n",
    "#filename = 'similar_documents_facebook.json'\n",
    "# create corpus\n",
    "with open(filename, 'r') as file:\n",
    "    docs = json.load(file)\n",
    "print(\"Length\",len(docs))\n",
    "# find duplicates\n",
    "index_list = []\n",
    "for i, doc in enumerate(docs):\n",
    "    for j in range(i+1, len(docs)):\n",
    "        doc2 = docs[j]\n",
    "        if doc['title'].lower().strip() == doc2['title'].lower().strip():\n",
    "            index_list.append(j)\n",
    "docs = [doc for j, doc in enumerate(docs) if not(j in index_list)]\n",
    "print(\"New length\", len(docs))\n",
    "\n",
    "with open(filename, 'w') as out:\n",
    "    json.dump(docs, out)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate clusterization\n",
    "## Over hand-clusterized set of docs\n",
    "\n",
    "## 1. visualize clusterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 940 ms, sys: 8 ms, total: 948 ms\n",
      "Wall time: 695 ms\n",
      "##Clusters##\n",
      "Cluster: ['Decolla Facebook Dating, il Tinder di Menlo Park: primi test in Colombia - Repubblica.it', 'Facebook: Tinder nel mirino, novità di Instagram e Whatsapp - Corriere.it', 'Facebook Dating: ecco come funziona l’anti-Tinder di Zuckerberg - Corriere.it', 'Social e bambini: YouTube assume nuovi moderatori e Facebook lancia Messenger Kids - Corriere.it', 'YouTube e i video con bambini «abusati» Google sotto accusa, ritirata la pubblicità - Corriere.it']\n",
      "Cluster: ['Tesla sotto indagine per colpa dei tweet di Elon Musk: crollo in Borsa - Corriere.it', 'Elon Musk denuciato per truffa, Tesla crolla in borsa - Wired', 'Tesla, Elon Musk lascia la presidenza']\n",
      "Cluster: [\"L'equinozio d'autunno non è il 21 settembre: quest'anno arriva il 23 - Repubblica.it\", \"E' l'equinozio d'autunno - Spazio & Astronomia - ANSA.it\"]\n",
      "Cluster: [\"iPhone Xs Max tira 3-4 volte più dell'Xs - Hi-tech - ANSA.it\", 'Apple conferma \"per errore\" iPhone XS, XS Max e XR', 'Speciale iPhone Xs', 'iPhone XS: perché Apple ha nascosto il notch?', 'Speciale iPhone Xs', 'Apple lancia iPhone Xs e la versione Max: sempre più grandi, gli smartphone sono la nuova Tv - Corriere.it', \"iPhone XS, XS Max e XR in anteprima: tutto quello che c'è da sapere - Corriere.it\", \"iPhone XS, XS Max e XR in anteprima: tutto quello che c'è da sapere - Corriere.it\", \"iPhone XS, XS Max e XR in anteprima: tutto quello che c'è da sapere - Corriere.it\", 'Problemi per iPhone XS e XS Max: «Non si caricano se il cavo è collegato mentre lo schermo è spento» - Corriere.it', \"iPhone XS, XS Max e XR in anteprima: tutto quello che c'è da sapere - Corriere.it\", 'iPhone XS: proteste per la carica troppo lenta']\n",
      "Cluster: [\"iPhone XS, XS Max e XR in anteprima: tutto quello che c'è da sapere - Corriere.it\", 'iPhone XS Max e XS: recensione e prova video in anteprima degli smartphone Apple - Corriere.it']\n",
      "Cluster: ['Fifa 19 contro Pes 2019: qual è il migliore quest’anno? Ecco la sfida giocata da noi - Corriere.it', 'Fifa 19, la prova in anteprima - Corriere.it', 'FIFA 19 sui campi della Champions League', 'FIFA 19, annunciata la disponibilità della demo', 'I miglioramenti di FIFA 19 per Nintendo Switch', \"FIFA 19: L'ora dei campioni, trailer di lancio\", 'FIFA 19, ecco i requisiti PC', 'FIFA 19: novità, uscita e prezzo', \"FIFA 19 si mostra nel trailer L'Ora dei Campioni\"]\n",
      "Cluster: ['A spasso attorno alla Luna, SpaceX annuncia il primo turista spaziale - Corriere.it', 'SpaceX, i viaggi sulla Luna possono attendere', 'SpaceX porterà un uomo in orbita attorno alla Luna']\n",
      "DBSCAN finished.\n",
      "\n",
      "##Clusters##\n",
      "Cluster: ['Marte, il sottosuolo può avere ospitato la vita - Spazio & Astronomia - ANSA.it', 'NASA, dalla CO2 al glucosio su Marte']\n",
      "DBSCAN finished.\n",
      "\n",
      "##Clusters##\n",
      "Cluster: ['iPhone XS e XS Max: novità, scheda tecnica e prezzo', 'iPhone XS appiana le rughe, protestano gli utenti']\n",
      "Noise:  ['Aspettando iPhone Xs: il giorno di iOS 12', 'Apple iPhone Xs e Xs Max: è troppo caro?', 'YouTube Kids, lo streaming dei bambini in Italia', 'SpaceX manderà Yusaku Maezawa sulla Luna']\n",
      "DBSCAN finished.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~D4nt3/74.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import my_dbscan\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import utils\n",
    "import plotly.plotly as py\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "import random\n",
    "import numpy as np\n",
    "# using my api-key\n",
    "tls.set_credentials_file(username='D4nt3', api_key='FdMB4O6qCfciGDOnLvdQ')\n",
    "\n",
    "# load file containing clusters\n",
    "with open('pre-clustered_docs.json', 'r') as json_data:\n",
    "    cdocs = json.load(json_data)\n",
    "# [ [,], [,], ....]\n",
    "cdocs = [doc for cluster in cdocs for doc in cluster]\n",
    "# clusterize this documents\n",
    "train_corpus = [gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(\n",
    "    d['title']+d['abstract']), [i]) for i, d in enumerate(cdocs) ]\n",
    "# dm = 0 / 1\n",
    "model = Doc2Vec(dm=0, vector_size=vec_size, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=epochs, workers=4, comment='live data')\n",
    "model.build_vocab(train_corpus)\n",
    "%time model.train(train_corpus, total_examples=len(train_corpus), epochs=model.epochs)\n",
    "\n",
    "doc_vecs = [model.docvecs[j] for j in range(len(model.docvecs))]\n",
    "titles = [doc['title'] for doc in cdocs]\n",
    "urls = [doc['url'] for doc in cdocs]\n",
    "urls_cluster_list = my_dbscan.apply_dbscan(doc_vecs = doc_vecs, titles = titles, \n",
    "                                           urls = urls, subset_length = len(titles),\n",
    "                                             eps = 0.27, eps_increment = 0.1, n_iterations = 3, verbose = False)\n",
    "\n",
    "\n",
    "titles_clusters = utils.getDocTitleFromUrl(cdocs, urls_cluster_list)\n",
    "data = utils.plot_clusters(titles_clusters)\n",
    "py.iplot(data, filename='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate clusterization\n",
    "(soft)Test rules: go through each pre-defined cluster; if you find the elements of a cluster correctly grouped, that's a 100% correct over that cluster; otherwise, you count the number of elements correctly put together (at least 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 7), (3, 0), (3, 1), (2, 2), (13, 3), (9, 5), (2, 0), (3, 6)]\n",
      "[100.0, 100.0, 100.0, 100.0, 72.22222222222223, 100.0, 66.66666666666667, 75.0]\n",
      "Accuracy over evaluation set:  89.2361111111111 %\n"
     ]
    }
   ],
   "source": [
    "with open('pre-clustered_docs.json', 'r') as json_data:\n",
    "    cdocs = json.load(json_data)\n",
    "\n",
    "def search_docs_in_clusters(doc_titles, clusters, verbose = True):\n",
    "    \"\"\" doc_titles: list of titles to search for.\n",
    "        clusters: list of cluster in which to search into.\n",
    "        Returns the highest number of occurences of doc_titles elements\n",
    "        in a cluster, paired with the aforementioned cluster index. \n",
    "    \"\"\"\n",
    "    occurences = []\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        if verbose: print(\"##Searching into cluster\", cluster, '##')\n",
    "        correct = 0\n",
    "        for title in doc_titles:\n",
    "            if verbose: print(\"==Searching for\",title,'==')\n",
    "            if title in cluster:\n",
    "                if verbose: print(title[:10],'.. is in cluster!')\n",
    "                correct += 1\n",
    "        occurences.append((correct, i))\n",
    "    # order list by most appearences and return the first pair\n",
    "    occurences.sort(key = lambda tup: tup[0])\n",
    "    return occurences[-1]\n",
    "\n",
    "occurences = []\n",
    "for docs in cdocs:\n",
    "    occurences.append(search_docs_in_clusters([doc['title'] for doc in docs], titles_clusters, verbose=False))\n",
    "print(occurences)\n",
    "\n",
    "# compute percentages\n",
    "correct = [c for (c, index) in occurences]\n",
    "percentages = []\n",
    "\n",
    "for i, docs_list in enumerate(cdocs):\n",
    "    # if less than 2 docs were correctly classified, the 'answer' is considered not correct\n",
    "    if correct[i] < 2:\n",
    "        percentages.append(0)\n",
    "    else:\n",
    "        percentages.append(correct[i] * 100 / len(docs_list))\n",
    "print(percentages)\n",
    "\n",
    "# compute the mean of percentages as final accuracy of the model over the test set (in terms of clustering)\n",
    "p_sum = 0\n",
    "for p in percentages:\n",
    "    p_sum += p\n",
    "print(\"Accuracy over evaluation set: \",p_sum/len(percentages), \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions:\n",
    "From now on we will use these tools to evaluate each different model, possibly with an improved evaluation-set.\n",
    "The results you can get from these (high) percentages must be intepreted: the test is not as 'strict' in terms of evaluationg the model, since it doesn't require a matching clusterization w.r.t the eval. set, and this inevitably leds to higher results in terms of accuracy. Why this choice? Essentatially, that's because I do not take this hand-made clustering as ground truth, they can be, up to a certain degree, freely contested, and sub-clusters may be made in some cases.\n",
    "This can be seen with the 4th cluster (iphone): the model created another meaningful sub-cluster, containing, by my personal interpretation, two docs regarding the new iPhone REVIEW, and yet it does get 'punished' by the score metric.\n",
    "These kinds of situations highlight the difficulty of creating a ground-truth test-set, as well as a functional score system using this approach.\n",
    "\n",
    "\n",
    "How can we improve the test results reliability? \n",
    "We can try to obtain a 'closer to ground-truth' test set, by clustering only really similar docs (e.g 'First Moon landing!', 'Man lands on the Moon!', 'Moon touchdown:'..); but while this would definitely improve the test results in terms of 'objective clustering', we'd most definitely lose something in terms of representing real case scenarios, by pushing the test model to perform well over set of docs which are pretty far from the standard set it will be analyzing in 'production'.\n",
    "I personally retain this last part to be fundamental, so I will use this kind of set of docs to evaluate models, and at the same time, I will analyze both visually and with other tools the results obtained (in particular, looking at the clusters the model generates, is there a way to say 'these clusters make sense, even tho they were not included in the eval. set?') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
