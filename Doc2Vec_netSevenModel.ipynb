{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 50\n",
      "iteration 100\n",
      "iteration 150\n",
      "iteration 200\n",
      "iteration 250\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import gensim\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "import random\n",
    "\n",
    "# Set file names for train and test data\n",
    "test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data'])\n",
    "lee_train_file = test_data_dir + os.sep + 'lee_background.cor'\n",
    "lee_test_file = test_data_dir + os.sep + 'lee.cor'\n",
    "\n",
    "#create the model\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "data = [\n",
    "\"Rete_di_computer Connessione Comunicazione Dato Livello_fisico Informazione Città_intelligente Azienda\",\n",
    "\"Comune Sestriere Turismo Via_Lattea_(comprensorio_sciistico) Torino Wireless Proprietà_(diritto) Casa Servizio\",\n",
    "\"Taxi Servizio Filosofia Acronimo Torino Europa Wireless\",\n",
    "\"Monaco di Baviera Siemens_(azienda) Kickoff Internet_delle_cose Euro\",\n",
    "\"Ministero_dei_trasporti Torino Wireless Comune_medievale Attraversamento_pedonale\",\n",
    "\"Unione_europea Torino Wireless Stati_Uniti_d'America Canada Piccola_e_media_impresa Mercato Città intelligente\"\n",
    "\n",
    "\n",
    "\n",
    "        ]\n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if tokens_only:\n",
    "                # 'yield' returns a generator, useful for large set of data (in terms of memory consumption)\n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])\n",
    "train_corpus = list(read_corpus(lee_train_file))  # train data has tag associated to each document\n",
    "test_corpus = list(read_corpus(lee_test_file, tokens_only=True))\n",
    "                \n",
    "\n",
    "#tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "tagged_data = train_corpus\n",
    "# print (tagged_data)\n",
    "\n",
    "\n",
    "max_epochs = 300\n",
    "vec_size = 35\n",
    "alpha = 0.030\n",
    "\n",
    "model = Doc2Vec(vector_size=vec_size,\n",
    "                alpha=alpha,\n",
    "                min_alpha=0.00030,\n",
    "                min_count=2,\n",
    "                dm=0) #dm=0 means \"distributed bag of words\"\n",
    "\n",
    "#C'è ancora da giocare un po' con il tuning dei parametri. Io mi sono messa in condizione di farlo e ci posso pure guardare nei prox giorni\n",
    "\n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    if(epoch%50==0):\n",
    "        print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nick/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow,d35,n5,mc2,s0.001,t3)\n",
      "TEST DATA TO INFER:hundreds of people have been forced to vacate their homes in the southern highlands of new south wales as strong winds today pushed huge bushfire towards the town of hill top new blaze near goulburn south west of sydney has forced the closure of the hume highway at about pm aedt marked deterioration in the weather as storm cell moved east across the blue mountains forced authorities to make decision to evacuate people from homes in outlying streets at hill top in the new south wales southern highlands an estimated residents have left their homes for nearby mittagong the new south wales rural fire service says the weather conditions which caused the fire to burn in finger formation have now eased and about fire units in and around hill top are optimistic of defending all properties as more than blazes burn on new year eve in new south wales fire crews have been called to new fire at gunning south of goulburn while few details are available at this stage fire authorities says it has closed the hume highway in both directions meanwhile new fire in sydney west is no longer threatening properties in the cranebrook area rain has fallen in some parts of the illawarra sydney the hunter valley and the north coast but the bureau of meteorology claire richards says the rain has done little to ease any of the hundred fires still burning across the state the falls have been quite isolated in those areas and generally the falls have been less than about five millimetres she said in some places really not significant at all less than millimetre so there hasn been much relief as far as rain is concerned in fact they ve probably hampered the efforts of the firefighters more because of the wind gusts that are associated with those thunderstorms\n",
      "\n",
      "Similar Doc-->(doctag:200,score:0.6791800260543823):<<the united states says video tape found inside afghanistan proves beyond doubt osama bin laden was behind the attacks on the world trade centre and the pentagon the tape is alleged to show bin laden discussing the success of the mission in the minute tape bin laden is said to be at dinner when told plane had crashed into the world trade centre he is alleged to have told others present what had happened and they cheered us vice president dick cheney says the video shows bin laden was clearly behind the attacks there ve some disputes in some quarters about it but this is one more piece of evidence confirming his responsibility he said republican chuck hagel of the foreign relations committee says the administration must make the tapes public the world needs to see this he said some officials hope it will be shown to counter concerns in the muslim world that bin laden has been unjustly accused osama bin laden was said to be staging defiant stand in the afghan mountains as taliban rule finally came to an ignominious end with the surrender of the last province under their control spokesman for the northern alliance said bin laden was now leading the defence of his mountain hideouts in person with about loyal fighters from his al qaeda organisation osama himself has taken the command of the fighting mohammad amin told the reuters news agency from the eastern city of jalalabad he along with around of his people including some taliban officials have now dug themselves into the forests of spin ghar after we overran all their bases in tora bora he is here for sure mr amin said american planes have been carrying out regular and severe bombings to kill him mr amin added that at least one of bin laden arab fighters had been killed in very intense fighting the saudi born islamist accused by washington of ordering the september attacks on the united states appeared ever more isolated after his taliban protectors handed over the zabul province to tribal elders the rule of the taliban in afghanistan has totally ended the pakistan based afghan news agency afghan islamic press aip said in reporting the surrender of zabul at least civilians were killed and injured in weekend bombing raids by us warplanes in afghanistan south eastern paktika province the afghan islamic press aip said late sunday the pakistan based news agency quoting informed sources said the us jets blasted several vehicles at sharana the provincial capital of paktika on saturday killing people and injuring several others the dead were five children four women and five men another people were killed when us planes bombarded vehicles in pre dawn raids on sunday in the mosh khil area near sharana aip said it said mosque was destroyed in the raids aip said taliban rule had been ended in paktika and the administration was being run by tribal shura council>>\n",
      "\n",
      "Similar Doc-->(doctag:237,score:-0.633489727973938):<<high interest rates on credit cards have prompted call for an inquiry by the australian competition and consumer commission accc the australian consumers association aca says banks are not passing on all reserve bank interest rate cuts the association katherine wolthuizen says while the reserve bank again cut official interest rates yesterday credit card interest rates remain high we would certainly like to see proper investigation into that and hopefully some movement to bring the banks to account for it they don like being regulated and sometimes the threat of regulation can cause them to amend their ways but of course they do have very long way to go ms wolthuizen said>>\n",
      "\n",
      "Similar Doc-->(doctag:164,score:-0.6340999603271484):<<japanese car maker mitsubishi has confirmed that it has asked for more money from the australian government mitsubishi sources in japan say that the car maker has applied for strategic investment coordination in short the company wants grant from the australian government the company has refused to say what the money would be used for or how much it wants although it has reaffirmed its commitment to upgrade its current model of magna at its adelaide plants the grant would be on top of the several hundred million dollars that has already been promised to mitsubishi under car assistance plan>>\n",
      "[]\n",
      "Clustering vectors by DBSCAN\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Estimated number of clusters: 1\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from nltk import word_tokenize\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics.pairwise import cosine_distances, pairwise_distances\n",
    "from scipy import sparse\n",
    "from sklearn.cluster import KMeans, DBSCAN, AffinityPropagation\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors.ball_tree import BallTree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "model= Doc2Vec.load(\"d2v.model\")\n",
    "print (model)\n",
    "#print(model.vocabulary)\n",
    "#to find the vector of a document which is not in training data\n",
    "#test_data = word_tokenize(\"Rete_di_computer Connessione Comunicazione Dato Livello_fisico Informazione Città_intelligente Azienda\".lower())\n",
    "test_data = train_corpus[0].words # test_data is a list of taggedDocuments,as returned by read_corpus\n",
    "print(\"TEST DATA TO INFER:{}\".format(' '.join(test_data)))\n",
    "v1 = model.infer_vector(test_data)\n",
    "# print(\"V1_infer\", v1)\n",
    "\n",
    "# Find the top-N(topn default = 10) most similar docvecs from the training set\n",
    "# This method computes cosine similarity between a simple mean of the projection weight vectors of the given docs\n",
    "# Returns: Sequence of (doctag/index, similarity).\n",
    "# Return type: list of ({str, int}, float)\n",
    "similar_docs = model.docvecs.most_similar([v1], topn = 3)\n",
    "\n",
    "# n_similarity(ds1, ds2) Compute cosine similarity between two sets of docvecs from the trained set.\n",
    "\n",
    "# print all similar documents with their score\n",
    "for doc_tag, similarity in similar_docs:\n",
    "    print(\"\\nSimilar Doc-->(doctag:{0},score:{1}):<<{2}>>\".format(doc_tag, similarity, ' '.join(train_corpus[int(doc_tag)].words)))\n",
    "# data contains words like: \"i, love, chatbots\".. ' '.join puts together strings separating them with a white space\n",
    "\n",
    "# #############################################################################\n",
    "vecs = []\n",
    "# docvecs (list of Doc2VecKeyedVectors) \n",
    "# – Vector representations of the documents in the corpus. Each vector has size == vector_size\n",
    "# check notes for more detailed info\n",
    "for doc in iter(range(0, len(model.docvecs))):\n",
    "    doc_vec = model.docvecs[doc]\n",
    "    vecs.append(doc_vec.reshape((1, 35)))\n",
    "\n",
    "# print(vecs[0]) not really interesting, they're just 1x35 vectors\n",
    "# print(vecs[1])\n",
    "print(model.docvecs.offset2doctag)\n",
    "        #  print model.docvecs.doctags.keys()\n",
    "doc_vecs = np.array(vecs, dtype='float')  # TSNE expects float type values\n",
    "\n",
    "# print doc_vecs\n",
    "docs = []\n",
    "for i in doc_vecs:\n",
    "    docs.append(i[0])\n",
    "    # print  docs\n",
    "\n",
    "# print vocabulary -list of words known-\n",
    "# print(model.wv.vocab)\n",
    "\n",
    "print (\"Clustering vectors by DBSCAN\")\n",
    "\n",
    "# lets try lower eps (eps: the minimum distance between two points. It means that if\n",
    "# the distance between two points is lower or equal to this value (eps), these points are considered neighbors)\n",
    "#  As a general rule, a minimum minPoints can be derived from a\n",
    "# number of dimensions (D) in the data set, as minPoints ≥ D + 1\n",
    "db = DBSCAN(eps=3.0,min_samples=4, metric='euclidean').fit(pairwise_distances(docs, metric='cosine'))\n",
    "#db = DBSCAN(eps=0.9, min_samples=4, metric='cosine',algorithm='brute').fit(docs)\n",
    "labels = db.labels_\n",
    "print (db.labels_)\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters)\n",
    "\n",
    "word_centroid_map = dict(zip(model.docvecs.offset2doctag, labels))\n",
    "\n",
    "print (list(word_centroid_map))\n",
    "\n",
    "\n",
    "ids = []\n",
    "rest = []\n",
    "for cluster in iter(range(0, n_clusters)):\n",
    "    for key, item in word_centroid_map.items():\n",
    "        if item == cluster:\n",
    "            ids.append(key)\n",
    "            rest.append(item)\n",
    "print (ids)\n",
    "print (rest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# let's try another way for clustering data: K-Mean, an even more popular algorithm,\n",
    "# which I know from the introductory course on AI, so it might be smarter \n",
    "# to utilize algorithms which I know and can talk about in the presentation\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# build k-means model\n",
    "# we could set-up an incremental version, increasing\n",
    "# number of clusters at each iteration\n",
    "kmeans = KMeans(n_clusters = 5, verbose=0) \n",
    "docs = []\n",
    "for doc in iter(range(0, len(model.docvecs))):\n",
    "    doc_vec = model.docvecs[doc]\n",
    "    docs.append(doc_vec.reshape((1, 35)))\n",
    "doc_vecs = np.array(docs).astype(float) \n",
    "# scikit-learn expects 2d num arrays for the training dataset for a fit function. \n",
    "# The dataset you are passing in is a 3d array you need to reshape the array into a 2d\n",
    "nsamples, nx, ny = doc_vecs.shape\n",
    "d2_train_dataset = doc_vecs.reshape((nsamples,nx*ny)) \n",
    "#First dimension is maintained and the other two dimensions are flattened (so 28x28 becomes 784\n",
    "\n",
    "kmeans.fit(d2_train_dataset) # data, as vectors of documents\n",
    "\n",
    "# let's see if it works by trying to predict the cluster of all elements in the training set \n",
    "correct = 0\n",
    "for i in range(len(d2_train_dataset)):\n",
    "    predict_me = d2_train_dataset[i]\n",
    "    predict_me = predict_me.reshape(-1, len(predict_me))\n",
    "    prediction = kmeans.predict(predict_me)\n",
    "    if((d2_train_dataset[prediction[0]]-d2_train_dataset[i]).all()):\n",
    "        correct += 1\n",
    "\n",
    "print(correct/len(d2_train_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation (fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ggplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9dff975b28af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#df['pca-three'] = pca_result[:,2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mggplot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ggplot'"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "pca_result = pca.fit_transform(X)\n",
    "\n",
    "df = dict()\n",
    "df['pca-one'] = pca_result[:,0]\n",
    "df['pca-two'] = pca_result[:,1] \n",
    "#df['pca-three'] = pca_result[:,2]\n",
    "\n",
    "from ggplot import *\n",
    "\n",
    "\n",
    "chart = ggplot( X, aes(x='pca-one', y='pca-two', color='label') ) \\\n",
    "        + geom_point(size=75,alpha=0.8) \\\n",
    "        + ggtitle(\"Some title\")\n",
    "chart\n",
    "\n",
    "\n",
    "print ('Explained variation per principal component: {0}'.format(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
