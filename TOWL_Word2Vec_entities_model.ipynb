{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec approach\n",
    "## Model will be trained on whole docs text, plus some 'reinforced' docs containing only entities; prediction phase will be tested on entities-only documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed libraries\n",
    "import json\n",
    "import random\n",
    "#import numpy as np\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "from collections import Counter\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded correctly, type:  <class 'list'> 1569\n",
      "{'fonte_dati': ['trend_analisys'], 'id': 'https://www.punto-informatico.it/fujitsu-si-separa-da-pc-e-mobile/', 'ta_id': [5], 'title': 'Fujitsu si separa da PC e mobile', 'abstract': '   Roma – Per guadagnare in efficienza e tentare di rincorrere una posizione più appetibile sul mercato mobile e sul mercato del PC, per affrontare anni di profondi cambiamenti per entrambi i settori, Fujitsu  ha annunciato  lo spinoff delle due divisioni dedicate l’una a notebook e PC e l’altra agli smartphone. \\n Le due aziende, che nasceranno ufficialmente nel mese di febbraio del prossimo anno, consentiranno all’azienda “di chiarire le responsabilità nella gestione, di agevolare decisioni più rapide della dirigenza e di ottenere una maggiore efficienza”: aspetti fondamentali nel momento in cui la diffusione sempre più di massa e sempre più ubiqua di PC e smartphone “ha reso progressivamente sempre più difficile differenziarsi e ha reso sempre più serrata la competizione con i nuovi produttori globali”. \\n Fujitsu lascerà dunque che la propria divisione mobile viva di vita propria in Fujitsu Connected Technologies e tenti di farsi largo in uno scenario mobile estremamente  complesso  , con le sue stagnazioni, le sue conferme di lusso e le nuove esigenze dei mercati emergenti. \\n La divisione PC e notebook, che da febbraio farà capo a Fujitsu Client Computing Limited, affronterà una  conginutura  affatto positiva sperando nella ripresa. Nei mesi scorsi, caratterizzati da  sommovimenti  che  hanno interessato  l’assetto degli storici produttori giapponesi, circolava un’  indiscrezione  che tratteggiava un futuro comune per Fujitsu, Vaio e Toshiba: lo spinoff annunciato dall’azienda  potrebbe rappresentare  un primo passo in questa direzione. ', 'url': ['https://www.punto-informatico.it/fujitsu-si-separa-da-pc-e-mobile/'], 'website': ['punto-informatico.it'], 'timestamp': [1451308920000], 'publication_date': ['2015-12-28T13:22:00Z'], 'flattened_entities': ['azienda client computer_portatile fujitsu mercato roma smartphone spin-off_diritto telefonia_mobile toshiba'], 'result_entities': ['Roma', 'Mercato', 'Telefonia mobile', 'Fujitsu', 'Spin-off (diritto)', 'Computer portatile', 'Smartphone', 'Azienda', 'Client', 'Toshiba'], '_version_': 1613295373058572288}\n"
     ]
    }
   ],
   "source": [
    "## Load documents from json\n",
    "filename = 'clean_dataset.json'\n",
    "with open(filename, 'r') as out:\n",
    "        docs = json.load(out)\n",
    "        \n",
    "print(\"File loaded correctly, type: \", type(docs), len(docs))\n",
    "# we need a single string instead of a list in result_entities\n",
    "\"\"\"\n",
    "for doc in docs:\n",
    "    if isinstance(doc['result_entities'], list):\n",
    "        for word in doc['result_entities']:\n",
    "            word_sum = word_sum + word + ' '\n",
    "        doc['result_entities'] = word_sum\"\"\"\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and train model\n",
    "## experiment: try to insert some entities-only docs in training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary created, number of known words:  10797\n",
      "Training Word2Vec(vocab=10797, size=100, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imacdev/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 16s, sys: 1.73 s, total: 4min 18s\n",
      "Wall time: 2min 27s\n",
      "Model Saved.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def lower_case_list(list_):\n",
    "    for i, word in enumerate(list_):\n",
    "        list_[i] = word.lower()\n",
    "    return list_\n",
    "# the effect I want to create by adding entities only docs is to 'pull' vectors towards meaningful words \n",
    "# in a doc, without losing the standard context they appear into\n",
    "\n",
    "# lower case training corpus too, so we don't have differences between this and entities\n",
    "train_corpus = [gensim.utils.simple_preprocess(doc['title'].lower() + doc['abstract'].lower()) for doc in docs]\n",
    "# no need to pre-process entities, just make-sure they're lower-cased\n",
    "        \n",
    "train_corpus = train_corpus + [lower_case_list(doc['result_entities']) for doc in docs]\n",
    "\n",
    "random.shuffle(train_corpus)\n",
    "\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "\n",
    "# sg ({0, 1}, optional) – Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "# negative (int, optional) – If > 0, negative sampling will be used, \n",
    "# the int for negative specifies how many “noise words” should be drawn (usually between 5-20).\n",
    "# every now and then we select a word and we ignore it by treating it as noise\n",
    "\n",
    "epochs = 30\n",
    "vec_size = 100\n",
    "entities_alpha = 0.10  \n",
    "abstract_alpha = 0.05 # here we have much more data\n",
    "MODEL_NAME = 'TestModels/w2v_entities+abstract_model.model'\n",
    "\n",
    "# let's introduce a higher min_count here, since we have a sufficient number of data\n",
    "\n",
    "# Skip-gram\n",
    "model = Word2Vec(size=vec_size, negative=5, hs=0, min_count=5, sample=0, \n",
    "        iter=epochs, workers=cores, sg = 1)\n",
    "\n",
    "\n",
    "# build our vocabulary of words (all the unique words encountered inside our corpus, needed for training)\n",
    "model.build_vocab(train_corpus)\n",
    "print(\"Vocabulary created, number of known words: \", len(model.wv.vocab))\n",
    "\n",
    "# train the models on the given data!\n",
    "\n",
    "print(\"Training %s\" % model)\n",
    "%time model.train(train_corpus, total_examples=len(train_corpus), epochs=model.iter)\n",
    "model.save(MODEL_NAME)\n",
    "\n",
    "print(\"Model Saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('marte', 0.646645188331604), ('extraterrestre', 0.6415097713470459), ('sistema solare', 0.6046188473701477), ('pianeta', 0.5946462154388428), ('iss', 0.5901428461074829), ('flora', 0.5829756855964661), ('sonda spaziale', 0.5825291872024536), ('marte (astronomia)', 0.580169677734375), ('fiume', 0.579222559928894), ('meteorite', 0.5746831297874451), ('venere', 0.568903386592865), ('rosso', 0.5658887028694153), ('luna', 0.5651825666427612)]\n",
      "[('torino', 0.6039671897888184), ('emilia', 0.5589956045150757), ('sapienza', 0.5537577867507935), ('reggio', 0.5534363389015198), ('fiumicino', 0.5531430244445801), ('fiera', 0.534198522567749), ('padova', 0.5295882225036621), ('verona', 0.5231437683105469), ('milano', 0.5223691463470459), ('bologna', 0.5136139392852783), ('lazio', 0.5101860761642456), ('piaggio', 0.5095441341400146), ('sicilia', 0.508764386177063)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imacdev/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"\n",
      "/Users/imacdev/anaconda3/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "/Users/imacdev/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# load model \n",
    "MODEL_NAME = 'TestModels/w2v_entities+abstract_model.model'\n",
    "model = Word2Vec.load(MODEL_NAME)\n",
    "# simple model testing\n",
    "print(model.most_similar('terra', topn=13))\n",
    "print(model.most_similar('roma', topn = 13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to represent a doc given its entities\n",
    "Average of entities vectors seem to perform better, especially because we can compute the distance between \n",
    "two documents defined by the same entities (also, does not depend on their order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_of_vectors(vectors):\n",
    "    \"\"\"given a list of vectors, return the simplest mean of vectors.\"\"\"\n",
    "    \n",
    "    sum_vectors = np.zeros(np.shape(vectors[0]))\n",
    "    for vec in vectors:\n",
    "        sum_vectors = sum_vectors + vec\n",
    "    return sum_vectors/len(vectors)\n",
    "\n",
    "def infer_vector(entities, model):\n",
    "    \"\"\"Given a list of entities, returns the vector representing the documents from which the entities \n",
    "    were extracted from, wrt a given W2V model.\n",
    "    \n",
    "    entities: list of entities, our way of representing a document.\n",
    "    model: w2v model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # get word vector of each entity; ignores word if the model does not know it\n",
    "    entities_vecs = []\n",
    "    for e in entities:\n",
    "        try:\n",
    "            entities_vecs.append(model[e])\n",
    "        except:\n",
    "            None # ignore unknown word\n",
    "    \n",
    "    return mean_of_vectors(entities_vecs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.35007365]]\n",
      "[[0.81275087]]\n",
      "[[1.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imacdev/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# usage test\n",
    "import sklearn.metrics.pairwise as sk\n",
    "a = infer_vector(['roma', 'blockchain', 'finanza', 'politica'], model)\n",
    "b = infer_vector(['terra', 'sole', 'spazio'], model)\n",
    "#print(model.similarity('milano', 'roma'))\n",
    "print(sk.cosine_similarity([a], [b]))\n",
    "a = infer_vector(['marte', 'stella', 'spazio', 'meteora'], model)\n",
    "print(sk.cosine_similarity([a], [b]))\n",
    "\n",
    "a = infer_vector(['sole', 'spazio', 'terra'], model)\n",
    "b = infer_vector(['terra', 'sole', 'spazio'], model)\n",
    "print(sk.cosine_similarity([a], [b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairwise distance approach\n",
    "Instead of defining a way to represent a doc, we define a distance between docs (seen as sets of WordVectors),\n",
    "very much like it is done in Hierachical Clustering with Group Averaging.\n",
    "Finding eps value will be easier, but computing this matrix might be costly for a high number of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros((2, 2)) # how to inizialize a matrix\n",
    "a[1, 1] = 1\n",
    "print(a)\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.metrics.pairwise as sk # for cosine_distance\n",
    "# TODO: add possibility of passing metric to use as parameters\n",
    "def get_pairwise_distances_matrix(docs, model, verbose = False):\n",
    "    \"\"\"\"\n",
    "        docs: list of documents, each represented as a list of entities.\n",
    "        model: w2v model used to fetch the representation of entities as word vectors.\n",
    "        verbose: print out operations.\n",
    "        \n",
    "        Returns the pairwise distances matrix between documents. \n",
    "        Distance between 2 docs will be computed by averaging the distances between all words\n",
    "        composing the 2 documents.\n",
    "        Metric used is the one used in group_averaging_distance.\n",
    "    \"\"\"\n",
    "    # initialize distance matrix\n",
    "    n = len(docs)\n",
    "    distances_m = np.zeros((n, n))\n",
    "    \n",
    "    # un-wrap each set of entities (doc) and compute the distance betweem them all\n",
    "    # this is all but efficient at the moment, okay for a debug version.\n",
    "    for i, doc1 in enumerate(docs):\n",
    "        if verbose: print(\"##Calculating distances from \", doc1)\n",
    "        for j, doc2 in enumerate(docs):\n",
    "            distances_m[i, j] = group_averaging_distance(entities_vector(doc1, model), entities_vector(doc2, model))\n",
    "            if verbose: print(\"Distance between %s and %s: %s\"%(doc1, doc2, distances_m[i, j]))\n",
    "    return distances_m\n",
    "    \n",
    "def group_averaging_distance(doc1, doc2):\n",
    "    \"\"\"\n",
    "        Computes and returns the distance between 2 'sets'/lists of vectors, \n",
    "        by computing the distance between a vector in doc1 and all the other in doc2,\n",
    "        and averaging all these distances.\n",
    "        Metric used to compute distance is the cosine_distance -by default-.\n",
    "    \"\"\"\n",
    "    sum_of_distances = 0\n",
    "    for vec1 in doc1:\n",
    "        for vec2 in doc2:\n",
    "            sum_of_distances += sk.cosine_distances([vec1], [vec2])\n",
    "    return sum_of_distances/(len(doc1) * len(doc2))\n",
    "\n",
    "def entities_vector(doc, model):\n",
    "    \"\"\"\"\n",
    "    Doc: document, represented as a list of entities.\n",
    "    model: w2v model used to fetch representation of each vector.\n",
    "    \n",
    "    Given these two arguments, returns a list of vectors, each vector representing \n",
    "    an entity word.\n",
    "    In case the model does NOT know the word in the list, it will be ignored.\n",
    "    Might return an empty list.\n",
    "    \"\"\"\n",
    "    ## TODO: print-out unknown words!\n",
    "    list_ = []\n",
    "    for word in doc:\n",
    "        try:\n",
    "            v = model[word]\n",
    "            list_.append(v)\n",
    "        except:\n",
    "            print(\"Unknown word found!\")\n",
    "    return list_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Calculating distances from  ['terra', 'sole', 'spazio']\n",
      "Distance between ['terra', 'sole', 'spazio'] and ['terra', 'sole', 'spazio']: 0.41432106494903564\n",
      "Distance between ['terra', 'sole', 'spazio'] and ['roma', 'blockchain', 'finanza', 'politica']: 0.8169827461242676\n",
      "##Calculating distances from  ['roma', 'blockchain', 'finanza', 'politica']\n",
      "Distance between ['roma', 'blockchain', 'finanza', 'politica'] and ['terra', 'sole', 'spazio']: 0.8169827461242676\n",
      "Distance between ['roma', 'blockchain', 'finanza', 'politica'] and ['roma', 'blockchain', 'finanza', 'politica']: 0.5215639472007751\n",
      "[[0.41432106 0.81698275]\n",
      " [0.81698275 0.52156395]]\n",
      "##Calculating distances from  ['terra', 'sole', 'spazio']\n",
      "Distance between ['terra', 'sole', 'spazio'] and ['terra', 'sole', 'spazio']: 0.41432106494903564\n",
      "Distance between ['terra', 'sole', 'spazio'] and ['marte', 'stella', 'spazio', 'meteora']: 0.5227131247520447\n",
      "##Calculating distances from  ['marte', 'stella', 'spazio', 'meteora']\n",
      "Distance between ['marte', 'stella', 'spazio', 'meteora'] and ['terra', 'sole', 'spazio']: 0.5227130651473999\n",
      "Distance between ['marte', 'stella', 'spazio', 'meteora'] and ['marte', 'stella', 'spazio', 'meteora']: 0.42349866032600403\n",
      "[[0.41432106 0.52271312]\n",
      " [0.52271307 0.42349866]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imacdev/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:59: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# test \n",
    "a = get_pairwise_distances_matrix([['terra', 'sole', 'spazio'], ['roma', 'blockchain', 'finanza', 'politica']], model, True)\n",
    "print(a)\n",
    "a = get_pairwise_distances_matrix([['terra', 'sole', 'spazio'], ['marte', 'stella', 'spazio', 'meteora']], model, True)\n",
    "print(a)\n",
    "#print(group_averaging_distance([model['roma'], model['milano']], [model['roma'], model['milano']]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
