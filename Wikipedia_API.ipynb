{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
      "Requirement already satisfied: beautifulsoup4 in /home/nick/anaconda3/lib/python3.6/site-packages (from wikipedia) (4.6.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /home/nick/anaconda3/lib/python3.6/site-packages (from wikipedia) (2.18.4)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/nick/anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /home/nick/anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /home/nick/anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nick/anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2018.4.16)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Running setup.py bdist_wheel for wikipedia ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/nick/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
      "Successfully built wikipedia\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downside Up\n",
      "Il Real Madrid Club de Fútbol, più semplicemente noto come Real Madrid, è una società polisportiva spagnola la cui fama mondiale è determinata soprattutto dalla sua sezione calcistica.\n",
      "Fondato a Madrid il 6 marzo 1902, con la denominazione ufficiale di Madrid Club de Fútbol, si vide assegnare il titolo di Real nel 1920, dal re Alfonso XIII di Spagna, insieme all’ormai famigerata corona a decorarne e arricchirne lo stemma.\n",
      "La squadra di calcio milita in Primera División fin dalla prima edizione del torneo nella stagione 1928-29 e può vantare il palmarès più prestigioso al mondo, relativamente ai campionati più competitivi.\n"
     ]
    }
   ],
   "source": [
    "print(wikipedia.random(pages = 1))\n",
    "try:\n",
    "    print(wikipedia.summary(\"Real Madrid\", sentences = 3))\n",
    "except wikipedia.exceptions.DisambiguationError as e:\n",
    "    wikipedia.summary(e.options[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Freddie Mercury']\n",
      "Freddie Mercury, pseudonimo di Farrokh Bulsara (Zanzibar, 5 settembre 1946 – Londra, 24 novembre 1991), è stato un cantautore, musicista e compositore britannico di origini parsi.\n",
      "Ricordato per il talento vocale e la sua esuberante personalità sul palco, è considerato uno dei più celebri e influenti artisti nella storia del rock: universalmente riconosciuto come uno dei migliori frontman nella storia della musica, nel 2008 la rivista statunitense Rolling Stone lo classificò 18º nella classifica dei migliori cento cantanti di tutti i tempi, mentre l'anno successivo Classic Rock lo classificò al primo posto tra le voci rock.Fu fondatore nel 1970 dei Queen, gruppo rock britannico di cui fece parte fino alla morte. Per i Queen fu autore della maggior parte dei brani, tra i quali si annoverano successi come Bohemian Rhapsody, Crazy Little Thing Called Love, Don't Stop Me Now, It's a Hard Life, Killer Queen, Love of My Life, Play the Game, Somebody to Love e We Are the Champions. Oltre all'attività con i Queen, negli anni ottanta intraprese la carriera solista con la pubblicazione di due album, Mr. Bad Guy (1985) e Barcelona (1988), quest'ultimo frutto della collaborazione con la cantante soprano spagnola Montserrat Caballé, il cui singolo omonimo divenne l'inno ufficiale dei Giochi della XXV Olimpiade svoltisi a Barcellona.Ammalatosi di AIDS, sviluppò a causa di ciò una grave broncopolmonite che lo portò alla morte, sopravvenuta il giorno seguente alla pubblica dichiarazione del suo grave stato di salute.\n",
      "In suo onore, il 20 aprile 1992 fu organizzato il Freddie Mercury Tribute Concert, al quale parteciparono molti artisti musicali internazionali; i proventi dell'evento furono utilizzati per fondare The Mercury Phoenix Trust, organizzazione impegnata nella lotta all'HIV, il virus alla base della sindrome da immunodeficienza acquisita.\n",
      "New York (AFI: /njuˈ\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "\n",
    "#print(wikipedia.search(\"Barack\", results=1))\n",
    "wikipedia.suggest(\"Mercury (element)\") #doesn't do much\n",
    "try:\n",
    "    print(wikipedia.search(\"Mercury\", results = 1))\n",
    "    print(wikipedia.summary(\"Mercury\", sentences = 6))\n",
    "except wikipedia.exceptions.DisambiguationError as e:\n",
    "    # if dimbaguations page shows up, take the first document\n",
    "    print(wikipedia.summary(e.options[0], sentences=6))\n",
    "    \n",
    "# get wikipedia page categorie's\n",
    "try:\n",
    "    ny = wikipedia.page('New York')\n",
    "except wikipedia.exceptions.DisambiguationError as e:\n",
    "    ny = wikipedia.page(e.options[0])\n",
    "print(ny.content[:20])\n",
    "#print(ny.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia-api\n",
      "  Downloading https://files.pythonhosted.org/packages/60/f1/8df8ce6885d1aec74a3c6169077086cd00868d5866c5a2225644a05037bf/Wikipedia-API-0.3.7.tar.gz\n",
      "Requirement already satisfied: requests in /home/nick/anaconda3/lib/python3.6/site-packages (from wikipedia-api) (2.18.4)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/nick/anaconda3/lib/python3.6/site-packages (from requests->wikipedia-api) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /home/nick/anaconda3/lib/python3.6/site-packages (from requests->wikipedia-api) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /home/nick/anaconda3/lib/python3.6/site-packages (from requests->wikipedia-api) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nick/anaconda3/lib/python3.6/site-packages (from requests->wikipedia-api) (2018.4.16)\n",
      "Building wheels for collected packages: wikipedia-api\n",
      "  Running setup.py bdist_wheel for wikipedia-api ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/nick/.cache/pip/wheels/cd/72/d5/465cd0913fcb1470633326011d9b758bc1c592ce68b3bb7a98\n",
      "Successfully built wikipedia-api\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "Installing collected packages: wikipedia-api\n",
      "Successfully installed wikipedia-api-0.3.7\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install wikipedia-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test-set with documents from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles length:  27\n",
      "['Microsoft', 'Lampone', 'Casa Bianca', 'Italia', 'Amazon', 'Berlino', 'Colosseo', 'New York', 'Instagram', 'Ciliegia', 'Mela', 'Parigi', 'Pasta', 'Londra', 'Bologna', 'Roma', 'Melone', 'Banana', 'Facebook', 'USA', 'Pizza', 'Milano', 'Quirinale', 'Pera', 'Google', 'Apple', 'Hot dog']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import wikipedia\n",
    "filename = 'wikipedia_dump.json'\n",
    "# downloading pages from the italian version of wikipedia\n",
    "wikipedia.set_lang('it')\n",
    "\n",
    "# create data structure to later dump as json file\n",
    "docs = [] # this list will hold dictionaries in the form of {'title':__, 'abstract':__, 'flattened_entities':__}\n",
    "\n",
    "# we'll fetch articles coming from this different fields\n",
    "cities = ['Parigi', 'Londra', 'Roma', 'Berlino', 'Milano', 'Bologna']\n",
    "fruits = ['Mela', 'Pera', 'Banana', 'Ciliegia', 'Lampone', 'Melone']\n",
    "companies = ['Apple', 'Microsoft','Google', 'Facebook', 'Instagram', 'Amazon']\n",
    "mixed_correalations = ['USA', 'Italia', 'Colosseo', 'New York', 'Casa Bianca', 'Quirinale', 'Pasta', 'Pizza', 'Hot dog']\n",
    "\n",
    "# make a single list shuffling all elements\n",
    "articles = cities + fruits + companies + mixed_correalations\n",
    "print(\"Articles length: \",len(articles))\n",
    "random.shuffle(articles)\n",
    "print(articles)\n",
    "\n",
    "# download each document in the list; document size will vary based on a random number of sentences returned\n",
    "# by wikipedia APIs\n",
    "for article in articles:\n",
    "    sentences = random.randint(1, 10)\n",
    "    text = ''\n",
    "    try:\n",
    "        text = wikipedia.summary(article, sentences = sentences)\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        # if dimbaguations page shows up, take the first document\n",
    "        text = wikipedia.summary(e.options[0], sentences = sentences)\n",
    "    # create new dict with title and text of wikipedia page\n",
    "    docs.append({'title': article, 'abstract': text, 'flattened_entities':''})\n",
    "    \n",
    "with open(filename, 'w') as outfile:\n",
    "    json.dump(docs, outfile)\n",
    "#print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 documents testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles length:  81\n",
      "[('Infante', 'Figlio'), ('Uomo', 'Donna'), ('Samsung', 'LG'), ('Milano', 'Bologna'), ('Re', 'Principe'), ('Roma', 'Berlino'), ('Armi', 'Pistola'), ('Brodo', 'Zuppa'), ('Incendio', 'Bruciare'), ('Questione', 'Domanda'), ('iPad', 'iPhone'), ('Chino', 'Curvo'), ('Regina', 'Principessa'), ('Ghepardo', 'Tigre'), ('Cadere', 'Cascare'), ('Instagram', 'Facebook'), ('RHCP', 'Guns n roses'), ('Italiani', 'Italia'), ('Cucciolo', 'Bambino'), ('Intelligenza Artificiale', 'Machine Learning'), ('Cane', 'Gatto'), ('Juventus', 'Real Madrid'), ('Corto', 'Succinto'), ('Parigi', 'Torre Eiffel'), ('Leopardo', 'Pantera nera'), ('Molto', 'Assai'), ('Astronomia', 'Pianeta'), ('Ginnasio', 'Palestra'), ('Vita Nuova', 'Divina Commedia'), ('Benestante', 'Ricco'), ('NASA', 'Marte'), ('Lampone', 'Melone'), ('Google', 'Amazon'), ('Cane', 'Lupo'), ('Principe', 'Principessa'), ('Android', 'Smartphone'), ('Chiocciola', 'Lumaca'), ('Gallus gallus domesticus', 'Tacchino'), ('Madre', 'Figlia'), ('Guelfi', 'Ghibellini'), ('Leopardo', 'Puma'), ('Banana', 'Ciliegia'), ('Bambino', 'Bambina'), ('Figlio', 'Bambino'), ('Apple', 'Microsoft'), ('Parigi', 'Londra'), ('Maiale', 'Cinghiale'), ('iPhone', 'Smartphone'), ('Fuoco', 'Fiamme'), ('Cina', 'Pechino'), ('Zio', 'Zia'), ('Tigre', 'Leopardo'), ('Francesi', 'Francia'), ('Mucca', 'Toro'), ('Marte (astronomia)', 'Venere (astronomia)'), ('Bestia', 'Animale'), ('Matematica', 'Fisica'), ('Robot', 'Intelligenza Artificiale'), ('Amazon', 'e-commerce'), ('Figlio', 'Padre'), ('Fratello', 'Sorella'), ('Acqua', 'Ghiaccio'), ('Mela', 'Pera'), ('Sogno', 'Sognare'), ('Intel', 'AMD'), ('Matematica', 'Ingegneria'), ('Padre', 'Nonno'), ('Vento', 'Uragano'), ('Francia', 'Italia'), ('Mark Zuckerberg', 'Facebook'), ('Armata', 'Esercito'), ('Madre', 'Nonna'), ('Piccione', 'Rondine'), ('Cavallo', 'Pony'), ('Valvassore', 'Valvassino'), ('Nonno', 'Nipote'), ('Cometa', 'Stella'), ('Apple', 'Samsung'), ('Cappello', 'Berretto'), ('Re', 'Regina'), ('Imperatore', 'Imperatrice')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /home/nick/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curva\n",
      "Regina\n",
      "Bestia\n",
      "monarca\n",
      "File saved\n"
     ]
    }
   ],
   "source": [
    "# get 2 similar documents, and a random one\n",
    "import json\n",
    "import random\n",
    "import wikipedia\n",
    "filename = 'wikipedia_3docs_dump.json'\n",
    "# downloading pages from the italian version of wikipedia\n",
    "wikipedia.set_lang('it')\n",
    "\n",
    "# create data structure to later dump as json file\n",
    "docs = [] # this list will hold multiple similar docs threes\n",
    "similar_docs = []# this list will hold dictionaries in the form of {'title':__, 'abstract':__, 'flattened_entities':__}\n",
    "\n",
    "# we'll fetch articles coming from this different fields\n",
    "cities = [('Parigi', 'Londra'), ('Roma', 'Berlino'), ('Milano', 'Bologna')]\n",
    "fruits = [('Mela', 'Pera'), ('Banana', 'Ciliegia'), ('Lampone', 'Melone')]\n",
    "companies = [('Apple','Microsoft'), ('Google', 'Amazon'), ('Instagram', 'Facebook')]\n",
    "mixed_correlations = [('Fuoco', 'Fiamme'), ('Acqua', 'Ghiaccio'), ('Vento', 'Uragano'), \n",
    "                      ('Mark Zuckerberg', 'Facebook'), ('Marte (astronomia)', 'Venere (astronomia)'), ('NASA', 'Marte'),('Cometa', 'Stella'),\n",
    "                      ('Juventus', 'Real Madrid'), ('iPad','iPhone'), ('Intel', 'AMD'), ('Cina', 'Pechino'),\n",
    "                      ('Robot', 'Intelligenza Artificiale'), ('Intelligenza Artificiale', 'Machine Learning'), \n",
    "                      ('iPhone', 'Smartphone'), ('Amazon', 'e-commerce'), ('Android', 'Smartphone'),\n",
    "                      ('Apple', 'Samsung'), ('Samsung', 'LG'), ('Astronomia', 'Pianeta'),\n",
    "                      ('Sogno', 'Sognare'), ('Matematica', 'Fisica'), ('Francia', 'Italia'), ('Matematica', 'Ingegneria'),\n",
    "                      ('Vita Nuova', 'Divina Commedia'), ('RHCP', 'Guns n roses'), ('Chino', 'Curvo'),\n",
    "                      ('Armata', 'Esercito'), ('Armi', 'Pistola'), ('Benestante', 'Ricco'), \n",
    "                      ('Corto', 'Succinto'), ('Ginnasio','Palestra'), ('Questione', 'Domanda'),\n",
    "                      ('Re','Regina'), ('Imperatore','Imperatrice'), ('Principe','Principessa'), ('Re','Principe'),\n",
    "                      ('Regina','Principessa'), ('Valvassore','Valvassino'), ('Guelfi','Ghibellini'), \n",
    "                      ('Francesi','Francia'), ('Italiani','Italia'), ('Cadere','Cascare'), ('Molto', 'Assai'),\n",
    "                      ('Uomo','Donna'), ('Figlio','Padre'), ('Bambino','Bambina'), ('Figlio','Bambino'), \n",
    "                      ('Cucciolo','Bambino'), ('Infante', 'Figlio'), ('Madre', 'Figlia'), ('Fratello', 'Sorella'),\n",
    "                      ('Zio', 'Zia'), ('Padre', 'Nonno'), ('Madre', 'Nonna'), ('Nonno', 'Nipote'), \n",
    "                      ('Cappello', 'Berretto'), ('Brodo', 'Zuppa'), ('Incendio', 'Bruciare'), ('Parigi', 'Torre Eiffel')\n",
    "                     ]\n",
    "animals = [('Chiocciola','Lumaca'), ('Cane','Gatto'), ('Cane','Lupo'), ('Tigre','Leopardo'), ('Leopardo','Puma'),\n",
    "          ('Ghepardo', 'Tigre'), ('Cavallo', 'Pony'), ('Mucca', 'Toro'), ('Bestia', 'Animale'), \n",
    "           ('Gallus gallus domesticus', 'Tacchino'),\n",
    "           ('Piccione', 'Rondine'), ('Maiale', 'Cinghiale'), ('Leopardo', 'Pantera nera')\n",
    "          ]\n",
    "\n",
    "# make a single list shuffling all elements\n",
    "articles = cities + fruits + companies + mixed_correlations + animals\n",
    "print(\"Articles length: \",len(articles))\n",
    "random.shuffle(articles)\n",
    "print(articles)\n",
    "\n",
    "# download each document in the list; document size will vary based on a random number of sentences returned\n",
    "# by wikipedia APIs\n",
    "for (art1, art2) in articles:\n",
    "    sentences = random.randint(3, 8)\n",
    "    similar_docs = []\n",
    "    text0 = ''\n",
    "    text1 = ''\n",
    "    text2 = ''\n",
    "    page_random = ''\n",
    "    # get the summary of the two 'linked' documents\n",
    "    try:\n",
    "        text0 = wikipedia.summary(art1, sentences = sentences)\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        # if dimbaguations page shows up, take the first document\n",
    "        print(art1)\n",
    "        text0 = wikipedia.summary(e.options[0], sentences = sentences)\n",
    "        \n",
    "    sentences = random.randint(3, 8)\n",
    "    try:\n",
    "        text1 = wikipedia.summary(art2, sentences = sentences)\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        # if dimbaguations page shows up, take the first document\n",
    "        print(e.options[0])\n",
    "        text1 = wikipedia.summary(e.options[0], sentences = sentences)\n",
    "        \n",
    "    # get a random article\n",
    "    try:\n",
    "        page_random = wikipedia.random(pages = 1)\n",
    "        sentences = random.randint(3, 8)\n",
    "        text2 = wikipedia.summary(page_random, sentences = sentences)\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        # if dimbaguations page shows up, take another one\n",
    "        page_random = wikipedia.random(pages = 1)\n",
    "        text2 = wikipedia.summary(page_random, sentences = sentences)\n",
    "            \n",
    "    # create new dict with title and text of the 3 wikipedia pages    \n",
    "    similar_docs.append({'title': art1, 'abstract': text0, 'flattened_entities':''})\n",
    "    similar_docs.append({'title': art2, 'abstract': text1, 'flattened_entities':''})\n",
    "    similar_docs.append({'title': page_random, 'abstract': text2, 'flattened_entities':''})\n",
    "    \n",
    "    docs.append(similar_docs)\n",
    "with open(filename, 'w') as outfile:\n",
    "    json.dump(docs, outfile)\n",
    "print(\"File saved\")\n",
    "#print(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
