{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA imports\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec imports\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import gensim\n",
    "\n",
    "import json # to open our data file\n",
    "DATA_FILENAME = \"trend_analisys.json\"\n",
    "# open json file\n",
    "with open(DATA_FILENAME, \"r\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "# we're expecting a list now, since our json file is a json array\n",
    "assert type(json_data) is list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1 - Training Model with Abstract field (whole text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 293, number of train examples: 234, number of test examples: 59\n",
      "[TaggedDocument(words=['il', 'mercato', 'degli', 'smartphone', 'si', 'fa', 'ogni', 'giorno', 'più', 'ricco', 'le', 'possibilità', 'di', 'scelta', 'tra', 'prodotti', 'di', 'buon', 'ottima', 'qualità', 'prezzi', 'diversi', 'sono', 'in', 'costante', 'crescita', 'certamente', 'tra', 'gli', 'smartphone', 'che', 'vi', 'consigliamo', 'prendere', 'in', 'considerazione', 'tra', 'quelli', 'usciti', 'di', 'recente', 'il', 'thinq', 'di', 'lg', 'sottile', 'elegante', 'il', 'modello', 'che', 'abbiamo', 'provato', 'aveva', 'un', 'elegantissimo', 'colore', 'blu', 'comodo', 'di', 'dimensioni', 'giuste', 'il', 'thinq', 'ha', 'tutte', 'le', 'caratteristiche', 'necessarie', 'per', 'accontentare', 'anche', 'il', 'pubblico', 'più', 'esigente', 'mobile', 'platform', 'qualcomm', 'snapdragon', 'gb', 'di', 'ram', 'gb', 'di', 'memoria', 'interna', 'espandibile', 'ovvia', 'dotazione', 'di', 'accelerometro', 'giroscopio', 'magnetometro', 'sensore', 'di', 'luminosità', 'prossimità', 'barometro', 'lettore', 'di', 'impronte', 'digitale', 'capacità', 'di', 'riconoscimento', 'facciale', 'il', 'display', 'da', 'pollici', 'decisamente', 'luminoso', 'con', 'pixel', 'non', 'mancano', 'lte', 'wifi', 'ac', 'bluetooth', 'nfc', 'gps', 'radio', 'fm', 'jack', 'audio', 'una', 'porta', 'usb', 'type', 'la', 'ricarica', 'wireless', 'ha', 'un', 'comodo', 'tasto', 'sulla', 'sinistra', 'per', 'richiamare', 'immediatamente', 'google', 'assistant', 'detto', 'questo', 'com', 'davvero', 'il', 'thinq', 'una', 'buonissima', 'macchina', 'con', 'un', 'eccellente', 'rapporto', 'prezzo', 'qualità', 'che', 'non', 'cede', 'nemmeno', 'un', 'istante', 'anche', 'quando', 'sotto', 'pressione', 'viene', 'usato', 'in', 'maniera', 'intensiva', 'sostenuto', 'da', 'una', 'buonissima', 'qualità', 'audio', 'usa', 'anche', 'la', 'tecnologia', 'boom', 'box', 'che', 'amplifica', 'la', 'potenza', 'sonora', 'dello', 'speaker', 'che', 'si', 'trova', 'sul', 'bordo', 'inferiore', 'facendo', 'vibrare', 'tutta', 'la', 'scocca', 'basta', 'quindi', 'appoggiare', 'lo', 'smartphone', 'su', 'una', 'superficie', 'piana', 'per', 'ottenere', 'un', 'discreto', 'aumento', 'della', 'potenza', 'sonora', 'il', 'display', 'uno', 'dei', 'punti', 'di', 'forza', 'del', 'device', 'un', 'wlcd', 'ips', 'da', 'sei', 'pollici', 'con', 'una', 'risoluzione', 'qhd', 'ha', 'come', 'dicevamo', 'una', 'notevolissima', 'luminosità', 'superiore', 'alla', 'media', 'soprattutto', 'se', 'usato', 'in', 'condizioni', 'di', 'forte', 'illuminazione', 'assomiglia', 'tutti', 'gli', 'altri', 'smartphone', 'in', 'circolazione', 'ma', 'questo', 'ormai', 'talmente', 'ovvio', 'da', 'non', 'essere', 'un', 'difetto', 'pesa', 'poco', 'più', 'di', 'un', 'etto', 'mezzo', 'questo', 'gioca', 'suo', 'favore', 'soprattutto', 'se', 'si', 'usa', 'molto', 'il', 'device', 'resistente', 'ad', 'acqua', 'polvere', 'non', 'scivola', 'con', 'facilità', 'dalle', 'mani', 'ed', 'entra', 'bene', 'in', 'tasca', 'arriviamo', 'dunque', 'alla', 'fotocamera', 'anzi', 'alle', 'fotocamere', 'perché', 'lg', 'thinq', 'ne', 'ha', 'due', 'posteriori', 'da', 'mp', 'una', 'standard', 'stabilizzata', 'otticamente', 'una', 'grandangolare', 'sul', 'frontale', 'ha', 'una', 'singola', 'fotocamera', 'da', 'mp', 'le', 'foto', 'sono', 'di', 'buona', 'qualità', 'sia', 'in', 'condizioni', 'normali', 'che', 'di', 'scarsa', 'illuminazione', 'con', 'un', 'buon', 'dettaglio', 'un', 'altrettanto', 'buon', 'contrasto', 'non', 'manca', 'ovviamente', 'intelligenza', 'artificiale', 'che', 'permette', 'alla', 'macchina', 'di', 'riconoscere', 'le', 'scene', 'ma', 'anche', 'una', 'modalità', 'notturna', 'che', 'combina', 'quattro', 'pixel', 'per', 'catturare', 'meglio', 'la', 'luce', 'ma', 'il', 'punto', 'di', 'forza', 'del', 'thinq', 'sono', 'video', 'con', 'una', 'modalità', 'manuale', 'una', 'cinematica', 'abbiamo', 'utilizzato', 'in', 'molte', 'situazioni', 'diverse', 'risultati', 'sono', 'stati', 'tutti', 'molto', 'buoni', 'soprattutto', 'quando', 'come', 'al', 'concerto', 'romano', 'di', 'roger', 'waters', 'le', 'condizioni', 'di', 'illuminazione', 'erano', 'particolari', 'per', 'chi', 'ama', 'realizzare', 'filmati', 'più', 'che', 'fotografie', 'un', 'device', 'assolutamente', 'da', 'consigliare', 'la', 'batteria', 'da', 'mah', 'non', 'proprio', 'il', 'massimo', 'per', 'reggere', 'tutto', 'equipaggiamento', 'dello', 'smartphone', 'ma', 'come', 'tutti', 'sanno', 'dipende', 'molto', 'dall', 'uso', 'che', 'si', 'fa', 'del', 'device', 'diciamo', 'che', 'con', 'un', 'forte', 'uso', 'della', 'macchina', 'si', 'rischia', 'di', 'arrivare', 'sera', 'scarichi', 'nel', 'confronto', 'con', 'due', 'principali', 'concorrenti', 'il', 'samsung', 'galaxy', 'plus', 'il', 'huawei', 'pro', 'si', 'comporta', 'molto', 'bene', 'ed', 'una', 'valida', 'alternativa', 'anche', 'se', 'sul', 'fronte', 'della', 'fotocamera', 'il', 'leggermente', 'superiore', 'sul', 'fronte', 'della', 'batteria', 'il', 'samsung', 'migliore', 'ma', 'tutto', 'sommato', 'nei', 'sono', 'pochi', 'in', 'una', 'macchina', 'ben', 'realizzata', 'certamente', 'molto', 'funzionale'], tags=[0])]\n"
     ]
    }
   ],
   "source": [
    "# we have our json data now, let's go ahead and divide into training and test set\n",
    "n_examples =  len(json_data)\n",
    "# how much of the data we're going to be using for training and for testing\n",
    "# default values: 80% train, 20% test\n",
    "TRAIN_DATA_LENGTH = 8 * n_examples // 10\n",
    "TEST_DATA_LENGTH = n_examples - TRAIN_DATA_LENGTH\n",
    "ABSTRACT_FIELD_NAME = 'abstract'\n",
    "\n",
    "print(\"Total examples: {0}, number of train examples: {1}, number of test examples: {2}\".format(n_examples,TRAIN_DATA_LENGTH, TEST_DATA_LENGTH))\n",
    "\n",
    "# TODO: Randomize selection of examples, don't just take the first ones\n",
    "# build training corpus: take the needed abstract, preprocess them (tokenize, delete spaces..)\n",
    "# and create the TaggedDocument needed for training\n",
    "train_corpus = [gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(dictionary[ABSTRACT_FIELD_NAME]), [i]) for i, dictionary in enumerate(json_data) if i<TRAIN_DATA_LENGTH]\n",
    "assert len(train_corpus)==TRAIN_DATA_LENGTH\n",
    "print(train_corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 50\n",
      "iteration 100\n"
     ]
    }
   ],
   "source": [
    "# create the doc2vec model\n",
    "# TODO: tune this parameters (personally, I think we could use a bigger vec_size, like 50)\n",
    "max_epochs = 300\n",
    "vec_size = 35\n",
    "alpha = 0.030\n",
    "MODEL_NAME = \"d2v_trend_analisys_model.model\"\n",
    "\n",
    "model = Doc2Vec(vector_size=vec_size,\n",
    "                alpha=alpha,\n",
    "                min_alpha=0.00030,\n",
    "                min_count=2,\n",
    "                dm=0) #dm=0 means \"distributed bag of words\"\n",
    "# build our vocabulary of words (all the uniques words encountered inside our corpus, needed for training)\n",
    "model.build_vocab(train_corpus)\n",
    "\n",
    "# train the model on the given data!\n",
    "for epoch in range(max_epochs):\n",
    "    if(epoch%50==0):\n",
    "        print('iteration {0}'.format(epoch))\n",
    "    model.train(train_corpus,\n",
    "                total_examples = model.corpus_count,\n",
    "                epochs = model.epochs)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(MODEL_NAME)\n",
    "print(\"Model Saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Data\n",
    "credits: https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try to visualize all document_vectors\n",
    "# get all vectors of documents we created from model training\n",
    "docs_vecs = []\n",
    "# docvecs (list of Doc2VecKeyedVectors) \n",
    "# – Vector representations of the documents in the corpus. Each vector has size == vector_size\n",
    "for doc in iter(range(0, len(model.docvecs))):\n",
    "    docs_vecs.append(model.docvecs[doc])\n",
    "\n",
    "# loading dataset into Pandas DataFrame\n",
    "df = pd.DataFrame.from_records(docs_vecs)\n",
    "df.head()\n",
    "\n",
    "#df[['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA is effected by scale so you need to scale the features in your data before applying PCA. \n",
    "features = [i for i in range(vec_size)]\n",
    "\n",
    "x = df.loc[:, features].values # get features values\n",
    "print(x)\n",
    "# we don't have target here y = df.loc[:,['target']].values # get target values (guess kind of flower/Iris)\n",
    "\n",
    "# standardize data\n",
    "x = StandardScaler().fit_transform(x) # scale data (especially in case different measures are used)\n",
    "# pd.DataFrame(data = x, columns = features).head() # show first data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Projection with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build PCA model in 2D\n",
    "pca = PCA(n_components=2) # The new components are just the two main dimensions of variation.\n",
    "\n",
    "principalComponents = pca.fit_transform(x)\n",
    "\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "principalDf.head()\n",
    "# these components drawn don't hold a lot of information 'per-se', they're just the result \n",
    "# of dimension-reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now visualize the data by plotting them\n",
    "\n",
    "# finalDf is the final DataFrame before plotting the data (nothing great, just taking result of PCA and attaching target to it)\n",
    "#finalDf = pd.concat([principalDf, df[['target']]], axis = 1)\n",
    "\n",
    "finalDf = principalDf # we don't have targets to show\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 Component PCA', fontsize = 20)\n",
    "\n",
    "ax.scatter(finalDf.loc[:, 'principal component 1']\n",
    "          , finalDf.loc[:,'principal component 2']\n",
    "          , c = 'r'\n",
    "          , s = 50)\n",
    "\n",
    "#targets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "#colors = ['r', 'g', 'b']\n",
    "#for target, color in zip(targets, colors):\n",
    "#    indicesToKeep = finalDf['target'] == target\n",
    "#    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "#               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "#               , c = color\n",
    "#               , s = 50)\n",
    "#ax.legend(targets)\n",
    "ax.grid()\n",
    "print(\"Number of points shown \", len(finalDf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "tls.set_credentials_file(username='D4nt3', api_key='FdMB4O6qCfciGDOnLvdQ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
