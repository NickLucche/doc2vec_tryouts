{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True # change this is if you don't need to display print()/log in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed libraries\n",
    "import json\n",
    "import random\n",
    "#import numpy as np\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import  gensim\n",
    "from collections import Counter\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load documents from JSON\n",
    "change this block to load from a pre-defined filename of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents:  96\n",
      "Number of documents:  604\n"
     ]
    }
   ],
   "source": [
    "filenames = ['blockchain.json', 'industria_4.0.json']\n",
    "\n",
    "# load multiple files, assuming same data format\n",
    "docs = []\n",
    "for filename in filenames:\n",
    "    with open(filename, 'r') as outfile:\n",
    "        json_data = json.load(outfile)\n",
    "\n",
    "    ## let's now retrieve the meaningful part of the json document\n",
    "    # response{}--->docs[] \n",
    "    ## -that's the way I was given JSON docs so far, change this part if format changes-\n",
    "\n",
    "    docs = docs + json_data['response']['docs']\n",
    "    if verbose:\n",
    "        print(\"Number of documents: \",len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This part can be ignored if we assume data is \"clean\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New length after removing docs:  412\n"
     ]
    }
   ],
   "source": [
    "## many documents have a failed abstract, let's remove them\n",
    "to_check = ' Questo sito web utilizza cookie tecnici e, previo Suo consenso, cookie di profilazione,'\n",
    "docs = [doc for i, doc in enumerate(docs) if not(to_check.strip() in doc['abstract'][0].strip())]\n",
    "\n",
    "if verbose:\n",
    "    print(\"New length after removing docs: \", len(docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Length:  362\n",
      "['L’annuncio è arrivato alla London Fintech News ed è stato rilanciato dalla testata The Fintech Times: il primo comunicatore crypto in versione Mobile per la Blockchain che con qualche forzatura si può...']\n"
     ]
    }
   ],
   "source": [
    "# List->String\n",
    "## Adjust data format: title and abstract came in as list, but they're more useful as strings\n",
    "for i, dictionary in enumerate(docs):\n",
    "    for field in ['title', 'abstract']:\n",
    "        if isinstance(dictionary[field], list):\n",
    "            # re-format data to hold string instead of single-list item\n",
    "            docs[i][field] = dictionary[field][0]\n",
    "            \n",
    "# remove duplicates\n",
    "docs = [doc for doc in docs\n",
    "            if not(\"Industry 4.0 (o industria 4.0): cos'è, notizie, normative, casi studio - I4T\" in doc['title'])]\n",
    "\n",
    "if verbose:\n",
    "    print(\"New Length: \", len(docs))\n",
    "\n",
    "# double check to be sure\n",
    "for doc in docs:\n",
    "    if to_check.strip() in doc['abstract'].strip():\n",
    "        print(\"cookie doc found\")\n",
    "if verbose:\n",
    "    print([d['abstract'][:200]+'...' for d in docs[:1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer Vectors from documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and prepara data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9031\n",
      "362\n",
      "['Con Laboratorio RISE e Vendor verso le “Best Practice 4.0” - Industry4Business', 'Una Academy per insegnare Industry 4.0 alle medie imprese - Industry4Business', \"Nokia volta pagina e punta (anche) sull'Iot: acquisita SpaceTime Insight - CorCom\", 'Servitization e smart product così la Industry 4.0 diventa realtà', 'Per innovare la pubblica amministrazione serve lo smart working: ecco perché']\n"
     ]
    }
   ],
   "source": [
    "# shuffle documents\n",
    "random.shuffle(docs)\n",
    "\n",
    "## !Change this if you want to rename model or change dir in the filesystem ##\n",
    "MODEL_NAME = 'TestModels/d2v_TA_abstract&title0.model'\n",
    "model = Doc2Vec.load(MODEL_NAME)\n",
    "\n",
    "# print out dimension of the vocabulary of the model (number of known words)\n",
    "if verbose:\n",
    "    print(len(model.wv.vocab))\n",
    "    \n",
    "# infer vectors from data\n",
    "# preprocess data first (remove capitals, strange unicode chars..)\n",
    "## title + abstract may change in future versions to flattened_entities, with a newer model!\n",
    "test_corpus = [gensim.utils.simple_preprocess(d['title']+d['abstract']) for d in docs]\n",
    "if verbose: \n",
    "    print(len(test_corpus))\n",
    "    \n",
    "# list of vectors of docs\n",
    "inferred_vectors = [model.infer_vector(doc) for doc in test_corpus]\n",
    "\n",
    "# get docs titles, needed to return results correctly after clustering\n",
    "titles = [doc['title'] for doc in docs]\n",
    "if verbose: \n",
    "    print(titles[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN \n",
    "density-based algorithm used to clusterize docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my function for performing dbscan and printing out cluster results\n",
    "def perform_dbscan(eps = 0.4, min_samples = 4, metric = 'euclidean', algorithm = 'auto', data = None, verbose = True\n",
    "                  , titles = None, print_noise = True):\n",
    "    \"\"\"perform DBSCAN over given data, using given parametrs. Returns dbscan object.\"\"\"\n",
    "    \n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples, metric=metric, algorithm=algorithm).fit(data)\n",
    "\n",
    "    # labels will print out the number of the cluster each example belongs to;\n",
    "    # -1 if the vector is considered noise (not belonging to any cluster)\n",
    "    #print(\"Labels: \", db.labels_)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"##Clusters##\")\n",
    "        cluster = [[]]\n",
    "        noise = []\n",
    "        noise_r = []\n",
    "        for i, label in enumerate(db.labels_):\n",
    "            if label != -1:\n",
    "                try:\n",
    "                    cluster[label].append(titles[i])\n",
    "                except Exception as e:\n",
    "                    cluster.append([titles[i]])\n",
    "            else:\n",
    "                noise.append(titles[i])\n",
    "                noise_r.append(i)\n",
    "        for list_ in cluster:\n",
    "            print(\"Cluster:\", list_)\n",
    "        if print_noise:\n",
    "            print(\"Noise: \", noise)\n",
    "\n",
    "        print(\"DBSCAN finished.\\n\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative/incremental DBSCAN \n",
    "that's the way I thought was best to use DBSCAN in our case\n",
    "TODO: heuristic to find out how many times to apply DBSCAN, right now is only based on eps size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply DBSCAN to SUBSET, change here to apply to 'docs' instead of 'subset' to clusterize all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Clusters##\n",
      "Cluster: ['Con Laboratorio RISE e Vendor verso le “Best Practice 4.0” - Industry4Business', 'Servitization e smart product così la Industry 4.0 diventa realtà', 'Per innovare la pubblica amministrazione serve lo smart working: ecco perché', \"Dall'Industria 4.0 all'impresa Smart: Pronti ad affrontare la sfida? - Industry4Business\", \"Storm Reply: nella monetizzazione del dato la nuova sfida per le imprese nell'era dell'IoT\"]\n",
      "Cluster: ['Industria 4.0, Ceresa (Fca): \"Pronti a sperimentare il 5G\" - CorCom', 'White Paper selection: Siemens spiega l’uso di MindSphere per l’IoT in ottica Industry 4.0', 'Industria 4.0, Tecnest: “Modello Italia forte perché human centered”']\n",
      "DBSCAN finished.\n",
      "\n",
      "##Clusters##\n",
      "Cluster: ['Una Academy per insegnare Industry 4.0 alle medie imprese - Industry4Business', 'Metalmeccanica: le imprese faticano a trovare lavoratori con le giuste competenze - Industry4Business', 'Industria 4.0: creare le basi per il futuro del settore manifatturiero -', 'Industria 4.0, più skill e un welfare \"agile\" per sostenere il lavoro - CorCom', 'PA 4.0 e blockchain - Pagina 2 di 4 - Blockchain 4innovation', 'blockchain e digital transformation nella PA: focus su standard e governance - Blockchain 4innovation', 'Chance digitale per l’automotive: dalla guida autonoma agli showroom virtuali', \"Che cos'è NEM, come funziona e quali sono i principali ambiti applicativi - Blockchain 4innovation\", 'Industry 4.0, ecco che cosa chiedono le aziende (dopo gli incentivi)']\n",
      "DBSCAN finished.\n",
      "\n",
      "##Clusters##\n",
      "Cluster: [\"Nokia volta pagina e punta (anche) sull'Iot: acquisita SpaceTime Insight - CorCom\", 'Industria 4.0, droni per la sicurezza nelle fabbriche Ford - Industry4Business', 'Chris Anderson: «Con il digitale, anche il manufacturing diventa fai-da-te»']\n",
      "Noise:  []\n",
      "DBSCAN finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "subset_length = 20\n",
    "# subset of docs vectors \n",
    "subset = inferred_vectors[:subset_length]\n",
    "subset_titles = titles[:subset_length]\n",
    "\n",
    "eps = 0.25\n",
    "eps_increment = 0.13\n",
    "noise_bool = False\n",
    "# starting eps will be the sum of eps + eps_increment \n",
    "for i in range(3):\n",
    "    if i==2: \n",
    "        noise_bool = True\n",
    "    eps = eps + eps_increment\n",
    "    # decrease eps_increment a bit \n",
    "    #eps_increment = eps_increment - .02\n",
    "    db = perform_dbscan(eps = eps, min_samples = 2, metric = 'cosine', algorithm = 'auto',\n",
    "                        data = subset, verbose = True, titles = subset_titles, print_noise = noise_bool)\n",
    "\n",
    "    # let's try and find other clusters in the noise data, with higher eps\n",
    "    subset = [subset[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "    subset_titles = [subset_titles[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "    if subset is None:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Save clusters to JSON using agreed format and Cluster entities as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Cluster Entities\n",
    "Each cluster will be represented by a few meaningful entities, which summarize the cluster: \n",
    "these entities are chosen based on the most 'popular' among the documents which form a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# efficient way of getting most common elements in a list (O(n))\n",
    "def mostCommons(lst, n):\n",
    "    \"\"\"given a list, returns the n most common elements; in case of ties, it may not return the first occurence. \"\"\"\n",
    "    data = Counter(lst)\n",
    "    item_count_list = data.most_common(n)\n",
    "\n",
    "    return [item for (item, counter) in item_count_list]\n",
    "\n",
    "def getClusterEntites(cluster_docs = None, n_entities = 3):\n",
    "    \"\"\"given all documents belonging to a cluster (as a list of dictionaries, each dictionary \n",
    "    representing a doc with its attributes), returns the most common 'n_entities' in the cluster.\n",
    "    \"\"\"\n",
    "    \n",
    "    # get list of flattened_entities from documents\n",
    "    entities_field_name = 'flattened_entities'\n",
    "    # we're expecting flattened_entities as a list of strings\n",
    "    f_entities = [entity for doc in cluster_docs for entity in doc[entities_field_name]]\n",
    "    \n",
    "    # get the 'n_entities' most 'frequent' entity in the cluster\n",
    "    return mostCommons(f_entities, n_entities)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tecnologia', 'azienda', 'industria_4.0', 'internet_delle_cose']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to use getClusterEntities, example:\n",
    "\n",
    "## convert flattened_entites from string to list of strings\n",
    "for doc in docs:\n",
    "    if isinstance(doc['flattened_entities'], str):\n",
    "        doc['flattened_entities'] = doc['flattened_entities'].split()\n",
    "getClusterEntites(docs, 4) # print out 4 - most common in whole dataset (it will be used in clusters, not whole dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
