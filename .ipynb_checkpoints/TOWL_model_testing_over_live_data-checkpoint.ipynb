{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test models on live-like version documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in new json:  96\n",
      "Number of documents in new json:  604\n"
     ]
    }
   ],
   "source": [
    "# fetch data\n",
    "import json\n",
    "filenames = ['blockchain.json', 'industria_4.0.json']\n",
    "\n",
    "with open(filenames[0], 'r') as outfile:\n",
    "    json_data = json.load(outfile)\n",
    "#print(\"Length of the json file: {0}, type: {1}\".format(len(json_data), type(json_data)))\n",
    "\n",
    "## let's now retrieve the meaningful part of the json document\n",
    "# response{}--->docs[]\n",
    "\n",
    "docs = json_data['response']['docs']\n",
    "print(\"Number of documents in new json: \",len(docs))\n",
    "\n",
    "# open file 2 and do the same things\n",
    "with open(filenames[1], 'r') as outfile:\n",
    "    json_data = json.load(outfile)\n",
    "\n",
    "docs = docs + json_data['response']['docs']\n",
    "print(\"Number of documents in new json: \",len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New length after removing docs:  412\n"
     ]
    }
   ],
   "source": [
    "## many documents have a failed abstract, let's remove them\n",
    "to_check = ' Questo sito web utilizza cookie tecnici e, previo Suo consenso, cookie di profilazione,'\n",
    "docs = [doc for i, doc in enumerate(docs) if not(to_check.strip() in doc['abstract'][0].strip())]\n",
    "\n",
    "print(\"New length after removing docs: \", len(docs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Length:  384\n",
      "[' PA digitale, al via il “Premio 10×10 = cento progetti per cambiare la PA” promosso da Forum PA all’interno della manifestazione Forum PA 2017 (23 – 25 maggio). Il premio è rivolto a enti centrali e locali, regioni, province, strutture della sanità pubblica, aziende di SPL, multiutility, piccole e medie aziende innovative, Istituzioni scolastiche, università e Centri di ricerca: si chiede di presentare idee progettuali o prodotti in grado di promuovere l’innovazione digitale del paese attraverso la PA.\\n Ecco gli ambiti entro i quali partecipare al Premio:\\n  Cybersecurity, business continuity, crisis management, sicurezza dei sistemi informativi;  Smart city, dati e IOT;  Servizi online, servizi su mobile, pagamenti elettronici;  PA senza carta;  Scuola e educazione digitale;  Industria 4.0;  Agricoltura Intelligente: dematerializzazione, sburocratizzazione, agricoltura e digital transformation, IoT per la coltivazione;  Comunicazione verso cittadini e stakeholder;  Sharing economy, sussidiarietà orizzontale, social innovation;  Smart environment e energy management. \\n Le soluzioni proposte possono essere in uno dei seguenti stati:\\n  Idea progettuale;  Progetto approvato, finanziato e/o in fase di realizzazione;  Prodotto approvato, finanziato e/o in fase di realizzazione;  Progetto concluso dopo il 31/12/2015;  Prodotto pronto per l’utilizzo o già in utilizzo (primo utilizzo a partire dal 01/01/2016) \\n Premiazione •  Per ciascun ambito, una Giuria di esperti selezionerà una short list di progetti/prodotti meritevoli. La valutazione terrà conto dei seguenti criteri: innovatività e originalità; trasferibilità; rilevanza; sostenibilità.\\n Tutti i progetti selezionati nella short list saranno promossi in un’apposita area nell’ambito di Forum PA 2017; pubblicati sulla piattaforma Forum PA Challenge con possibilità di promozione e valorizzazione del progetto, attraverso strumenti di comunicazione virale e di social ranking messi a disposizione dalla piattaforma.\\n Nel corso della cerimonia di premiazione, che si terrà durante Forum PA 2017 saranno comunicati i 10 progetti vincitori decretati dalla Giuria. Durante i tre giorni della manifestazione i visitatori individueranno il progetto più apprezzato che riceverà un riconoscimento a conclusione dell’evento.\\n Le candidature sono state prorogate al 18 aprile 2017. I candidati selezionati nella short list verranno contattati dalla direzione organizzativa del Premio e si impegneranno a fornire, su richiesta, il logo del soggetto proponente e, ove presente, l’immagine di riferimento della soluzione in alta risoluzione (formato EPS).\\n Per regolamento e regole di partecipazione consultare il sito.']\n"
     ]
    }
   ],
   "source": [
    "## Adjust data format\n",
    "for i, dictionary in enumerate(docs):\n",
    "    for field in ['title', 'abstract', 'flattened_entities']:\n",
    "        if isinstance(dictionary[field], list):\n",
    "            # re-format data to hold string instead of single-list item\n",
    "            docs[i][field] = dictionary[field][0]\n",
    "# remove duplicates\n",
    "for i, doc in enumerate(docs):\n",
    "    if \"Industry 4.0 (o industria 4.0): cos'è, notizie, normative, casi studio - I4T\" in doc['title']:\n",
    "        del(docs[i])\n",
    "\"\"\"\n",
    "duplicates_indeces = []\n",
    "for i, doc in enumerate(docs):\n",
    "    for j in range(i+1, len(docs)):\n",
    "        if docs[j]['title'] == doc['title']:\n",
    "            duplicates_indeces.append(j)\n",
    "print(\"Number of duplicates: \", len(duplicates_indeces))\n",
    "docs = [doc for i, doc in enumerate(docs) if not(i in duplicates_indeces)]\n",
    "print(\"New Length: \", len(docs))\n",
    "\"\"\"\n",
    "## randomize everything by shuffling the documents around\n",
    "import random\n",
    "random.shuffle(docs)\n",
    "\n",
    "for doc in docs:\n",
    "    if to_check.strip() in doc['abstract'].strip():\n",
    "        print(\"cookie doc found\")\n",
    "print([d['abstract'] for d in docs[:1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's first try to infer vector from model; if that doesn't work much, let's train another model with this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5923\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import  gensim\n",
    "\n",
    "MODEL_NAME = 'TestModels/d2v_TA_abstract&title0.model'\n",
    "MODEL_TWO = 'Models/d2v_TA_abstract&title0.model'\n",
    "#model = Doc2Vec.load(MODEL_NAME)\n",
    "model = Doc2Vec.load(MODEL_TWO)\n",
    "inferred_vectors = []\n",
    "# print out dimension of the vocabulary \n",
    "print(len(model.wv.vocab))\n",
    "#print(model.most_similar(positive=['re', 'donna'], negative=['uomo']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "# infer vectors from data\n",
    "test_corpus = [gensim.utils.simple_preprocess(d['title']+d['abstract']) for d in docs]\n",
    "print(len(test_corpus))\n",
    "inferred_vectors = [model.infer_vector(doc) for doc in test_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def perform_dbscan(eps = 0.4, min_samples = 4, metric = 'euclidean', algorithm = 'auto', data = None, verbose = True\n",
    "                  , titles = None, print_noise = True):\n",
    "    \"\"\"perform DBSCAN over given data, using given parametrs. Returns dbscan object.\"\"\"\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples, metric=metric, algorithm=algorithm).fit(data)\n",
    "\n",
    "    #print(\"Core samples: \")\n",
    "    #for i in db.core_sample_indices_ :\n",
    "    #    print(titles[i]+\"\\n\")\n",
    "\n",
    "    # labels will print out the number of the cluster each example belongs to;\n",
    "    # -1 if the vector is considered noise (not belonging to any cluster)\n",
    "    #print(\"Labels: \", db.labels_)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"##Clusters##\")\n",
    "        cluster = [[]]\n",
    "        noise = []\n",
    "        noise_r = []\n",
    "        for i, label in enumerate(db.labels_):\n",
    "            if label != -1:\n",
    "                try:\n",
    "                    cluster[label].append(titles[i])\n",
    "                except Exception as e:\n",
    "                    cluster.append([titles[i]])\n",
    "            else:\n",
    "                noise.append(titles[i])\n",
    "                noise_r.append(i)\n",
    "        for list_ in cluster:\n",
    "            print(\"Cluster:\", list_)\n",
    "        if print_noise:\n",
    "            print(\"Noise: \", noise)\n",
    "\n",
    "        print(\"DBSCAN finished.\\n\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IoT, quasi mezzo miliardo di oggetti connessi \"mobile\": Cina in pole - CorCom', 'Fabbrica intelligente, 340 milioni per le Pmi del Sud - CorCom', \"200 nuovi laureati per IBM: l'innovazione ha bisogno anche della Blockchain - Blockchain 4innovation\", 'La progettazione virtuale di Siemens al Forum Meccatronica di Torino - Industry4Business', \"Industry 4.0 (o industria 4.0): cos'è, notizie, normative, casi studio - I4T\"]\n"
     ]
    }
   ],
   "source": [
    "# get docs titles\n",
    "titles = [doc['title'] for doc in docs]\n",
    "print(titles[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental DBSCAN over small subset\n",
    "## TODO: check if noise is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Clusters##\n",
      "Cluster: ['Skill e competenze per la digital servitization - Industry4Business', \"Telecom: 5G, sicurezza e piattaforme per l'IoT\", \"Che cos'è TrustedChain e perché può cambiare la logica di gestione delle transazioni grazie alla Blockchain - Blockchain 4innovation\", \"Cos'è, come fare ed esempi concreti di Industria 4.0 - Internet4Things\", 'Abbanoa punta sulla blockchain per certificare la lettura dei contatori - Blockchain 4innovation', 'Blockchain e Industry 4.0: Sap Leonardo porta la Data Intelligence nella Smart Factory - Blockchain 4innovation', 'SpidChain, identità digitale 4.0 per PA e aziende - Blockchain 4innovation', \"Industry 4.0 (o industria 4.0): cos'è, notizie, normative, casi studio - I4T\", 'Ecco tutte le tecnologie Industry 4.0 prorogate dalla legge di Stabilità', 'Legal e blockchain - Blockchain 4innovation', \"Industry 4.0 (o industria 4.0): cos'è, notizie, normative, casi studio - I4T\", \"Industry 4.0 (o industria 4.0): cos'è, notizie, normative, casi studio - I4T\", \"Dallo Smart Manufacturing all'Industria 4.0: la nuova era per il manifatturiero parte dall'Industrial IoT - Industry4Business\", 'Osservatorio Industria 4.0: Industrial IoT, Analytics e Cloud Manufacturing spingono il mercato a 2,4 Mld con un +30% - Industry4Business', \"Tech Data entra nell'alleanza globale per l'IoT\", \"Industry 4.0 (o industria 4.0): cos'è, notizie, normative, casi studio - I4T\"]\n",
      "Noise:  [\"Industria 4.0, si rafforza l'asse Confindustria-Ubi Banca - CorCom\", 'Deloitte-Dnv GL, nasce la certificazione blockchain: “È solo l’inizio” - Blockchain 4innovation', 'Paradigma Industry X.0: parte da Modena la nuova rivoluzione del made in Italy - CorCom', 'La blockchain piace al Web, ma dietro l’angolo c\\'è il pericolo \"bolla\" - Blockchain 4innovation', 'Blockchain & Bitcoin: una guida per capire e per orientarsi dedicata ai lettori di MilanoFinanza - Blockchain 4innovation', 'Moreschini (Microsoft): Blockchain e Cloud accoppiata vincente per la PA - Blockchain 4innovation', 'Industria 4.0, così finisce il \"diritto pesante\" del lavoro - CorCom', 'Sharp Europe punta su Iot e intelligenza artificiale: Bob Ishida nuovo Ceo - CorCom', 'Tassa sui robot, Bentivogli: \"Per l\\'Italia sarebbe un boomerang\" - CorCom']\n",
      "DBSCAN finished.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# let's try and find other clusters in the noise data, with higher eps\\nnoise_data = [subset[i] for i, label in enumerate(db.labels_) if label==-1]\\nnoise_titles = [subset_titles[i] for i, label in enumerate(db.labels_) if label==-1]\\n\\ndb = perform_dbscan(eps = eps + eps_increment, min_samples = 2, metric = 'cosine', algorithm = 'auto',\\n                    data = noise_data, verbose = True, titles = noise_titles, print_noise = False)\\n\\nnoise_data = [subset[i] for i, label in enumerate(db.labels_) if label==-1]\\nnoise_titles = [subset_titles[i] for i, label in enumerate(db.labels_) if label==-1]\\n\\ndb = perform_dbscan(eps = eps + eps_increment + eps_increment, min_samples = 2, metric = 'cosine', algorithm = 'auto',\\n                    data = noise_data, verbose = True, titles = noise_titles)\""
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_length = 25\n",
    "# subset of docs vectors \n",
    "subset = inferred_vectors[:subset_length]\n",
    "subset_titles = titles[:subset_length]\n",
    "\n",
    "eps = 0.25\n",
    "eps_increment = .15\n",
    "db = perform_dbscan(eps = 0.45, min_samples = 2, metric = 'cosine', algorithm = 'auto',\n",
    "                    data = subset, verbose = True, titles = subset_titles, print_noise = True)\n",
    "\"\"\"\n",
    "# let's try and find other clusters in the noise data, with higher eps\n",
    "noise_data = [subset[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "noise_titles = [subset_titles[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "\n",
    "db = perform_dbscan(eps = eps + eps_increment, min_samples = 2, metric = 'cosine', algorithm = 'auto',\n",
    "                    data = noise_data, verbose = True, titles = noise_titles, print_noise = False)\n",
    "\n",
    "noise_data = [subset[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "noise_titles = [subset_titles[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "\n",
    "db = perform_dbscan(eps = eps + eps_increment + eps_increment, min_samples = 2, metric = 'cosine', algorithm = 'auto',\n",
    "                    data = noise_data, verbose = True, titles = noise_titles)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Clusters##\n",
      "Cluster: []\n",
      "Noise:  ['IoT, quasi mezzo miliardo di oggetti connessi \"mobile\": Cina in pole - CorCom', 'Fabbrica intelligente, 340 milioni per le Pmi del Sud - CorCom', \"200 nuovi laureati per IBM: l'innovazione ha bisogno anche della Blockchain - Blockchain 4innovation\", 'La progettazione virtuale di Siemens al Forum Meccatronica di Torino - Industry4Business', \"Industry 4.0 (o industria 4.0): cos'è, notizie, normative, casi studio - I4T\", 'Metalmeccanica: le imprese faticano a trovare lavoratori con le giuste competenze - Industry4Business', 'Industry 4.0, che succede ai Competence center: ci sarà un bando', 'Nuove competenze per la Digital Servitization: i risultati del Focus Group ASAP - Industry4Business', 'Formazione 4.0 in Legge di Stabilità 2019, ecco che chiedono le imprese', 'Smart Contract e blockchain - Pagina 4 di 5 - Blockchain 4innovation', 'Libro Bianco FPA per l’innovazione nella Pubblica Amministrazione: le consultazioni sono aperte - Blockchain 4innovation', \"Il dibattito dell'Osservatorio Industria 4.0 sui social media - Industry4Business\", 'Apre al Politecnico di Torino un nuovo centro di compentenza sull’Industria 4.0', 'Industria 4.0, al via la sfida della Campania - CorCom', 'Chris Anderson: «Con il digitale, anche il manufacturing diventa fai-da-te»', 'Masperi, SAP: Verso la servitizzazione della produzione con il dialogo tra IT e OT - Industry4Business', \"Libri e white paper per capire le potenzialità dell'Industria 4.0 e della digital transformation - Industry4Business\", 'Bosch si affida a Fincons Group per lo smart manufacturing - Industry4Business', 'IoT e Smart Manufacturing trovano spazio alla BI-MU', 'Industria 4.0, Italia e Germania insieme per la leadership europea - CorCom']\n",
      "DBSCAN finished.\n",
      "\n",
      "##Clusters##\n",
      "Cluster: [\"200 nuovi laureati per IBM: l'innovazione ha bisogno anche della Blockchain - Blockchain 4innovation\", 'La progettazione virtuale di Siemens al Forum Meccatronica di Torino - Industry4Business', \"Industry 4.0 (o industria 4.0): cos'è, notizie, normative, casi studio - I4T\", 'Metalmeccanica: le imprese faticano a trovare lavoratori con le giuste competenze - Industry4Business', 'Industry 4.0, che succede ai Competence center: ci sarà un bando', 'Nuove competenze per la Digital Servitization: i risultati del Focus Group ASAP - Industry4Business', 'Formazione 4.0 in Legge di Stabilità 2019, ecco che chiedono le imprese', 'Libro Bianco FPA per l’innovazione nella Pubblica Amministrazione: le consultazioni sono aperte - Blockchain 4innovation', \"Il dibattito dell'Osservatorio Industria 4.0 sui social media - Industry4Business\", 'Apre al Politecnico di Torino un nuovo centro di compentenza sull’Industria 4.0', 'Industria 4.0, al via la sfida della Campania - CorCom', 'Masperi, SAP: Verso la servitizzazione della produzione con il dialogo tra IT e OT - Industry4Business', \"Libri e white paper per capire le potenzialità dell'Industria 4.0 e della digital transformation - Industry4Business\", 'Bosch si affida a Fincons Group per lo smart manufacturing - Industry4Business', 'IoT e Smart Manufacturing trovano spazio alla BI-MU', 'Industria 4.0, Italia e Germania insieme per la leadership europea - CorCom']\n",
      "Noise:  ['IoT, quasi mezzo miliardo di oggetti connessi \"mobile\": Cina in pole - CorCom', 'Fabbrica intelligente, 340 milioni per le Pmi del Sud - CorCom', 'Smart Contract e blockchain - Pagina 4 di 5 - Blockchain 4innovation', 'Chris Anderson: «Con il digitale, anche il manufacturing diventa fai-da-te»']\n",
      "DBSCAN finished.\n",
      "\n",
      "##Clusters##\n",
      "Cluster: ['IoT, quasi mezzo miliardo di oggetti connessi \"mobile\": Cina in pole - CorCom', 'Fabbrica intelligente, 340 milioni per le Pmi del Sud - CorCom']\n",
      "Noise:  ['Smart Contract e blockchain - Pagina 4 di 5 - Blockchain 4innovation', 'Chris Anderson: «Con il digitale, anche il manufacturing diventa fai-da-te»']\n",
      "DBSCAN finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subset_length = 20\n",
    "# subset of docs vectors \n",
    "subset = inferred_vectors[:subset_length]\n",
    "subset_titles = titles[:subset_length]\n",
    "\n",
    "eps = 0.25\n",
    "eps_increment = 0.13\n",
    "# starting eps will be the sum of eps + eps_increment \n",
    "for i in range(3):\n",
    "    eps = eps + eps_increment\n",
    "    # decrease eps_increment a bit \n",
    "    #eps_increment = eps_increment - .02\n",
    "    db = perform_dbscan(eps = eps, min_samples = 2, metric = 'cosine', algorithm = 'auto',\n",
    "                        data = subset, verbose = True, titles = subset_titles, print_noise = True)\n",
    "\n",
    "    # let's try and find other clusters in the noise data, with higher eps\n",
    "    subset = [subset[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "    subset_titles = [subset_titles[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "    if subset is None:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model Approach\n",
    "other idea, use entities as tags!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train corpus:  351\n",
      "Doc2Vec(\"live data\",dbow,d100,n5,mc2,t4)\n",
      "Doc2Vec(\"alpha=0.1-live data\",dm/m,d100,n5,w10,mc2,t4)\n",
      "Vocabulary created!\n",
      "Training Doc2Vec(\"live data\",dbow,d100,n5,mc2,t4)\n",
      "CPU times: user 33.4 s, sys: 220 ms, total: 33.6 s\n",
      "Wall time: 11.5 s\n",
      "Training Doc2Vec(\"alpha=0.1-live data\",dm/m,d100,n5,w10,mc2,t4)\n",
      "CPU times: user 51.9 s, sys: 204 ms, total: 52.1 s\n",
      "Wall time: 15.9 s\n",
      "Models Saved\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import  gensim\n",
    "# get train corpus\n",
    "train_corpus = [gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(\n",
    "    d['title']+d['abstract']), [i]) for i, d in enumerate(docs) ]\n",
    "print(\"Length of train corpus: \",len(train_corpus))\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "\n",
    "# let's try training two models at once: Paragraph Vector - Distributed Memory (PV-DM), just like CBOW to W2V\n",
    "# and Paragraph Vector - Distributed Bag of Words (PV-DBOW), analogous to W2V Skip-gram\n",
    "epochs = 45\n",
    "vec_size = 100\n",
    "alpha = 0.10  # default= 0.030\n",
    "MODEL_NAME = \"Models_Live_Test/d2v_abstract&title\"\n",
    "\n",
    "models = [\n",
    "    # PV-DBOW plain\n",
    "    Doc2Vec(dm=0, vector_size=vec_size, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=epochs, workers=cores, comment='live data'),\n",
    "    # PV-DM w/ default averaging; a higher starting alpha may improve CBOW/PV-DM modes\n",
    "    Doc2Vec(dm=1, vector_size= vec_size, window=10, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs= epochs, workers=cores, alpha= alpha, comment='alpha=0.1-live data'),\n",
    "]\n",
    "\n",
    "# build our vocabulary of words (all the unique words encountered inside our corpus, needed for training)\n",
    "for model in models:\n",
    "    print(model)\n",
    "    model.build_vocab(train_corpus)\n",
    "print(\"Vocabulary created!\")\n",
    "\n",
    "# train the models on the given data!\n",
    "counter = 0\n",
    "for model in models:\n",
    "    print(\"Training %s\" % model)\n",
    "    %time model.train(train_corpus, total_examples=len(train_corpus), epochs=model.epochs)\n",
    "    model.save(MODEL_NAME+str(counter)+'.model')\n",
    "    counter = counter + 1\n",
    "print(\"Models Saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental DBSCAN over model vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"tag '351' not seen in training corpus/invalid\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-db2ce726b5e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msubset_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdocvecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvec\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# inferred vectors should result in the same vec as above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#inferred_vectors  = [model.infer_vector(doc.words) for i, doc in enumerate(train_corpus) if i<subset_length]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-db2ce726b5e4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msubset_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdocvecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvec\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# inferred vectors should result in the same vec as above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#inferred_vectors  = [model.infer_vector(doc.words) for i, doc in enumerate(train_corpus) if i<subset_length]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m   1529\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_int_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoctags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_rawint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1531\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tag '%s' not seen in training corpus/invalid\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"tag '351' not seen in training corpus/invalid\""
     ]
    }
   ],
   "source": [
    "# load model\n",
    "modelname = 'Models_Live_Test/d2v_abstract&title0.model'\n",
    "model = Doc2Vec.load(modelname)\n",
    "\n",
    "subset_length = 25\n",
    "print(len(model.docvecs))\n",
    "docvecs = [vec for vec in model.docvecs]\n",
    "# inferred vectors should result in the same vec as above\n",
    "#inferred_vectors  = [model.infer_vector(doc.words) for i, doc in enumerate(train_corpus) if i<subset_length]\n",
    "\n",
    "# subset of docs vectors \n",
    "subset = docvecs[:subset_length]\n",
    "subset_titles = titles[:subset_length]\n",
    "\n",
    "eps = 0.35\n",
    "eps_increment = .15\n",
    "db = perform_dbscan(eps = eps, min_samples = 2, metric = 'cosine', algorithm = 'auto',\n",
    "                    data = subset, verbose = True, titles = subset_titles)\n",
    "\n",
    "# let's try and find other clusters in the noise data, with higher eps\n",
    "noise_data = [subset[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "noise_titles = [subset_titles[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "\n",
    "db = perform_dbscan(eps = eps + eps_increment, min_samples = 2, metric = 'cosine', algorithm = 'auto',\n",
    "                    data = noise_data, verbose = True, titles = noise_titles)\n",
    "\n",
    "# let's try and find other clusters in the noise data, with higher eps\n",
    "noise_data = [subset[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "noise_titles = [subset_titles[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "\n",
    "db = perform_dbscan(eps = eps + eps_increment + 0.1, min_samples = 2, metric = 'cosine', algorithm = 'auto',\n",
    "                    data = noise_data, verbose = True, titles = noise_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize clusters over whole data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA imports\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline\n",
    "\n",
    "# load model\n",
    "MODEL_NAME = 'TestModels/d2v_TA_abstract&title0.model'\n",
    "MODEL_TWO = 'Models/d2v_TA_abstract&title0.model'\n",
    "#model = Doc2Vec.load(MODEL_NAME)\n",
    "model = Doc2Vec.load(MODEL_TWO)\n",
    "\n",
    "inferred_vectors = [model.infer_vector(doc) for doc in test_corpus]\n",
    "# loading dataset into Pandas DataFrame\n",
    "df = pd.DataFrame.from_records(inferred_vectors)\n",
    "\n",
    "# PCA is effected by scale so you need to scale the features in your data before applying PCA. \n",
    "vec_size = 100\n",
    "features = [i for i in range(vec_size)]\n",
    "\n",
    "x = df.loc[:, features].values # get features values\n",
    "\n",
    "# standardize data\n",
    "x = StandardScaler().fit_transform(x) # scale data (especially in case different measures are used)\n",
    "# build PCA model in 2D\n",
    "pca = PCA(n_components=2) # The new components are just the two main dimensions of variation.\n",
    "\n",
    "principalComponents = pca.fit_transform(x)\n",
    "\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "finalDf = principalDf \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~D4nt3/44.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "from scipy.spatial import distance\n",
    "\n",
    "tls.set_credentials_file(username='D4nt3', api_key='FdMB4O6qCfciGDOnLvdQ')\n",
    "\n",
    "COMPONENT_ONE = \"principal component 1\"\n",
    "COMPONENT_TWO = \"principal component 2\"\n",
    "#centroids = kmeans.cluster_centers_\n",
    "titles = [dictionary['title'] for dictionary in docs]\n",
    "traces = []\n",
    "\n",
    "# each trace will represent a point (squeezed vector from higher dimensions),\n",
    "# and each point will have the title of the news assigned\n",
    "for i in range(len(finalDf)):\n",
    "    # assign a color to each point belonging to a specific cluster\n",
    "    # computing distance from centroid\n",
    "    #x = finalDf.loc[i:i, \"principal component 1\"]\n",
    "    #y = finalDf.loc[i:i, \"principal component 2\"]\n",
    "    x , y = finalDf.iat[i, 0], finalDf.iat[i, 1]\n",
    "    color = 'rgba(0, 0, 180, 0.8)'\n",
    "    \"\"\"\n",
    "    centroid_index = kmeans.predict([[x, y]])\n",
    "    closest_centroid = centroids[centroid_index]\n",
    "    #print(closest_centroid, centroids[0])\n",
    "    if np.array_equal(closest_centroid, [centroids[0]]):\n",
    "        color = 'blue'\n",
    "    elif np.array_equal(closest_centroid, [centroids[1]]):\n",
    "        color = 'pink'\n",
    "    elif np.array_equal(closest_centroid, [centroids[2]]):\n",
    "        color = 'yellow'\n",
    "    elif np.array_equal(closest_centroid, [centroids[3]]):\n",
    "        color = 'green'\n",
    "    else:\n",
    "        color = 'black'\n",
    "    \"\"\"\n",
    "    \n",
    "    trace0 = go.Scatter(\n",
    "        x = [x], \n",
    "        y = [y],\n",
    "        mode = 'markers',\n",
    "            #name = 'blue markers',\n",
    "        marker = dict(\n",
    "            size = 7,\n",
    "            color = color,\n",
    "        ),\n",
    "        text = str(titles[i])\n",
    "    )\n",
    "    traces.append(trace0)\n",
    "\n",
    "# draw centroids\n",
    "\"\"\"\n",
    "c_colors = ['blue', 'pink', 'yellow', 'green', 'black']\n",
    "for i in range(len(centroids)):\n",
    "    c_trace = go.Scatter(\n",
    "        x = [centroids[i, 0]],\n",
    "        y = [centroids[i, 1]],\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 9,\n",
    "            color = 'red',\n",
    "        ),\n",
    "        text = c_colors[i]\n",
    "    )\n",
    "    traces.append(c_trace)\n",
    "\"\"\"\n",
    "data = traces \n",
    "layout = dict(title = 'PCA Representantion of D2V on Title+Abstract',\n",
    "            hovermode= 'closest',\n",
    "            xaxis= dict(\n",
    "                title= 'first component',\n",
    "                ticklen= 5,\n",
    "                gridwidth= 2,\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                title= 'second component',\n",
    "                ticklen= 5,\n",
    "                gridwidth= 2,\n",
    "            ),\n",
    "            showlegend = False\n",
    "        )\n",
    "# Plot and embed in ipython notebook!\n",
    "    \n",
    "fig = dict(data = data, layout = layout)\n",
    "py.iplot(fig, filename='live-test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Cluster Entities\n",
    "Each cluster will be represented by a few meaningful entities, which summarize the cluster: \n",
    "these entities are chosen based on the most 'popular' among the documents which form a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# efficient way of getting most common elements in a list (O(n))\n",
    "def mostCommons(lst, n):\n",
    "    \"\"\"given a list, returns the n most common elements; in case of ties, it may not return the first occurence. \"\"\"\n",
    "    data = Counter(lst)\n",
    "    item_count_list = data.most_common(n)\n",
    "\n",
    "    return [item for (item, counter) in item_count_list]\n",
    "\n",
    "def getClusterEntites(cluster_docs = None, n_entities = 3):\n",
    "    \"\"\"given all documents belonging to a cluster (as a list of dictionaries, each dictionary \n",
    "    representing a doc with its attributes), returns the most common 'n_entities' in the cluster.\n",
    "    \"\"\"\n",
    "    \n",
    "    # get list of flattened_entities from documents\n",
    "    entities_field_name = 'flattened_entities'\n",
    "    f_entities = [doc[entities_field_name] for doc in cluster_docs]\n",
    "    \n",
    "    # get the 'n_entities' most 'frequent' entity in the cluster\n",
    "    return mostCommons(f_entities, n_entities)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['automazione cloud_manufacturing industria_4.0 internet_delle_cose rivoluzione_industriale']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to use getClusterEntities\n",
    "\n",
    "print(type(docs[1]['flattened_entities']))\n",
    "getClusterEntites(docs, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
