{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test models on live-like version documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in new json:  96\n",
      "Number of documents in new json:  604\n"
     ]
    }
   ],
   "source": [
    "# fetch data\n",
    "import json\n",
    "filenames = ['blockchain.json', 'industria_4.0.json']\n",
    "\n",
    "with open(filenames[0], 'r') as outfile:\n",
    "    json_data = json.load(outfile)\n",
    "#print(\"Length of the json file: {0}, type: {1}\".format(len(json_data), type(json_data)))\n",
    "\n",
    "## let's now retrieve the meaningful part of the json document\n",
    "# response{}--->docs[]\n",
    "\n",
    "docs = json_data['response']['docs']\n",
    "print(\"Number of documents in new json: \",len(docs))\n",
    "\n",
    "# open file 2 and do the same things\n",
    "with open(filenames[1], 'r') as outfile:\n",
    "    json_data = json.load(outfile)\n",
    "\n",
    "docs = docs + json_data['response']['docs']\n",
    "print(\"Number of documents in new json: \",len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New length after removing docs:  412\n"
     ]
    }
   ],
   "source": [
    "## many documents have a failed abstract, let's remove them\n",
    "to_check = ' Questo sito web utilizza cookie tecnici e, previo Suo consenso, cookie di profilazione,'\n",
    "docs = [doc for i, doc in enumerate(docs) if not(to_check.strip() in doc['abstract'][0].strip())]\n",
    "\n",
    "print(\"New length after removing docs: \", len(docs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Length:  384\n",
      "['Il Piano Industria 4.0, lanciato per la prima volta dal MISE nel settembre 2016, ha dato vita ad una rete infrastrutturale dell’innovazione digitale con l’obiettivo di creare strette interazioni tra ricerca e impresa, formazione e lavoro, innovazione e territori. Questa rete ruota intorno a due soggetti fondamentali:\\n i Competence Center che rappresentano poli di ricerca e innovazione legati, allo stesso tempo, alle università ed alle imprese e capaci di fornire altissime competenze e “facilities” sulle tecnologie 4.0; i Digital Innovation Hub (DIH), che collaborano con i Competence Center e forniscono servizi alle imprese valorizzando e mettendo in rete i vari attori dell’ecosistema dell’innovazione digitale. \\nI DIH, digital innovation hub, costituiscono la vera e propria “porta” di accesso per le imprese al mondo di Industria 4.0 nella misura in cui mettono a loro disposizione servizi per introdurre tecnologie 4.0; sviluppare progetti di trasformazione digitale; accedere all’ecosistema dell’innovazione a livello regionale, nazionale ed europeo. Specificamente, nella visione di Confindustria, i DIH dovrebbero attivare un network degli “attori territoriali dell’innovazione” composto da Università, Competence Center, Cluster, Player industriali, Centri di ricerca, Parchi scientifici e tecnologici, Incubatori di Start-up, FabLab, Investitori, Enti locali. I Digital Innovation Hub, nodi essenziali di queste reti, dovrebbero entrare anche nelle reti di livello nazionale ed europeo configurandosi come vere e proprie cinghie di trasmissione dell’innovazione.\\nI DIH dovrebbero avere una dimensione regionale o interregionale e per la loro costituzione non sono previsti finanziamenti pubblici nazionali; per poter sostenere economicamente le iniziative formative e di acquisizione di nuove competenze, dovrebbero ricorrere a risorse regionali derivanti da fondi strutturali europei e dai fondi interprofessionali. Fondamentale, ai fini della riuscita della loro mission, è la partecipazione di soggetti istituzionali come gli Enti locali e potenziali finanziatori dei progetti di innovazione aziendale come le banche, venture capitalist e fondazioni.\\nI Digital Innovation Hub, secondo Carlo Calenda (cit. da Weisz, 2017), dovrebbero nascere spontaneamente e ovunque. In realtà, osserva Barbara Weisz (2017), l’Italia presenta già una mappa di DIH attivati in collaborazione con Confindustria: “T2i Digital Innovation Hub” che interessa Veneto, Trentino, Alto Adige e Friuli ed è specializzato in high performance computing e internet of things; “DIMA-HUB” in Piemonte rivolto alle tecnologie laser; “Cicero Hub” nel Lazio che si occupa di cyber physical systems e IoT; “4M4.0” nelle Marche specializzato in high performance computing e robotics; “SMILE – Smart Manufacturing Lean Innovation Excellence”, in Emilia Romagna, centrato su lean innovation, cyber physical systems e IoT; “Apulia Manifacturing” in Puglia specializzato in cyber physical systems e IoT; “Manifattuta sarda 4.0” in Sardegna teso ad applicare le tecnologie digitali nei settori tradizionali dell’agricoltura, del turismo, dei beni culturali.\\nLa mappa dei CC e dei DIH attualmente previsti in Italia\\nLa differenza tra digital innovation hub e competence center\\nI Competence Center costituiscono invece la spina dorsale di conoscenze e competenze qualificate rispetto ad alcune dimensioni essenziali di Industria 4.0: robotica, additive manufacturing, realtà aumentata, Internet of Things, cloud, big data e analytics, simulazione, cybersecurity. Questi Centri rappresentano poli di innovazione costituiti nella forma di partenariato pubblico-privato da almeno un organismo di ricerca e da una o più imprese. Debbono dunque essere legati a poli universitari, player privati, centri di ricerca pubblici e privati, start up. I Centri di competenza hanno l’obiettivo di fornire l’advisory tecnologica soprattutto alle PMI, favorire la sperimentazione e la produzione di nuove tecnologie, formare i giovani ed accrescere le competenze dei lavoratori attraverso la formazione 4.0.\\nIl bando per l’istituzione dei competence center è stato pubblicato in Gazzetta ufficiale il 29 gennaio 2018; le domande per la loro istituzione possono essere presentate dal 1 febbraio 2018 fino al 30 aprile 2018. Il bando non prevede un numero fisso di Centri ma il loro numero sarà determinato dalle risorse in base ai progetti proposti. La dotazione economica complessiva per i Competence Center è di 40 milioni di euro (Weisz, 2018).\\nLe proposte per la creazione dei Competence Center sono state avanzate sinora dal Politecnico di Milano, il Politecnico di Torino; gli atenei veneti capitanati da Padova, l’Alma Mater di Bologna, l’Istituto sant’Anna di Pisa, l’Università Federico II di Napoli, l’Università di Bari, La Sapienza di Roma. Il Competence Center del Politecnico di Milano dovrebbe aiutare le aziende ad integrare l’uso di tecnologie come robotica, additive manufacuring, IoT, big data e sensoristica. Il Politecnico di Torino metterebbe a disposizione le sue competenze in tecnologie quali robotica, big data, IoT e si rivolgerà in particolare alle aziende dell’aerospazio, dell’aeronautica e dell’automotive. I Competence Center degli atenei veneti intendono offrire assistenza nei settori industriali di punta del territorio: abbigliamento, arredamento, automazione ed agrifood. Il Centro dell’Alma Mater di Bologna dovrebbe offrire competenze soprattutto nel dominio dei big data e rivolgersi a filiere regionali quali la meccatronica, la motoristica ed il biomedicale.  Pisa punterà essenzialmente sulla robotica collaborativa e sugli ambienti virtuali, ma riguarderà anche il digital manufacturing, l’ergonomia e le scienze della vita. Il Centro della Federico II di Napoli sarà specializzato nella robotica e nei materiali innovativi. Il Centro del Politecnico di Bari offrirà competenze nei domini dell’aerospazio, dell’automotive e dell’agricoltura 4.0.\\nCon l’istituzione dei Competence Center, sottolinea Carlo Calenda, vogliamo attrezzare il Paese di poli di eccellenza tesi a valorizzare le competenze di università e imprese (Meta, 2018). La quarta rivoluzione digitale introduce essenziali trasformazioni nel mercato del lavoro ed investe su percorsi formativi virtuosi capaci di dar vita a nuove competenze. La circolare del MISE su Industria 4.0 del 30/03/2017 sottolinea al riguardo che “… il Piano prevede la diffusione di una cultura 4.0 lungo l’intero ciclo formativo, dalla scuola all’università, dagli istituti tecnici superiori ai corsi di dottorato” (Agenzia delle Entrate e MISE, 2017). Si prevede in particolare una crescita degli studenti universitari di 200.000 unità, un raddoppio degli iscritti agli istituti tecnici superiori, 1400 dottorati di ricerca, 3000 manager specializzati sui temi 4.0.\\nDal punto di vista dei contenuti, la formazione alle competenze 4.0 implica la necessità di investire sia sulla verticalizzazione delle competenze digitali che sulle competenze trasversali e trasformazionali che potrebbero consentire soprattutto ai giovani l’occupabilità e la possibilità di lavorare in contesti fortemente interessati da processi innovativi. Marco Taisch, membro della Cabina di regia di Industria 4.0, sottolinea come i modelli di apprendimento oggi rilevanti siano quelli legati ai “connecting the dots di Steve Jobs” (Weisz, 2017). La sfida di un modello di apprendimento che non invecchi con la tecnologia è legata alla capacità logica di stabilire connessioni, saper scomporre e ricomporre i problemi. Questa capacità è specificamente riferita, in termini di industria 4.0, ai domini METS: mathematics, engineering, technology, science.\\nProprio le strette interconnessioni tra università e imprese, tra pubblico e privato, tra diritto all’istruzione e formazione alle competenze 4.0 può dar vita alla fabbrica del futuro in versione “research factory” distribuita, per trasformarla in un luogo che va oltre i suoi confini fisici, dove manifattura e ricerca si uniscono. La sfida di Industria 4.0 è quella di valorizzare “…l’artigianalità del tessuto imprenditoriale italiano, grazie all’utilizzo delle tecnologie digitali, fornite dai grandi player e rese fruibili alle PMI dalle università” (Spadoni, 2017), come premessa per la realizzazione di quell’importante progetto di politica industriale che è Impresa 4.0. La sfida italiana di Industria 4.0 deve passare dalle PMI e queste imprese possono puntare all’innovazione imparando anche la programmazione al computer, la stampa in 3D, l’utilizzo dei big data. “…un tornitore può diventare così un addetto alla modellizzazione. Prima stava sul tornio, adesso sta sul pc, genera il pezzo e controlla che esca” (Spadoni, 2017). Egualmente importante è l’impatto dei big data, della sensoristica: anche le piccole imprese possono avere importanti vantaggi dall’uso e dal trattamento dei dati e dai rinnovati rapporti tra conoscenze, tecnologie e lavoro.\\nLa sfida di Industria 4.0 è far si che la manifattura, rilanciata dalla trasformazione digitale, passi dall’attuale 15% di contributo al PIL ad almeno il 20% trainando verso la crescita l’intera economia. All’interno di questa prospettiva, i Digital Innovation Hub costituiscono la vera e propria “porta” di accesso per le imprese al mondo di Industria 4.0 nella misura in cui mettono a loro disposizione servizi per introdurre tecnologie 4.0, sviluppare progetti di trasformazione digitale, accedere all’ecosistema dell’innovazione a livello regionale, nazionale ed europeo. Specificamente, nella visione di Confindustria, i DIH dovrebbero attivare un network degli “attori territoriali dell’innovazione” composto da Università, Competence Center, Cluster, Player industriali, Centri di ricerca, Parchi scientifici e tecnologici, Incubatori di Start-up, FabLab, Investitori, Enti locali. I Digital Innovation Hub, nodi essenziali di queste reti, dovrebbero entrare anche nelle reti di livello nazionale ed europeo configurandosi come vere e proprie cinghie di trasmissione dell’innovazione.\\nI DIH dovrebbero avere una dimensione regionale o interregionale e per la loro costituzione non sono previsti finanziamenti pubblici nazionali. Fondamentale, ai fini della riuscita della loro mission, è la partecipazione di soggetti istituzionali come gli Enti locali e potenziali finanziatori dei progetti di innovazione aziendale come le banche, venture capitalist e fondazioni.\\nAttualmente si contano in Italia 20 DIH. Alcuni di questi sono connessi all’interno delle reti previste dall’iniziativa europea I4MS – ICT Innovation for Manufacturing SMEs – volta proprio a potenziare gli effetti della trasformazione digitale. I DIH italiani della Rete I4MS sono: “T2i Digital Innovation Hub” che interessa Veneto, Trentino Alto Adige e Friuli ed è specializzato in high performance computing e internet of things; “DIMA-HUB” in Piemonte rivolto alle tecnologie laser; “Cicero Hub” nel Lazio che si occupa di cyber physical systems e IoT; “4M4.0” nelle Marche specializzato in high performance computing e robotics; “SMILE – Smart Manufacturing Lean Innovation Excellence”, in Emilia Romagna, centrato su lean innovation, cyber physical systems e IoT; “Apulia Manifacturing” in Puglia specializzato in cyber physical systems e IoT; “Manifattuta sarda 4.0” in Sardegna teso ad applicare le tecnologie digitali nei settori tradizionali dell’agricoltura, del turismo, dei beni culturali.\\n \\n Industry 4.0, tutto quello che c’è da sapere su piano e attuazione \\n\\nConfindustria, Digital Innovation Hub. Un ponte tra impresa, ricerca e finanza, testo accessibile al sito http://preparatialfuturo.confindustria.it/digital-innovation-hub/cosa-sono/\\nAgenzia delle Entrate e MISE, 2017, Circolare N.4/E del 30/03/2017 Industria 4.0, testo accessibile al sito: http://www.camera.it/temiap/allegati/2017/03/31/OCD177-2828.pdf\\nMeta F., 2018, “Competence Center, si parte: online il bando. Calenda: Strumento strategico per Industria 4.0”, CorCom, 30 Gennaio, testo accessibile al sito www.corrierecomunicazioni.it/industria-4-0/competence-center-si-parte-online-bando-calenda-strumento-strategico-industria-4-0/\\nSpadoni E., 2017, “È già ora di parlare di Industria 5.0”, Agi Blog Scienze, 6 aprile, testo accessibile al sito: https://www.agi.it/blog-italia/scienza/e_gi_ora_di_parlare_di_industria_5_0-1657359/news/2017-04-06/\\nWeisz B., 2016, “Ecco la mappa dell’innovazione italiana che nutrirà l’Industry 4.0”, AgendaDigitale.eu, 27 settembre, testo accessibile al sito: www.agendadigitale.eu/cultura-digitale/ecco-la-mappa-dell-innovazione-italiana-che-nutrira-l-industry-40/\\nWeisz B., 2017, “Industry 4.0. Che succede ai Competence Center: ci sarà un bando”, Agenda Digitale.eu, 27 febbraio, testo accessibile al sito: www.agendadigitale.eu/industry-4-0/industry-4-0-che-succede-ai-competence-center-ci-sara-un-bando/\\nWeisz B., 2018, “Industry 4.0, come saranno i competences center: tutti i dettagli”, AgendaDigitale.eu, 29 gennaio, testo accessibile al sito: www.agendadigitale.eu/industry-4-0/industry-4-0-saranno-competence-center/']\n"
     ]
    }
   ],
   "source": [
    "## Adjust data format\n",
    "for i, dictionary in enumerate(docs):\n",
    "    for field in ['title', 'abstract', 'flattened_entities']:\n",
    "        if isinstance(dictionary[field], list):\n",
    "            # re-format data to hold string instead of single-list item\n",
    "            docs[i][field] = dictionary[field][0]\n",
    "# remove duplicates\n",
    "for i, doc in enumerate(docs):\n",
    "    if \"Industry 4.0 (o industria 4.0): cos'è, notizie, normative, casi studio - I4T\" in doc['title']:\n",
    "        del(docs[i])\n",
    "\"\"\"\n",
    "duplicates_indeces = []\n",
    "for i, doc in enumerate(docs):\n",
    "    for j in range(i+1, len(docs)):\n",
    "        if docs[j]['title'] == doc['title']:\n",
    "            duplicates_indeces.append(j)\n",
    "print(\"Number of duplicates: \", len(duplicates_indeces))\n",
    "docs = [doc for i, doc in enumerate(docs) if not(i in duplicates_indeces)]\n",
    "\"\"\"\n",
    "print(\"New Length: \", len(docs))\n",
    "\n",
    "## randomize everything by shuffling the documents around\n",
    "import random\n",
    "random.shuffle(docs)\n",
    "\n",
    "for doc in docs:\n",
    "    if to_check.strip() in doc['abstract'].strip():\n",
    "        print(\"cookie doc found\")\n",
    "print([d['abstract'] for d in docs[:1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's first try to infer vector from model; if that doesn't work much, let's train another model with this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5923\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import  gensim\n",
    "\n",
    "MODEL_NAME = 'TestModels/d2v_TA_abstract&title0.model'\n",
    "MODEL_TWO = 'Models/d2v_TA_abstract&title0.model'\n",
    "#model = Doc2Vec.load(MODEL_NAME)\n",
    "model = Doc2Vec.load(MODEL_TWO)\n",
    "inferred_vectors = []\n",
    "# print out dimension of the vocabulary \n",
    "print(len(model.wv.vocab))\n",
    "#print(model.most_similar(positive=['re', 'donna'], negative=['uomo']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "# infer vectors from data\n",
    "test_corpus = [gensim.utils.simple_preprocess(d['title']+d['abstract']) for d in docs]\n",
    "print(len(test_corpus))\n",
    "inferred_vectors = [model.infer_vector(doc) for doc in test_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def perform_dbscan(eps = 0.4, min_samples = 4, metric = 'euclidean', algorithm = 'auto', data = None, verbose = True\n",
    "                  , titles = None, print_noise = True):\n",
    "    \"\"\"perform DBSCAN over given data, using given parametrs. Returns dbscan object.\"\"\"\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples, metric=metric, algorithm=algorithm).fit(data)\n",
    "\n",
    "    #print(\"Core samples: \")\n",
    "    #for i in db.core_sample_indices_ :\n",
    "    #    print(titles[i]+\"\\n\")\n",
    "\n",
    "    # labels will print out the number of the cluster each example belongs to;\n",
    "    # -1 if the vector is considered noise (not belonging to any cluster)\n",
    "    #print(\"Labels: \", db.labels_)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"##Clusters##\")\n",
    "        cluster = [[]]\n",
    "        noise = []\n",
    "        noise_r = []\n",
    "        for i, label in enumerate(db.labels_):\n",
    "            if label != -1:\n",
    "                try:\n",
    "                    cluster[label].append(titles[i])\n",
    "                except Exception as e:\n",
    "                    cluster.append([titles[i]])\n",
    "            else:\n",
    "                noise.append(titles[i])\n",
    "                noise_r.append(i)\n",
    "        for list_ in cluster:\n",
    "            print(\"Cluster:\", list_)\n",
    "        if print_noise:\n",
    "            print(\"Noise: \", noise)\n",
    "\n",
    "        print(\"DBSCAN finished.\\n\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Innovazione 4.0, competence center e digital innovation hub', \"Almaviva rilancia su Industria 4.0: nel Polo Meccatronica la nuova sede per l'R&S - CorCom\", 'Internet of Things alla prova degli utenti, serve più AI per renderlo \"infallibile\" - CorCom', 'Industria 4.0, Sap: \"Pmi italiane sulla via della digitalizzazione\" - CorCom', \"Dalle parole ai fatti, ecco la via italiana all'industria 4.0 - CorCom\"]\n"
     ]
    }
   ],
   "source": [
    "# get docs titles\n",
    "titles = [doc['title'] for doc in docs]\n",
    "print(titles[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental DBSCAN over small subset\n",
    "## TODO: check if noise is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Clusters##\n",
      "Cluster: ['Skill e competenze per la digital servitization - Industry4Business', \"Telecom: 5G, sicurezza e piattaforme per l'IoT\", \"Che cos'è TrustedChain e perché può cambiare la logica di gestione delle transazioni grazie alla Blockchain - Blockchain 4innovation\", \"Cos'è, come fare ed esempi concreti di Industria 4.0 - Internet4Things\", 'Abbanoa punta sulla blockchain per certificare la lettura dei contatori - Blockchain 4innovation', 'Blockchain e Industry 4.0: Sap Leonardo porta la Data Intelligence nella Smart Factory - Blockchain 4innovation', 'SpidChain, identità digitale 4.0 per PA e aziende - Blockchain 4innovation', \"Industry 4.0 (o industria 4.0): cos'è, notizie, normative, casi studio - I4T\", 'Ecco tutte le tecnologie Industry 4.0 prorogate dalla legge di Stabilità', 'Legal e blockchain - Blockchain 4innovation', \"Industry 4.0 (o industria 4.0): cos'è, notizie, normative, casi studio - I4T\", \"Industry 4.0 (o industria 4.0): cos'è, notizie, normative, casi studio - I4T\", \"Dallo Smart Manufacturing all'Industria 4.0: la nuova era per il manifatturiero parte dall'Industrial IoT - Industry4Business\", 'Osservatorio Industria 4.0: Industrial IoT, Analytics e Cloud Manufacturing spingono il mercato a 2,4 Mld con un +30% - Industry4Business', \"Tech Data entra nell'alleanza globale per l'IoT\", \"Industry 4.0 (o industria 4.0): cos'è, notizie, normative, casi studio - I4T\"]\n",
      "Noise:  [\"Industria 4.0, si rafforza l'asse Confindustria-Ubi Banca - CorCom\", 'Deloitte-Dnv GL, nasce la certificazione blockchain: “È solo l’inizio” - Blockchain 4innovation', 'Paradigma Industry X.0: parte da Modena la nuova rivoluzione del made in Italy - CorCom', 'La blockchain piace al Web, ma dietro l’angolo c\\'è il pericolo \"bolla\" - Blockchain 4innovation', 'Blockchain & Bitcoin: una guida per capire e per orientarsi dedicata ai lettori di MilanoFinanza - Blockchain 4innovation', 'Moreschini (Microsoft): Blockchain e Cloud accoppiata vincente per la PA - Blockchain 4innovation', 'Industria 4.0, così finisce il \"diritto pesante\" del lavoro - CorCom', 'Sharp Europe punta su Iot e intelligenza artificiale: Bob Ishida nuovo Ceo - CorCom', 'Tassa sui robot, Bentivogli: \"Per l\\'Italia sarebbe un boomerang\" - CorCom']\n",
      "DBSCAN finished.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# let's try and find other clusters in the noise data, with higher eps\\nnoise_data = [subset[i] for i, label in enumerate(db.labels_) if label==-1]\\nnoise_titles = [subset_titles[i] for i, label in enumerate(db.labels_) if label==-1]\\n\\ndb = perform_dbscan(eps = eps + eps_increment, min_samples = 2, metric = 'cosine', algorithm = 'auto',\\n                    data = noise_data, verbose = True, titles = noise_titles, print_noise = False)\\n\\nnoise_data = [subset[i] for i, label in enumerate(db.labels_) if label==-1]\\nnoise_titles = [subset_titles[i] for i, label in enumerate(db.labels_) if label==-1]\\n\\ndb = perform_dbscan(eps = eps + eps_increment + eps_increment, min_samples = 2, metric = 'cosine', algorithm = 'auto',\\n                    data = noise_data, verbose = True, titles = noise_titles)\""
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_length = 25\n",
    "# subset of docs vectors \n",
    "subset = inferred_vectors[:subset_length]\n",
    "subset_titles = titles[:subset_length]\n",
    "\n",
    "eps = 0.25\n",
    "eps_increment = .15\n",
    "db = perform_dbscan(eps = 0.45, min_samples = 2, metric = 'cosine', algorithm = 'auto',\n",
    "                    data = subset, verbose = True, titles = subset_titles, print_noise = True)\n",
    "\"\"\"\n",
    "# let's try and find other clusters in the noise data, with higher eps\n",
    "noise_data = [subset[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "noise_titles = [subset_titles[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "\n",
    "db = perform_dbscan(eps = eps + eps_increment, min_samples = 2, metric = 'cosine', algorithm = 'auto',\n",
    "                    data = noise_data, verbose = True, titles = noise_titles, print_noise = False)\n",
    "\n",
    "noise_data = [subset[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "noise_titles = [subset_titles[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "\n",
    "db = perform_dbscan(eps = eps + eps_increment + eps_increment, min_samples = 2, metric = 'cosine', algorithm = 'auto',\n",
    "                    data = noise_data, verbose = True, titles = noise_titles)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Clusters##\n",
      "Cluster: ['Industria 4.0, Sap: \"Pmi italiane sulla via della digitalizzazione\" - CorCom', \"Dalle parole ai fatti, ecco la via italiana all'industria 4.0 - CorCom\", \"Industria 4.0, si rafforza l'asse Confindustria-Ubi Banca - CorCom\", 'Industria 4.0, al via la sfida della Campania - CorCom']\n",
      "Cluster: ['Italtel e Rold uniscono le forze per affermarsi nell’Industria 4.0', 'Industria 4.0 e blockchain - Pagina 4 di 5 - Blockchain 4innovation']\n",
      "Cluster: [\"EY: ambiti applicativi e roadmap per l'industria 4.0 in Italia\", 'Cisco, Alascom, Fanuc: una robotica collaborativa basata (anche) sul linguaggio naturale - Industry4Business']\n",
      "Noise:  ['Innovazione 4.0, competence center e digital innovation hub', \"Almaviva rilancia su Industria 4.0: nel Polo Meccatronica la nuova sede per l'R&S - CorCom\", 'Internet of Things alla prova degli utenti, serve più AI per renderlo \"infallibile\" - CorCom', 'IoT, Migliarina (Vodafone): \"Così le telco andranno oltre la connettività\" - CorCom', 'Se Blockchain fa rima con supply chain: la killer application che il mercato aspettava? - Blockchain 4innovation', \"Olivetti, IoT per l'ambiente in mostra al G7 di Taormina - CorCom\", \"Dall'IoT ai big data, la svolta 4.0 di Black&Decker - CorCom\", \"Industry 4.0 (o industria 4.0): cos'è, notizie, normative, casi studio - I4T\", 'Ansaldo Energia lancia la call \"Digital X Factory\" - CorCom', 'MecSpe 2018, in mostra a Parma la via italiana a Industria 4.0', \"Arriva da LorenzoVinci.it il food in chiave Blockchain di Genuino. Con ICO all'orizzonte - Blockchain 4innovation\", \"Taglio del nastro per Co+Factory, l'hub 4.0 per startup e Pmi - CorCom\"]\n",
      "DBSCAN finished.\n",
      "\n",
      "##Clusters##\n",
      "Cluster: ['Innovazione 4.0, competence center e digital innovation hub', \"Almaviva rilancia su Industria 4.0: nel Polo Meccatronica la nuova sede per l'R&S - CorCom\", 'IoT, Migliarina (Vodafone): \"Così le telco andranno oltre la connettività\" - CorCom', 'Se Blockchain fa rima con supply chain: la killer application che il mercato aspettava? - Blockchain 4innovation', \"Olivetti, IoT per l'ambiente in mostra al G7 di Taormina - CorCom\", \"Dall'IoT ai big data, la svolta 4.0 di Black&Decker - CorCom\", \"Industry 4.0 (o industria 4.0): cos'è, notizie, normative, casi studio - I4T\", 'Ansaldo Energia lancia la call \"Digital X Factory\" - CorCom', 'MecSpe 2018, in mostra a Parma la via italiana a Industria 4.0', \"Arriva da LorenzoVinci.it il food in chiave Blockchain di Genuino. Con ICO all'orizzonte - Blockchain 4innovation\", \"Taglio del nastro per Co+Factory, l'hub 4.0 per startup e Pmi - CorCom\"]\n",
      "Noise:  ['Internet of Things alla prova degli utenti, serve più AI per renderlo \"infallibile\" - CorCom']\n",
      "DBSCAN finished.\n",
      "\n",
      "##Clusters##\n",
      "Cluster: []\n",
      "Noise:  ['Internet of Things alla prova degli utenti, serve più AI per renderlo \"infallibile\" - CorCom']\n",
      "DBSCAN finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subset_length = 20\n",
    "# subset of docs vectors \n",
    "subset = inferred_vectors[:subset_length]\n",
    "subset_titles = titles[:subset_length]\n",
    "\n",
    "eps = 0.25\n",
    "eps_increment = 0.13\n",
    "# starting eps will be the sum of eps + eps_increment \n",
    "for i in range(3):\n",
    "    eps = eps + eps_increment\n",
    "    # decrease eps_increment a bit \n",
    "    #eps_increment = eps_increment - .02\n",
    "    db = perform_dbscan(eps = eps, min_samples = 2, metric = 'cosine', algorithm = 'auto',\n",
    "                        data = subset, verbose = True, titles = subset_titles, print_noise = True)\n",
    "\n",
    "    # let's try and find other clusters in the noise data, with higher eps\n",
    "    subset = [subset[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "    subset_titles = [subset_titles[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "    if subset is None:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model Approach\n",
    "other idea, use entities as tags!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train corpus:  351\n",
      "Doc2Vec(\"live data\",dbow,d100,n5,mc2,t4)\n",
      "Doc2Vec(\"alpha=0.1-live data\",dm/m,d100,n5,w10,mc2,t4)\n",
      "Vocabulary created!\n",
      "Training Doc2Vec(\"live data\",dbow,d100,n5,mc2,t4)\n",
      "CPU times: user 33.4 s, sys: 220 ms, total: 33.6 s\n",
      "Wall time: 11.5 s\n",
      "Training Doc2Vec(\"alpha=0.1-live data\",dm/m,d100,n5,w10,mc2,t4)\n",
      "CPU times: user 51.9 s, sys: 204 ms, total: 52.1 s\n",
      "Wall time: 15.9 s\n",
      "Models Saved\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import  gensim\n",
    "# get train corpus\n",
    "train_corpus = [gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(\n",
    "    d['title']+d['abstract']), [i]) for i, d in enumerate(docs) ]\n",
    "print(\"Length of train corpus: \",len(train_corpus))\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "\n",
    "# let's try training two models at once: Paragraph Vector - Distributed Memory (PV-DM), just like CBOW to W2V\n",
    "# and Paragraph Vector - Distributed Bag of Words (PV-DBOW), analogous to W2V Skip-gram\n",
    "epochs = 45\n",
    "vec_size = 100\n",
    "alpha = 0.10  # default= 0.030\n",
    "MODEL_NAME = \"Models_Live_Test/d2v_abstract&title\"\n",
    "\n",
    "models = [\n",
    "    # PV-DBOW plain\n",
    "    Doc2Vec(dm=0, vector_size=vec_size, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=epochs, workers=cores, comment='live data'),\n",
    "    # PV-DM w/ default averaging; a higher starting alpha may improve CBOW/PV-DM modes\n",
    "    Doc2Vec(dm=1, vector_size= vec_size, window=10, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs= epochs, workers=cores, alpha= alpha, comment='alpha=0.1-live data'),\n",
    "]\n",
    "\n",
    "# build our vocabulary of words (all the unique words encountered inside our corpus, needed for training)\n",
    "for model in models:\n",
    "    print(model)\n",
    "    model.build_vocab(train_corpus)\n",
    "print(\"Vocabulary created!\")\n",
    "\n",
    "# train the models on the given data!\n",
    "counter = 0\n",
    "for model in models:\n",
    "    print(\"Training %s\" % model)\n",
    "    %time model.train(train_corpus, total_examples=len(train_corpus), epochs=model.epochs)\n",
    "    model.save(MODEL_NAME+str(counter)+'.model')\n",
    "    counter = counter + 1\n",
    "print(\"Models Saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental DBSCAN over model vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"tag '351' not seen in training corpus/invalid\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-db2ce726b5e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msubset_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdocvecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvec\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# inferred vectors should result in the same vec as above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#inferred_vectors  = [model.infer_vector(doc.words) for i, doc in enumerate(train_corpus) if i<subset_length]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-db2ce726b5e4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msubset_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdocvecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvec\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# inferred vectors should result in the same vec as above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#inferred_vectors  = [model.infer_vector(doc.words) for i, doc in enumerate(train_corpus) if i<subset_length]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m   1529\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_int_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoctags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_rawint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1531\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tag '%s' not seen in training corpus/invalid\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"tag '351' not seen in training corpus/invalid\""
     ]
    }
   ],
   "source": [
    "# load model\n",
    "modelname = 'Models_Live_Test/d2v_abstract&title0.model'\n",
    "model = Doc2Vec.load(modelname)\n",
    "\n",
    "subset_length = 25\n",
    "print(len(model.docvecs))\n",
    "docvecs = [vec for vec in model.docvecs]\n",
    "# inferred vectors should result in the same vec as above\n",
    "#inferred_vectors  = [model.infer_vector(doc.words) for i, doc in enumerate(train_corpus) if i<subset_length]\n",
    "\n",
    "# subset of docs vectors \n",
    "subset = docvecs[:subset_length]\n",
    "subset_titles = titles[:subset_length]\n",
    "\n",
    "eps = 0.35\n",
    "eps_increment = .15\n",
    "db = perform_dbscan(eps = eps, min_samples = 2, metric = 'cosine', algorithm = 'auto',\n",
    "                    data = subset, verbose = True, titles = subset_titles)\n",
    "\n",
    "# let's try and find other clusters in the noise data, with higher eps\n",
    "noise_data = [subset[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "noise_titles = [subset_titles[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "\n",
    "db = perform_dbscan(eps = eps + eps_increment, min_samples = 2, metric = 'cosine', algorithm = 'auto',\n",
    "                    data = noise_data, verbose = True, titles = noise_titles)\n",
    "\n",
    "# let's try and find other clusters in the noise data, with higher eps\n",
    "noise_data = [subset[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "noise_titles = [subset_titles[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "\n",
    "db = perform_dbscan(eps = eps + eps_increment + 0.1, min_samples = 2, metric = 'cosine', algorithm = 'auto',\n",
    "                    data = noise_data, verbose = True, titles = noise_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize clusters over whole data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA imports\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline\n",
    "\n",
    "# load model\n",
    "MODEL_NAME = 'TestModels/d2v_TA_abstract&title0.model'\n",
    "MODEL_TWO = 'Models/d2v_TA_abstract&title0.model'\n",
    "#model = Doc2Vec.load(MODEL_NAME)\n",
    "model = Doc2Vec.load(MODEL_TWO)\n",
    "\n",
    "inferred_vectors = [model.infer_vector(doc) for doc in test_corpus]\n",
    "# loading dataset into Pandas DataFrame\n",
    "df = pd.DataFrame.from_records(inferred_vectors)\n",
    "\n",
    "# PCA is effected by scale so you need to scale the features in your data before applying PCA. \n",
    "vec_size = 100\n",
    "features = [i for i in range(vec_size)]\n",
    "\n",
    "x = df.loc[:, features].values # get features values\n",
    "\n",
    "# standardize data\n",
    "x = StandardScaler().fit_transform(x) # scale data (especially in case different measures are used)\n",
    "# build PCA model in 2D\n",
    "pca = PCA(n_components=2) # The new components are just the two main dimensions of variation.\n",
    "\n",
    "principalComponents = pca.fit_transform(x)\n",
    "\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "finalDf = principalDf \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~D4nt3/44.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "from scipy.spatial import distance\n",
    "\n",
    "tls.set_credentials_file(username='D4nt3', api_key='FdMB4O6qCfciGDOnLvdQ')\n",
    "\n",
    "COMPONENT_ONE = \"principal component 1\"\n",
    "COMPONENT_TWO = \"principal component 2\"\n",
    "#centroids = kmeans.cluster_centers_\n",
    "titles = [dictionary['title'] for dictionary in docs]\n",
    "traces = []\n",
    "\n",
    "# each trace will represent a point (squeezed vector from higher dimensions),\n",
    "# and each point will have the title of the news assigned\n",
    "for i in range(len(finalDf)):\n",
    "    # assign a color to each point belonging to a specific cluster\n",
    "    # computing distance from centroid\n",
    "    #x = finalDf.loc[i:i, \"principal component 1\"]\n",
    "    #y = finalDf.loc[i:i, \"principal component 2\"]\n",
    "    x , y = finalDf.iat[i, 0], finalDf.iat[i, 1]\n",
    "    color = 'rgba(0, 0, 180, 0.8)'\n",
    "    \"\"\"\n",
    "    centroid_index = kmeans.predict([[x, y]])\n",
    "    closest_centroid = centroids[centroid_index]\n",
    "    #print(closest_centroid, centroids[0])\n",
    "    if np.array_equal(closest_centroid, [centroids[0]]):\n",
    "        color = 'blue'\n",
    "    elif np.array_equal(closest_centroid, [centroids[1]]):\n",
    "        color = 'pink'\n",
    "    elif np.array_equal(closest_centroid, [centroids[2]]):\n",
    "        color = 'yellow'\n",
    "    elif np.array_equal(closest_centroid, [centroids[3]]):\n",
    "        color = 'green'\n",
    "    else:\n",
    "        color = 'black'\n",
    "    \"\"\"\n",
    "    \n",
    "    trace0 = go.Scatter(\n",
    "        x = [x], \n",
    "        y = [y],\n",
    "        mode = 'markers',\n",
    "            #name = 'blue markers',\n",
    "        marker = dict(\n",
    "            size = 7,\n",
    "            color = color,\n",
    "        ),\n",
    "        text = str(titles[i])\n",
    "    )\n",
    "    traces.append(trace0)\n",
    "\n",
    "# draw centroids\n",
    "\"\"\"\n",
    "c_colors = ['blue', 'pink', 'yellow', 'green', 'black']\n",
    "for i in range(len(centroids)):\n",
    "    c_trace = go.Scatter(\n",
    "        x = [centroids[i, 0]],\n",
    "        y = [centroids[i, 1]],\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 9,\n",
    "            color = 'red',\n",
    "        ),\n",
    "        text = c_colors[i]\n",
    "    )\n",
    "    traces.append(c_trace)\n",
    "\"\"\"\n",
    "data = traces \n",
    "layout = dict(title = 'PCA Representantion of D2V on Title+Abstract',\n",
    "            hovermode= 'closest',\n",
    "            xaxis= dict(\n",
    "                title= 'first component',\n",
    "                ticklen= 5,\n",
    "                gridwidth= 2,\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                title= 'second component',\n",
    "                ticklen= 5,\n",
    "                gridwidth= 2,\n",
    "            ),\n",
    "            showlegend = False\n",
    "        )\n",
    "# Plot and embed in ipython notebook!\n",
    "    \n",
    "fig = dict(data = data, layout = layout)\n",
    "py.iplot(fig, filename='live-test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means using SSE as tool to evaluate clustering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
