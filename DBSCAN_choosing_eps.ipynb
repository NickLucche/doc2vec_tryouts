{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN: Choose Eps value by plotting distances between points\n",
    "### e.g. for min_count = 2, plot distances of 2nd closest neighbour of each point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed libraries\n",
    "import json\n",
    "import random\n",
    "from gensim.models import Word2Vec\n",
    "import  gensim\n",
    "from collections import Counter\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from collections import OrderedDict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_eps(min_count, docs_vecs, doc_titles):\n",
    "    \"\"\"Plot the graph and choose best eps based on the composition of our data: \n",
    "    to do so, we will need to compute the distances between every point in the data-space, and its\n",
    "    2nd/3rd closest neighbour (based on 'min_count'). \n",
    "    Take the eps corresponding to a great change in the derivative of the plotted function ('knee' or 'elbow' shape).\n",
    "    \n",
    "    Docs_vecs is the list of vectors we will analyze, each representing a document.\n",
    "    \n",
    "    min_count is the number of points needed to define a core point in DBSCAN.\n",
    "    \n",
    "    doc_titles is a matching list (wrt to docs_vecs), containing the titles of each doc.\n",
    "    \n",
    "    Returns a list of tuples (doc_title, distance from k-th neighbour), ORDERED by distance (ascendantly).\n",
    "    \"\"\"\n",
    "    \n",
    "    # first thing to do: compute the matrix of all pairwise elements distances\n",
    "    # warning: this code is not optimized\n",
    "    dist_matrix = get_pairwise_distances_matrix(docs_vecs)\n",
    "    \n",
    "    \n",
    "    list_ = [] \n",
    "    # for each document vec, only keep the DISTANCE from k-th closest document\n",
    "    for j, doc_distances in enumerate(dist_matrix):\n",
    "        # get a row of the matrix (vector of distances for doc_j)\n",
    "        \n",
    "        # discard the distance between a doc and itself\n",
    "        doc_distances = np.delete(doc_distances, j)\n",
    "        for i in range(0, min_count-1):\n",
    "            # get the closest doc to it and discard it, we only need the k-th closest doc.\n",
    "            doc_distances = np.delete(doc_distances, np.argmin(doc_distances))\n",
    "        # now create the pair: (doc_name, distance from k-th neighbour)\n",
    "        list_.append((doc_titles[j], np.amin(doc_distances)))\n",
    "        \n",
    "    # sort the list by the second parameter (distance)\n",
    "    list_.sort(key=lambda tup: tup[1])  # sorts in place\n",
    "    return list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics.pairwise as sk # for cosine_distance\n",
    "# TODO: add possibility of passing metric to use as parameters\n",
    "def get_pairwise_distances_matrix(docs):\n",
    "    \"\"\"\"\n",
    "        docs: list of documents, each represented as a vector.\n",
    "        \n",
    "        Returns the pairwise distances matrix between documents. \n",
    "    \n",
    "        Metric used to compute the distance is cosine_distance -by default-.\n",
    "    \"\"\"\n",
    "    # initialize distance matrix\n",
    "    n = len(docs)\n",
    "    distances_m = np.zeros((n, n))\n",
    "    \n",
    "    # compute the distance betweem each vector (doc)\n",
    "    # this is all but efficient at the moment, okay for a debug version.\n",
    "    for i, doc1 in enumerate(docs):\n",
    "        for j, doc2 in enumerate(docs):\n",
    "            distances_m[i, j] = sk.cosine_distances([doc1], [doc2])\n",
    "    return distances_m\n",
    "    \n",
    "#def get_kth_neighbour_distance(docvec, k):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "tls.set_credentials_file(username='D4nt3', api_key='FdMB4O6qCfciGDOnLvdQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.1180829  0.         0.02747092]\n",
      " [0.1180829  0.         0.1180829  0.03509872]\n",
      " [0.         0.1180829  0.         0.02747092]\n",
      " [0.02747092 0.03509872 0.02747092 0.        ]]\n",
      "[('Terzo', 0.03509871864598446), ('Primo', 0.11808289631180302), ('Secondo', 0.11808289631180302), ('Primo 2', 0.11808289631180302)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "b = get_pairwise_distances_matrix([[1, 2, 4], [2, 2, 2], [1, 2, 4], [2, 3, 4]])\n",
    "print(b)\n",
    "for i, row in enumerate(b):\n",
    "    row = np.delete(row, i)\n",
    "    row = np.delete(row, np.argmin(row))\n",
    "    #print(row, i, np.amin(row))\n",
    "a = choose_eps(3, [[1, 2, 4], [2, 2, 2], [1, 2, 4], [2, 3, 4]], [\"Primo\", \"Secondo\", \"Primo 2\", \"Terzo\"])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer vector given doc (as list of entities) and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_of_vectors(vectors):\n",
    "    \"\"\"given a list of vectors, return the simplest mean of vectors.\"\"\"\n",
    "    \n",
    "    sum_vectors = np.zeros(np.shape(vectors[0]))\n",
    "    for vec in vectors:\n",
    "        sum_vectors = sum_vectors + vec\n",
    "    return sum_vectors/len(vectors)\n",
    "\n",
    "def infer_vector(entities, model):\n",
    "    \"\"\"Given a list of entities, returns the vector representing the documents from which the entities \n",
    "    were extracted from, wrt a given W2V model.\n",
    "    \n",
    "    entities: list of entities, our way of representing a document.\n",
    "    model: w2v model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # get word vector of each entity; ignores word if the model does not know it\n",
    "    entities_vecs = []\n",
    "    for e in entities:\n",
    "        try:\n",
    "            entities_vecs.append(model[e])\n",
    "        except:\n",
    "            None # ignore unknown word\n",
    "    \n",
    "    return mean_of_vectors(entities_vecs)\n",
    "\n",
    "MODEL_NAME = 'TestModels/w2v_entities+abstract_model.model'\n",
    "model = Word2Vec.load(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real data test -OVER SUBSAMPLE OF DATA-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs: 96\n",
      "New length after removing docs:  91\n",
      "Number of docs: 599\n",
      "New length after removing docs:  362\n"
     ]
    }
   ],
   "source": [
    "filenames = ['blockchain.json', 'industria_4.0.json']\n",
    "\n",
    "# load multiple files, assuming same data format\n",
    "docs = []\n",
    "for filename in filenames:\n",
    "    with open(filename, 'r') as outfile:\n",
    "        json_data = json.load(outfile)\n",
    "\n",
    "    ## let's now retrieve the meaningful part of the json document\n",
    "    # response{}--->docs[] \n",
    "\n",
    "    docs = docs + json_data['response']['docs']\n",
    "    print(\"Number of docs:\",len(docs))\n",
    "    ## many documents have a failed abstract, let's remove them\n",
    "    to_check = ' Questo sito web utilizza cookie tecnici e, previo Suo consenso, cookie di profilazione,'\n",
    "    docs = [doc for i, doc in enumerate(docs) if not(to_check.strip() in doc['abstract'][0].strip())]\n",
    "\n",
    "    # remove duplicates (of a particular doc)\n",
    "    # TODO: remove all duplicates\n",
    "    docs = [doc for doc in docs\n",
    "                if not(\"Industry 4.0 (o industria 4.0): cos'è, notizie, normative, casi studio - I4T\" in doc['title'])]\n",
    "    print(\"New length after removing docs: \", len(docs))\n",
    "    \n",
    "## Adjust data format: title, abstract and url came in as list, but they're more useful as strings\n",
    "for i, dictionary in enumerate(docs):\n",
    "    for field in ['title', 'abstract', 'url']:\n",
    "        if isinstance(dictionary[field], list):\n",
    "            # re-format data to hold string instead of single-list item\n",
    "            docs[i][field] = dictionary[field][0]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['aristotele', 'automazione', 'azienda', 'basta_così_negramaro', 'benessere', 'capitale_umano', 'costituzione', \"credito_d'imposta\", 'digitale_informatica', 'digitalizzazione', 'domanda_di_lavoro', 'euro', 'europa', 'fattore_produttivo', 'filiera', 'forza-lavoro', 'homo_sapiens', 'impresa', 'industria', 'industria_4.0', 'lavoro', 'luce', 'macroeconomia', 'milano', 'numero', 'numero_reale', 'parlamento', 'partenariato', 'piccola_e_media_impresa', 'politecnico_di_milano', 'produttività', 'sabbia', 'scetticismo_scientifico', 'sindacato', 'sistema', 'tecnologia', 'università', 'velocità', 'xxi_secolo']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# shuffle docs to get a random sub-sample\n",
    "random.shuffle(docs)\n",
    "subsample_length = 50\n",
    "subsample = docs[:subsample_length]\n",
    "subsample_titles = [doc['title'] for doc in subsample]\n",
    "\n",
    "\n",
    "# get flattened_entities for each document, AS LIST of words (not a single string)\n",
    "doc_entities = [doc['flattened_entities'].split() for doc in subsample]\n",
    "print(doc_entities[:1])\n",
    "\n",
    "# now we have to 'convert' every doc to vector form\n",
    "docs_vecs = [infer_vector(list_ent, model) for list_ent in doc_entities]\n",
    "\n",
    "# finally, call eps-estimate function\n",
    "title_dist_tuples = choose_eps(2, docs_vecs, subsample_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~D4nt3/48.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace = go.Scatter(\n",
    "    x =[x for (x, y) in title_dist_tuples],  # list of x\n",
    "    y = [y for (x, y) in title_dist_tuples],\n",
    "    mode = 'lines',\n",
    "    name = 'lines'\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "py.iplot(data, filename = 'dbscan-eps-choosing')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
