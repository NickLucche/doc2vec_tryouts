{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Y = tsne.tsne(X, no_dims, perplexity) to perform t-SNE on your dataset.\n",
      "Running example on 2,500 MNIST digits...\n",
      "Computing pairwise distances...\n",
      "Computing P-values for point 0 of 4...\n",
      "Mean value of sigma: 33554432.000000\n",
      "Iteration 10: error is 15.152367\n",
      "Iteration 20: error is 13.556963\n",
      "Iteration 30: error is 9.621467\n",
      "Iteration 40: error is 7.790555\n",
      "Iteration 50: error is 7.732668\n",
      "Iteration 60: error is 7.742886\n",
      "Iteration 70: error is 7.756797\n",
      "Iteration 80: error is 8.250028\n",
      "Iteration 90: error is 15.152523\n",
      "Iteration 100: error is 16.903028\n",
      "Iteration 110: error is 0.268926\n",
      "Iteration 120: error is 0.148632\n",
      "Iteration 130: error is 0.137321\n",
      "Iteration 140: error is 0.135301\n",
      "Iteration 150: error is 0.131533\n",
      "Iteration 160: error is 0.124905\n",
      "Iteration 170: error is 0.111211\n",
      "Iteration 180: error is 0.086243\n",
      "Iteration 190: error is 0.060491\n",
      "Iteration 200: error is 0.052400\n",
      "Iteration 210: error is 0.051403\n",
      "Iteration 220: error is 0.050671\n",
      "Iteration 230: error is 0.050003\n",
      "Iteration 240: error is 0.049519\n",
      "Iteration 250: error is 0.049190\n",
      "Iteration 260: error is 0.048983\n",
      "Iteration 270: error is 0.048860\n",
      "Iteration 280: error is 0.048791\n",
      "Iteration 290: error is 0.048755\n",
      "Iteration 300: error is 0.048738\n",
      "Iteration 310: error is 0.048731\n",
      "Iteration 320: error is 0.048728\n",
      "Iteration 330: error is 0.048727\n",
      "Iteration 340: error is 0.048726\n",
      "Iteration 350: error is 0.048726\n",
      "Iteration 360: error is 0.048726\n",
      "Iteration 370: error is 0.048726\n",
      "Iteration 380: error is 0.048726\n",
      "Iteration 390: error is 0.048726\n",
      "Iteration 400: error is 0.048726\n",
      "Iteration 410: error is 0.048726\n",
      "Iteration 420: error is 0.048726\n",
      "Iteration 430: error is 0.048726\n",
      "Iteration 440: error is 0.048726\n",
      "Iteration 450: error is 0.048726\n",
      "Iteration 460: error is 0.048726\n",
      "Iteration 470: error is 0.048726\n",
      "Iteration 480: error is 0.048726\n",
      "Iteration 490: error is 0.048726\n",
      "Iteration 500: error is 0.048726\n",
      "Iteration 510: error is 0.048726\n",
      "Iteration 520: error is 0.048726\n",
      "Iteration 530: error is 0.048726\n",
      "Iteration 540: error is 0.048726\n",
      "Iteration 550: error is 0.048726\n",
      "Iteration 560: error is 0.048726\n",
      "Iteration 570: error is 0.048726\n",
      "Iteration 580: error is 0.048726\n",
      "Iteration 590: error is 0.048726\n",
      "Iteration 600: error is 0.048726\n",
      "Iteration 610: error is 0.048726\n",
      "Iteration 620: error is 0.048726\n",
      "Iteration 630: error is 0.048726\n",
      "Iteration 640: error is 0.048726\n",
      "Iteration 650: error is 0.048726\n",
      "Iteration 660: error is 0.048726\n",
      "Iteration 670: error is 0.048726\n",
      "Iteration 680: error is 0.048726\n",
      "Iteration 690: error is 0.048726\n",
      "Iteration 700: error is 0.048726\n",
      "Iteration 710: error is 0.048726\n",
      "Iteration 720: error is 0.048726\n",
      "Iteration 730: error is 0.048726\n",
      "Iteration 740: error is 0.048726\n",
      "Iteration 750: error is 0.048726\n",
      "Iteration 760: error is 0.048726\n",
      "Iteration 770: error is 0.048726\n",
      "Iteration 780: error is 0.048726\n",
      "Iteration 790: error is 0.048726\n",
      "Iteration 800: error is 0.048726\n",
      "Iteration 810: error is 0.048726\n",
      "Iteration 820: error is 0.048726\n",
      "Iteration 830: error is 0.048726\n",
      "Iteration 840: error is 0.048726\n",
      "Iteration 850: error is 0.048726\n",
      "Iteration 860: error is 0.048726\n",
      "Iteration 870: error is 0.048726\n",
      "Iteration 880: error is 0.048726\n",
      "Iteration 890: error is 0.048726\n",
      "Iteration 900: error is 0.048726\n",
      "Iteration 910: error is 0.048726\n",
      "Iteration 920: error is 0.048726\n",
      "Iteration 930: error is 0.048726\n",
      "Iteration 940: error is 0.048726\n",
      "Iteration 950: error is 0.048726\n",
      "Iteration 960: error is 0.048726\n",
      "Iteration 970: error is 0.048726\n",
      "Iteration 980: error is 0.048726\n",
      "Iteration 990: error is 0.048726\n",
      "Iteration 1000: error is 0.048726\n",
      "This is X: [[ 1.11   2.11   3.11   4.11   5.11   6.11   7.11   8.11   9.11  10.11 ]\n",
      " [ 1.112  2.112  3.112  4.112  5.112  6.112  7.112  8.112  9.112 10.112]\n",
      " [ 1.112  2.112  3.112  4.112  5.112  6.112  7.112  8.112  9.112 10.112]\n",
      " [ 1.     2.     3.     4.     5.     6.     7.     8.     9.    10.   ]]\n",
      "[[ -13.16019801  167.35782495]\n",
      " [  13.16019945 -167.35782447]\n",
      " [ 167.35741199   13.1601685 ]\n",
      " [-167.35741343  -13.16016898]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "#  tsne.py\n",
    "#\n",
    "# Implementation of t-SNE in Python. The implementation was tested on Python\n",
    "# 2.7.10, and it requires a working installation of NumPy. The implementation\n",
    "# comes with an example on the MNIST dataset. In order to plot the\n",
    "# results of this example, a working installation of matplotlib is required.\n",
    "#\n",
    "# The example can be run by executing: `ipython tsne.py`\n",
    "#\n",
    "#\n",
    "#  Created by Laurens van der Maaten on 20-12-08.\n",
    "#  Copyright (c) 2008 Tilburg University. All rights reserved.\n",
    "\n",
    "import numpy as np\n",
    "import pylab\n",
    "\n",
    "\n",
    "def Hbeta(D=np.array([]), beta=1.0):\n",
    "    \"\"\"\n",
    "        Compute the perplexity and the P-row for a specific value of the\n",
    "        precision of a Gaussian distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute P-row and corresponding perplexity\n",
    "    P = np.exp(-D.copy() * beta)\n",
    "    sumP = sum(P)\n",
    "    H = np.log(sumP) + beta * np.sum(D * P) / sumP\n",
    "    P = P / sumP\n",
    "    return H, P\n",
    "\n",
    "\n",
    "def x2p(X=np.array([]), tol=1e-5, perplexity=30.0):\n",
    "    \"\"\"\n",
    "        Performs a binary search to get P-values in such a way that each\n",
    "        conditional Gaussian has the same perplexity.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize some variables\n",
    "    print(\"Computing pairwise distances...\")\n",
    "    (n, d) = X.shape\n",
    "    sum_X = np.sum(np.square(X), 1)\n",
    "    D = np.add(np.add(-2 * np.dot(X, X.T), sum_X).T, sum_X)\n",
    "    P = np.zeros((n, n))\n",
    "    beta = np.ones((n, 1))\n",
    "    logU = np.log(perplexity)\n",
    "\n",
    "    # Loop over all datapoints\n",
    "    for i in range(n):\n",
    "\n",
    "        # Print progress\n",
    "        if i % 500 == 0:\n",
    "            print(\"Computing P-values for point %d of %d...\" % (i, n))\n",
    "\n",
    "        # Compute the Gaussian kernel and entropy for the current precision\n",
    "        betamin = -np.inf\n",
    "        betamax = np.inf\n",
    "        Di = D[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))]\n",
    "        (H, thisP) = Hbeta(Di, beta[i])\n",
    "\n",
    "        # Evaluate whether the perplexity is within tolerance\n",
    "        Hdiff = H - logU\n",
    "        tries = 0\n",
    "        while np.abs(Hdiff) > tol and tries < 50:\n",
    "\n",
    "            # If not, increase or decrease precision\n",
    "            if Hdiff > 0:\n",
    "                betamin = beta[i].copy()\n",
    "                if betamax == np.inf or betamax == -np.inf:\n",
    "                    beta[i] = beta[i] * 2.\n",
    "                else:\n",
    "                    beta[i] = (beta[i] + betamax) / 2.\n",
    "            else:\n",
    "                betamax = beta[i].copy()\n",
    "                if betamin == np.inf or betamin == -np.inf:\n",
    "                    beta[i] = beta[i] / 2.\n",
    "                else:\n",
    "                    beta[i] = (beta[i] + betamin) / 2.\n",
    "\n",
    "            # Recompute the values\n",
    "            (H, thisP) = Hbeta(Di, beta[i])\n",
    "            Hdiff = H - logU\n",
    "            tries += 1\n",
    "\n",
    "        # Set the final row of P\n",
    "        P[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))] = thisP\n",
    "\n",
    "    # Return final P-matrix\n",
    "    print(\"Mean value of sigma: %f\" % np.mean(np.sqrt(1 / beta)))\n",
    "    return P\n",
    "\n",
    "\n",
    "def pca(X=np.array([]), no_dims=50):\n",
    "    \"\"\"\n",
    "        Runs PCA on the NxD array X in order to reduce its dimensionality to\n",
    "        no_dims dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Preprocessing the data using PCA...\")\n",
    "    (n, d) = X.shape\n",
    "    X = X - np.tile(np.mean(X, 0), (n, 1))\n",
    "    (l, M) = np.linalg.eig(np.dot(X.T, X))\n",
    "    Y = np.dot(X, M[:, 0:no_dims])\n",
    "    return Y\n",
    "\n",
    "\n",
    "def tsne(X=np.array([]), no_dims=2, initial_dims=50, perplexity=30.0):\n",
    "    \"\"\"\n",
    "        Runs t-SNE on the dataset in the NxD array X to reduce its\n",
    "        dimensionality to no_dims dimensions. The syntaxis of the function is\n",
    "        `Y = tsne.tsne(X, no_dims, perplexity), where X is an NxD NumPy array.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check inputs\n",
    "    if isinstance(no_dims, float):\n",
    "        print(\"Error: array X should have type float.\")\n",
    "        return -1\n",
    "    if round(no_dims) != no_dims:\n",
    "        print(\"Error: number of dimensions should be an integer.\")\n",
    "        return -1\n",
    "\n",
    "    # Initialize variables\n",
    "    # X = pca(X, initial_dims).real\n",
    "    (n, d) = X.shape\n",
    "    max_iter = 1000\n",
    "    initial_momentum = 0.5\n",
    "    final_momentum = 0.8\n",
    "    eta = 500\n",
    "    min_gain = 0.01\n",
    "    Y = np.random.randn(n, no_dims)\n",
    "    dY = np.zeros((n, no_dims))\n",
    "    iY = np.zeros((n, no_dims))\n",
    "    gains = np.ones((n, no_dims))\n",
    "\n",
    "    # Compute P-values\n",
    "    P = x2p(X, 1e-5, perplexity)\n",
    "    P = P + np.transpose(P)\n",
    "    P = P / np.sum(P)\n",
    "    P = P * 4.\t\t\t\t\t\t\t\t\t# early exaggeration\n",
    "    P = np.maximum(P, 1e-12)\n",
    "\n",
    "    # Run iterations\n",
    "    for iter in range(max_iter):\n",
    "\n",
    "        # Compute pairwise affinities\n",
    "        sum_Y = np.sum(np.square(Y), 1)\n",
    "        num = -2. * np.dot(Y, Y.T)\n",
    "        num = 1. / (1. + np.add(np.add(num, sum_Y).T, sum_Y))\n",
    "        num[range(n), range(n)] = 0.\n",
    "        Q = num / np.sum(num)\n",
    "        Q = np.maximum(Q, 1e-12)\n",
    "\n",
    "        # Compute gradient\n",
    "        PQ = P - Q\n",
    "        for i in range(n):\n",
    "            dY[i, :] = np.sum(np.tile(PQ[:, i] * num[:, i], (no_dims, 1)).T * (Y[i, :] - Y), 0)\n",
    "\n",
    "        # Perform the update\n",
    "        if iter < 20:\n",
    "            momentum = initial_momentum\n",
    "        else:\n",
    "            momentum = final_momentum\n",
    "        gains = (gains + 0.2) * ((dY > 0.) != (iY > 0.)) + \\\n",
    "                (gains * 0.8) * ((dY > 0.) == (iY > 0.))\n",
    "        gains[gains < min_gain] = min_gain\n",
    "        iY = momentum * iY - eta * (gains * dY)\n",
    "        Y = Y + iY\n",
    "        Y = Y - np.tile(np.mean(Y, 0), (n, 1))\n",
    "\n",
    "        # Compute current value of cost function\n",
    "        if (iter + 1) % 10 == 0:\n",
    "            C = np.sum(P * np.log(P / Q))\n",
    "            print(\"Iteration %d: error is %f\" % (iter + 1, C))\n",
    "\n",
    "        # Stop lying about P-values\n",
    "        if iter == 100:\n",
    "            P = P / 4.\n",
    "\n",
    "    # Return solution\n",
    "    return Y\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Run Y = tsne.tsne(X, no_dims, perplexity) to perform t-SNE on your dataset.\")\n",
    "    print(\"Running example on 2,500 MNIST digits...\")\n",
    "    #X = np.loadtxt(\"mnist2500_X.txt\")\n",
    "    X = np.loadtxt(\"sample_data.txt\")\n",
    "    labels = np.loadtxt(\"mnist2500_labels.txt\")\n",
    "    Y = tsne(X, 2, 10, 20.0)\n",
    "    #pylab.scatter(Y[:, 0], Y[:, 1], 20, labels)\n",
    "    print(\"This is X: {}\".format(X))\n",
    "    print(Y)\n",
    "    pylab.scatter(Y[:, 0], Y[:, 1], 20)\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
