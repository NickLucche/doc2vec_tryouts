{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2: Model trained on text entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA imports\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec imports\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import gensim\n",
    "\n",
    "import json # to open our data file\n",
    "DATA_FILENAME = \"trend_analisys.json\"\n",
    "# open json file\n",
    "with open(DATA_FILENAME, \"r\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "# we're expecting a list now, since our json file is a json array\n",
    "assert type(json_data) is list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanitize data from duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-92\n",
      "Number of duplicates:  85\n",
      "New length:  208\n"
     ]
    }
   ],
   "source": [
    "# utilize dictionary comprehension, since dictionary does not allow duplicate keys\n",
    "\n",
    "unique_json = { obj['title'] : obj for obj in json_data }.values()\n",
    "#unique_json = json_data\n",
    "print(len(unique_json)-len(json_data))\n",
    "\n",
    "unique_json = json_data\n",
    "counter = 0\n",
    "for i, dictionary in enumerate(unique_json):\n",
    "    try:\n",
    "        index = json_data.index(dictionary, i+1, len(json_data))\n",
    "        #print(\"Found a duplicate with index {0} from index {1}\".format(index, i))\n",
    "        del(unique_json[index])\n",
    "        counter = counter + 1\n",
    "    except ValueError:\n",
    "        None\n",
    "print(\"Number of duplicates: \", counter)\n",
    "print(\"New length: \", len(unique_json))\n",
    "#print(unique_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold Cross Validation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 training time: \n",
      "0.178477\n",
      "Model 0 Saved\n",
      "Model 1 training time: \n",
      "0.19014599999999948\n",
      "Model 1 Saved\n",
      "Model 2 training time: \n",
      "0.1763439999999994\n",
      "Model 2 Saved\n",
      "Model 3 training time: \n",
      "0.18684499999999993\n",
      "Model 3 Saved\n",
      "Model 4 training time: \n",
      "0.2258849999999999\n",
      "Model 4 Saved\n"
     ]
    }
   ],
   "source": [
    "# Since we don't have many data at our disposal, we'll use a k-fold \n",
    "# training/testing method, with a fold value of 5\n",
    "# (we might have to try at least a few values of K and see how the models behave)\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "import time\n",
    "\n",
    "KFOLD_VALUE = 5\n",
    "# args: number of folds, shuffle/don't shuffle data, seed for random permutation\n",
    "kfold = KFold(KFOLD_VALUE, True, 1)\n",
    "ENTITIES_FIELD_NAME = \"flattened_entities\"\n",
    "model_number = 0\n",
    "vec_size = 50\n",
    "MODEL_BASENAME = \"Models\"+os.sep+\"d2v_TA_entities_model\"\n",
    "unique_json = np.array(unique_json)\n",
    "#print(unique_json[:1])\n",
    "\n",
    "models = []\n",
    "\n",
    "for train, test in kfold.split(unique_json):\n",
    "    # create a TaggedDocument out of the training corpus \n",
    "    ## CARE: simple_preprocess also removes 'stop-words' (in english)\n",
    "    train_corpus = [gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(\n",
    "    d[ENTITIES_FIELD_NAME]), [i]) for i, d in enumerate(unique_json[train])]\n",
    "    # pre-process test_corpus\n",
    "    test_corpus = [gensim.utils.simple_preprocess(d[ENTITIES_FIELD_NAME]) for d in unique_json[test]]\n",
    "    \n",
    "    #print(train_corpus[:1])\n",
    "    ## create model ##\n",
    "    # changes I tried in this model: vector_size, epochs, default learning rate, epochs\n",
    "    # and no loop while training model\n",
    "    model = gensim.models.doc2vec.Doc2Vec(vector_size=vec_size,\n",
    "                                          min_count=2,\n",
    "                                          epochs=40,\n",
    "                                          dm = 0) # distributed bag of words (PV-DBOW) is employed\n",
    "    model.build_vocab(train_corpus)\n",
    "    #print(\"Length of train corpus: \", len(train_corpus))\n",
    "    ## Train model ##\n",
    "    print(\"Model %s training time: \" %(model_number))\n",
    "    \n",
    "    time_start = time.clock()\n",
    "    #for epoch in range(model.epochs):\n",
    "    model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    time_elapsed = (time.clock() - time_start)\n",
    "    print(str(time_elapsed))\n",
    "    \n",
    "    model.save(MODEL_BASENAME + str(model_number) + '.model')\n",
    "    print(\"Model %s Saved\" %(model_number))\n",
    "    model_number = model_number + 1\n",
    "    \n",
    "    ## Test model in order to choose best one ##\n",
    "    \n",
    "    # add model to list of models\n",
    "    models.append((model, unique_json[train]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test given model, over given test set\n",
    "# TODO: put some meaningful checks here (data type, raise errors and such..)\n",
    "def test_model (model, test_set):\n",
    "    # we're expecting flattened_entities here,\n",
    "    # which are contained in the form of a string separated by spaces by default\n",
    "    for document in test_set:\n",
    "        # infer_vector() does not take a string, but rather a list of string tokens\n",
    "        # The str.split() method without an argument splits on whitespace\n",
    "        inferred_vector = model.infer_vector(document.split())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~D4nt3/8.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "tls.set_credentials_file(username='D4nt3', api_key='FdMB4O6qCfciGDOnLvdQ')\n",
    "\n",
    "COMPONENT_ONE = \"principal component 1\"\n",
    "COMPONENT_TWO = \"principal component 2\"\n",
    "\n",
    "docs_vecs = []\n",
    "model, training_set = models[2]\n",
    "# docvecs (list of Doc2VecKeyedVectors) \n",
    "# â€“ Vector representations of the documents in the corpus. Each vector has size == vector_size\n",
    "for doc in iter(range(0, len(model.docvecs))):\n",
    "    docs_vecs.append(model.docvecs[doc])\n",
    "#print(docs_vecs)\n",
    "# loading dataset into Pandas DataFrame\n",
    "df = pd.DataFrame.from_records(docs_vecs)    \n",
    "    ## PCA dimensionality-reduction ##\n",
    "# PCA is effected by scale so you need to scale the features in your data before applying PCA. \n",
    "features = [i for i in range(vec_size)]\n",
    "x = df.loc[:, features].values # get features values\n",
    "#print(x)\n",
    "# standardize data\n",
    "x = StandardScaler().fit_transform(x) # scale data (especially in case different measures are used)\n",
    "    \n",
    "# build PCA model in 2D\n",
    "pca = PCA(n_components=2) # The new components are just the two main dimensions of variation.\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents, \n",
    "                           columns = [COMPONENT_ONE, COMPONENT_TWO])\n",
    "    \n",
    "# we'll draw a scatter graph with labels\n",
    "traces = []\n",
    "# let's get the labels\n",
    "titles = [dictionary['title'] for dictionary in training_set]\n",
    "#print(print(titles))\n",
    "finalDf = principalDf\n",
    "\n",
    "# double check to be sure we got labels just right\n",
    "#sample_title = training_set[10]['title']\n",
    "#sample_text = training_set[10]['flattened_entities']\n",
    "#inferred_vector = model.infer_vector(gensim.utils.simple_preprocess(sample_text))\n",
    "#print(gensim.utils.simple_preprocess(sample_text))\n",
    "# pca sample\n",
    "#x = np.array(inferred_vector)\n",
    "#pca = PCA(n_components=2)\n",
    "#pca_result = pca.fit_transform(x)\n",
    "#trace_sample = go.Scatter(\n",
    "#        x = pca_result[0],\n",
    "#        y = pca_result[1],\n",
    "#        mode = 'markers',\n",
    "#            #name = 'blue markers',\n",
    "#        marker = dict(\n",
    "#            size = 7,\n",
    "#            color = 'green',\n",
    "#        ),\n",
    "#        text = str(sample_title)\n",
    "#    )\n",
    "#traces.append(trace_sample)\n",
    "\n",
    "\n",
    "    # each trace will represent a point (squeezed vector from higher dimensions),\n",
    "    # and each point will have the title of the news assigned\n",
    "for i in range(len(finalDf)):\n",
    "    color = 'rgba(0, 0, 110, .8)'\n",
    "    if 'Apple' in titles[i]:\n",
    "            color = 'rgba(120, 0, 0, .9)'\n",
    "    elif 'Amazon' in titles[i]:\n",
    "        color = 'yellow'\n",
    "    elif 'Facebook' in titles[i] or 'Instagram' in titles[i]:\n",
    "        color = 'green'\n",
    "    elif 'spazio' in training_set[i][ENTITIES_FIELD_NAME]:\n",
    "        color = 'black'\n",
    "    \n",
    "    trace0 = go.Scatter(\n",
    "        x = finalDf.loc[i:i, \"principal component 1\"],\n",
    "        y = finalDf.loc[i:i, \"principal component 2\"],\n",
    "        mode = 'markers',\n",
    "            #name = 'blue markers',\n",
    "        marker = dict(\n",
    "            size = 7,\n",
    "            color = color,\n",
    "        ),\n",
    "        text = str(titles[i])\n",
    "    )\n",
    "    traces.append(trace0)\n",
    "\n",
    "data = traces \n",
    "layout = dict(title = 'PCA Representantion of DocVectors',\n",
    "            hovermode= 'closest',\n",
    "            xaxis= dict(\n",
    "                title= 'first component',\n",
    "                ticklen= 5,\n",
    "                gridwidth= 2,\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                title= 'second component',\n",
    "                ticklen= 5,\n",
    "                gridwidth= 2,\n",
    "            ),\n",
    "            showlegend = False\n",
    "        )\n",
    "# Plot and embed in ipython notebook!\n",
    "    \n",
    "fig = dict(data = data, layout = layout)\n",
    "py.iplot(fig, filename='TA_model_entities-scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means on PCA-reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=600,\n",
       "    n_clusters=5, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try another way for clustering data: K-Mean, an even more popular algorithm,\n",
    "# which I know from the introductory course on AI, so it might be better \n",
    "# to utilize algorithms which I know and can talk about in the presentation\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# build k-means model\n",
    "kmeans = KMeans(n_clusters = 5, max_iter=600, algorithm = 'auto', verbose=0,\n",
    "               init='k-means++', n_init=10) \n",
    "\n",
    "kmeans.fit(principalComponents) # data, as vectors of documents (in 2D)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize centroids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~D4nt3/22.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "tls.set_credentials_file(username='D4nt3', api_key='FdMB4O6qCfciGDOnLvdQ')\n",
    "\n",
    "COMPONENT_ONE = \"principal component 1\"\n",
    "COMPONENT_TWO = \"principal component 2\"\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# each trace will represent a point (squeezed vector from higher dimensions),\n",
    "# and each point will have the title of the news assigned\n",
    "for i in range(len(finalDf)):\n",
    "    # assign a color to each point belonging to a specific cluster\n",
    "    # computing distance from centroid\n",
    "    x = finalDf.loc[i:i, \"principal component 1\"]\n",
    "    y = finalDf.loc[i:i, \"principal component 2\"]\n",
    "    color = 'rgba(0, 0, 180, 0.8)'\n",
    "    min_d = 10000\n",
    "    closest_centroid = []\n",
    "    for centroid in centroids:\n",
    "        dist = np.linalg.norm(centroid-np.array(x, y))\n",
    "        if dist<min_d:\n",
    "            min_d = dist\n",
    "            closest_centroid = centroid\n",
    "    #print(\"Prediction: \",closest_centroid)\n",
    "    if np.array_equal(closest_centroid, centroids[0]):\n",
    "        color = 'blue'\n",
    "    elif np.array_equal(closest_centroid, centroids[1]):\n",
    "        color = 'red'\n",
    "    elif np.array_equal(closest_centroid, centroids[2]):\n",
    "        color = 'yellow'\n",
    "    elif np.array_equal(closest_centroid, centroids[3]):\n",
    "        color = 'green'\n",
    "    else:\n",
    "        color = 'black'\n",
    "        \n",
    "    trace0 = go.Scatter(\n",
    "        x = x, \n",
    "        y = y,\n",
    "        mode = 'markers',\n",
    "            #name = 'blue markers',\n",
    "        marker = dict(\n",
    "            size = 7,\n",
    "            color = color,\n",
    "        ),\n",
    "        text = str(titles[i])\n",
    "    )\n",
    "    traces.append(trace0)\n",
    "\n",
    "# draw centroids\n",
    "c_trace = go.Scatter(\n",
    "    x = centroids[:, 0],\n",
    "    y = centroids[:, 1],\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        size = 9,\n",
    "        color = 'red',\n",
    "    ),\n",
    "    text = 'centroid'\n",
    ")\n",
    "traces.append(c_trace)\n",
    "\n",
    "data = traces \n",
    "layout = dict(title = 'PCA Representantion of DocVectors',\n",
    "            hovermode= 'closest',\n",
    "            xaxis= dict(\n",
    "                title= 'first component',\n",
    "                ticklen= 5,\n",
    "                gridwidth= 2,\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                title= 'second component',\n",
    "                ticklen= 5,\n",
    "                gridwidth= 2,\n",
    "            ),\n",
    "            showlegend = False\n",
    "        )\n",
    "# Plot and embed in ipython notebook!\n",
    "    \n",
    "fig = dict(data = data, layout = layout)\n",
    "py.iplot(fig, filename='TA_model_entities_kmeans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
