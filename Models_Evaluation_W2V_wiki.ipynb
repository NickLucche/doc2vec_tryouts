{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare W2V models results over eval.set\n",
    "## TODO: Weighted mean of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load eval.set (duplicates free)\n",
    "import json\n",
    "filename = 'pre-clustered_docs_harder.json'\n",
    "#filename = 'english_3_clusters.json' we need an english wikipedia model for this\n",
    "with open(filename, 'r') as file:\n",
    "    cdocs = json.load(file)\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install plotly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load entities models\n",
    "## TODO: Check entities format, does it really match the word in the vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nick_\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nick_\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained model\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "unwrapped_docs = [doc for cluster in cdocs for doc in cluster]\n",
    "\n",
    "wiki_model = Word2Vec.load('./models/wiki_iter=5_algorithm=skipgram_window=10_size=300_neg-samples=10.m')\n",
    "#my_model = Word2Vec.load('./models/w2v_entities+abstract_model.model')\n",
    "print(\"Models loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "[[-0.4029415  -0.13100496  0.11882563 ... -0.33560228 -0.11999029\n",
      "   0.24970573]\n",
      " [-0.33952245 -0.22362994  0.06498575 ... -0.28428683 -0.10469311\n",
      "   0.16570613]\n",
      " [-0.3010722  -0.08241285  0.12940371 ... -0.35231194 -0.03992369\n",
      "   0.22636156]\n",
      " ...\n",
      " [-0.45489705  0.24162287  0.19298626 ... -0.348121   -0.03367177\n",
      "  -0.03407436]\n",
      " [-0.72975105  0.35789543  0.35934475 ... -0.50478923 -0.03901616\n",
      "  -0.08851031]\n",
      " [-0.723315    0.32045954  0.33745176 ... -0.49936923 -0.03127839\n",
      "  -0.07138227]]\n",
      "{0: []} DBSCAN(algorithm='auto', eps=0.27, leaf_size=30, metric='euclidean',\n",
      "    metric_params=None, min_samples=2, n_jobs=None, p=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:54: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "\n",
      "/home/nick/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "\n",
      "/home/nick/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # Make sure that numpy is imported\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "        elif word.lower() in index2word_set:\n",
    "            nwords += 1.\n",
    "            featureVec = np.add(featureVec, model[word.lower()])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(docs, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(docs),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for doc in docs:\n",
    "       #\n",
    "       # Print a status message every 1000th review\n",
    "        #if counter%1000. == 0.:\n",
    "        #    print(\"Review %d of %d\" % (counter, len(docs)))\n",
    "       # \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(doc['result_entities'], model, \\\n",
    "           num_features)\n",
    "       #\n",
    "       # Increment the counter\n",
    "        counter = counter + 1\n",
    "    return reviewFeatureVecs\n",
    "print(my_model['re'].shape)\n",
    "trainDataVecs = getAvgFeatureVecs( unwrapped_docs, my_model, my_model.wv.vector_size )\n",
    "print(trainDataVecs)\n",
    "#urls_cluster_list = my_dbscan.perform_dbscan(eps = 0.4, min_samples = 4, data = trainDataVecs, verbose = True\n",
    "#                  , titles = None, urls = None, print_noise = True)\n",
    "db, clusters = my_dbscan.perform_dbscan(eps = 0.27, min_samples = 2, data = my_doc_vecs, verbose = True\n",
    "                  , titles = titles, urls = urls, print_noise = True)\n",
    "print(clusters, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known words: 96.66666666666667%  (29/30)\n",
      "Unknown entities found:  ['Radioattivita']\n",
      "Known words: 100.0%  (9/9)\n",
      "Known words: 100.0%  (23/23)\n",
      "Known words: 100.0%  (17/17)\n",
      "Known words: 75.0%  (3/4)\n",
      "Unknown entities found:  ['Tinder']\n",
      "Known words: 92.85714285714286%  (13/14)\n",
      "Unknown entities found:  ['Tinder']\n",
      "Known words: 92.3076923076923%  (12/13)\n",
      "Unknown entities found:  ['Tinder']\n",
      "Known words: 100.0%  (15/15)\n",
      "Known words: 100.0%  (18/18)\n",
      "Known words: 100.0%  (26/26)\n",
      "Known words: 100.0%  (28/28)\n",
      "Known words: 97.72727272727273%  (43/44)\n",
      "Unknown entities found:  ['Perpendicolarita']\n",
      "Known words: 100.0%  (7/7)\n",
      "Known words: 100.0%  (21/21)\n",
      "Known words: 96.92307692307692%  (63/65)\n",
      "Unknown entities found:  ['Pro12', 'CarPlay']\n",
      "Known words: 100.0%  (20/20)\n",
      "Known words: 100.0%  (61/61)\n",
      "Known words: 100.0%  (13/13)\n",
      "Known words: 100.0%  (18/18)\n",
      "Known words: 97.87234042553192%  (92/94)\n",
      "Unknown entities found:  ['1080p', 'IBeacon']\n",
      "Known words: 93.33333333333333%  (14/15)\n",
      "Unknown entities found:  ['Occhiaia']\n",
      "Known words: 100.0%  (16/16)\n",
      "Known words: 100.0%  (26/26)\n",
      "Known words: 100.0%  (11/11)\n",
      "Known words: 100.0%  (18/18)\n",
      "Known words: 100.0%  (29/29)\n",
      "Known words: 100.0%  (24/24)\n",
      "Known words: 100.0%  (18/18)\n",
      "Known words: 100.0%  (33/33)\n",
      "Known words: 99.11504424778761%  (112/113)\n",
      "Unknown entities found:  ['2. Fuball-Bundesliga']\n",
      "Known words: 100.0%  (18/18)\n",
      "Known words: 100.0%  (31/31)\n",
      "Known words: 83.33333333333333%  (5/6)\n",
      "Unknown entities found:  ['Pubblicita']\n",
      "Known words: 85.71428571428571%  (12/14)\n",
      "Unknown entities found:  ['Freaks!', 'Sessualita']\n",
      "Known words: 100.0%  (18/18)\n",
      "Known words: 100.0%  (16/16)\n",
      "Known words: 100.0%  (21/21)\n",
      "Known words: 100.0%  (30/30)\n",
      "Known words: 100.0%  (22/22)\n",
      "Known words: 96.42857142857143%  (27/28)\n",
      "Unknown entities found:  ['Multimedialita']\n",
      "Known words: 100.0%  (26/26)\n",
      "Known words: 100.0%  (12/12)\n",
      "Known words: 100.0%  (9/9)\n",
      "Known words: 94.73684210526316%  (18/19)\n",
      "Unknown entities found:  ['Multimedialita']\n",
      "Known words: 98.11320754716981%  (52/53)\n",
      "Unknown entities found:  ['Milliamperora']\n",
      "Known words: 100.0%  (27/27)\n",
      "Known words: 100.0%  (15/15)\n",
      "Known words: 97.0873786407767%  (100/103)\n",
      "Unknown entities found:  ['Multi-band', 'Milliamperora', 'Multimedialita']\n",
      "Known words: 93.33333333333333%  (14/15)\n",
      "Unknown entities found:  ['1080p']\n",
      "Known words: 88.23529411764706%  (15/17)\n",
      "Unknown entities found:  ['1080p', 'Milliamperora']\n",
      "Known words: 100.0%  (14/14)\n",
      "Known words: 93.33333333333333%  (28/30)\n",
      "Unknown entities found:  ['Qualita', 'FTTx']\n",
      "Known words: 97.1830985915493%  (69/71)\n",
      "Unknown entities found:  ['Portabilita', '1080p']\n",
      "Known words: 97.33333333333333%  (73/75)\n",
      "Unknown entities found:  ['1080p', 'Usabilita']\n",
      "1533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nick_\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\nick_\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "# check unknown words to model\n",
    "import numpy as np\n",
    "import w2v_model_utils as mm_utils\n",
    "import importlib\n",
    "importlib.reload(mm_utils)\n",
    "model = wiki_model\n",
    "unknown = []\n",
    "n_words = 0\n",
    "index2word_set = set(model.wv.index2word)\n",
    "for doc in unwrapped_docs:\n",
    "    mm_utils.infer_vector(doc['result_entities'], model, index2word_set, verbose=True) \n",
    "                    \n",
    "#print(unknown, len(unknown),'over', n_words)\n",
    "#model['elon musk']\n",
    "model.most_similar(positive=['re', 'donna'], negative=['uomo'])\n",
    "#model['energia']\n",
    "print(len([e for d in unwrapped_docs for e in d['result_entities']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nick_\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word 'Sessualita' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-62f4504efa9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m#text = re.sub('[^0-9a-zA-Z_-]', '', text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[0mwiki_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext_to_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sessualità\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;31m#text_to_id(\"Montréal\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1420\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1421\u001b[0m                 )\n\u001b[1;32m-> 1422\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1424\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m   1101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m         \"\"\"\n\u001b[1;32m-> 1103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Method will be removed in 4.0.0, use self.wv.__contains__() instead\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, entities)\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m             \u001b[1;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 455\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    450\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'Sessualita' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "#model['Elon Musk'] this one is known elon musk is not; consider capitalizing first letters\n",
    "import unicodedata\n",
    "output = unicodedata.normalize('NFD', u\"radioattività\").encode('ascii', 'ignore')\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "\n",
    "def strip_accents(text):\n",
    "    \"\"\"\n",
    "    Strip accents from input String.\n",
    "\n",
    "    :param text: The input string.\n",
    "    :type text: String.\n",
    "\n",
    "    :returns: The processed String.\n",
    "    :rtype: String.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except (TypeError, NameError): # unicode is a default on python 3 \n",
    "        pass\n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    text = text.encode('ascii', 'ignore')\n",
    "    text = text.decode(\"utf-8\")\n",
    "    return str(text)\n",
    "\n",
    "def text_to_id(text):\n",
    "    \"\"\"\n",
    "    Convert input text to id.\n",
    "\n",
    "    :param text: The input string.\n",
    "    :type text: String.\n",
    "\n",
    "    :returns: The processed String.\n",
    "    :rtype: String.\n",
    "    \"\"\"\n",
    "    text = strip_accents(text)\n",
    "    #text = re.sub('[ ]+', '_', text)\n",
    "    #text = re.sub('[^0-9a-zA-Z_-]', '', text)\n",
    "    return text\n",
    "wiki_model[text_to_id(\"Luminosità\").lower()]\n",
    "\n",
    "#text_to_id(\"Montréal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: I found many of the problems I expected, in fact the model hasn't picked up on composed-words, and it mostly knows lower-cased ones; ex: Marte (astronomia), is the name of the wiki page linked to it, but we don't have that token, nor we have a way to map wikipedia pages to single words (like Marte(astronomia) to marte). \n",
    "My solution was to split every word, and create a vector from this by adding them up; if a word is unknown, like (astronomia), the other one is used, and in this case this works pretty well. \n",
    "Words containing accents are still a problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs:  54\n",
      "Length of titles retrieved 54, and of docvecs 54\n",
      "Accuracy (Precision) over each cluster:  [7.4074074074074066, 5.555555555555555, 5.555555555555555, 3.7037037037037033, 18.51851851851852, 16.666666666666664, 5.555555555555555, 7.4074074074074066, 18.51851851851852, 11.11111111111111]\n",
      "Accuracy (Recall) over each cluster:  [100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
      "Precision score: 10.0, Recall score: 100.0\n",
      "#Number of clusters found: 1, against number of pre-computed clusters: 10#\n",
      "\n",
      "Accuracy (Precision) over each cluster:  [83.33333333333333, 51.785714285714285, 100.0, 100.0, 35.714285714285715, 67.85714285714285, 7.142857142857142, 66.66666666666666, 41.07142857142857, 33.92857142857143]\n",
      "Accuracy (Recall) over each cluster:  [50.0, 49.99999999999999, 66.66666666666666, 100.0, 100.0, 25.925925925925924, 66.66666666666666, 50.0, 50.0, 50.0]\n",
      "Precision score: 58.75, Recall score: 60.925925925925924\n",
      "#Number of clusters found: 10, against number of pre-computed clusters: 10#\n",
      "\n",
      "Accuracy (Precision) over each cluster:  [83.33333333333333, 51.785714285714285, 100.0, 100.0, 35.714285714285715, 67.85714285714285, 7.142857142857142, 66.66666666666666, 41.07142857142857, 33.92857142857143]\n",
      "Accuracy (Recall) over each cluster:  [50.0, 49.99999999999999, 66.66666666666666, 100.0, 100.0, 25.925925925925924, 66.66666666666666, 50.0, 50.0, 50.0]\n",
      "Precision score: 58.75, Recall score: 60.925925925925924\n",
      "#Number of clusters found: 11, against number of pre-computed clusters: 10#\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import my_dbscan\n",
    "import model_evaluation as me\n",
    "import w2v_model_utils as mm_utils\n",
    "import numpy as np\n",
    "import importlib\n",
    "importlib.reload(mm_utils)\n",
    "from gensim.utils import simple_preprocess as sp\n",
    "\n",
    "# infer document vectors (once, they won't change)\n",
    "# here's a problem we didn't consider: some docs might not have a representation at all \n",
    "# (all its entities are unknown to the model); we have to keep track of those.\n",
    "my_doc_vecs = []\n",
    "unknown_i = []\n",
    "index2word_set = set(wiki_model.wv.index2word)\n",
    "print(\"Number of docs: \", len(unwrapped_docs))\n",
    "for i, doc in enumerate(unwrapped_docs):\n",
    "    dvec = mm_utils.infer_vector(doc['result_entities'], wiki_model, index2word_set, verbose=False)\n",
    "    if len(dvec) > 0:\n",
    "        my_doc_vecs.append(dvec)\n",
    "    else:\n",
    "        unknown_i.append(i)\n",
    "#print(my_doc_vecs[:2])\n",
    "#wiki_doc_vecs = [mm_utils.infer_vector(doc['result_entities'], wiki_model) for doc in unwrapped_docs]\n",
    "# get titles and urls, ONLY from valid docs\n",
    "titles = [doc['title'] for j, doc in enumerate(unwrapped_docs) if not(j in unknown_i)]\n",
    "urls = [doc['url'] for j, doc in enumerate(unwrapped_docs) if not(j in unknown_i)]\n",
    "assert len(titles) == len(urls)\n",
    "print(\"Length of titles retrieved %s, and of docvecs %s\" %(len(titles), len(my_doc_vecs)))\n",
    "\n",
    "models_recall = []\n",
    "models_precision = []\n",
    "clusters_found = [] # keep clusters results, they're useful later on\n",
    "min_s = 2 # min_samples\n",
    "\n",
    "# let's try 3 combinations of clustering\n",
    "for k in range(3):\n",
    "    # try different eps for some model\n",
    "    if k == 0:\n",
    "        # default eps dbscan\n",
    "        eps = 0.27\n",
    "        eps_increment = 0.1\n",
    "    elif k==1:\n",
    "        eps = 0.11\n",
    "        eps_increment = 0.04\n",
    "    else:\n",
    "        eps = 0.094\n",
    "        eps_increment = 0.04\n",
    "        \"\"\"\n",
    "        # online update of model\n",
    "        print(\"#######Online-training########\")\n",
    "        eps = 0.11\n",
    "        eps_increment = 0.13\n",
    "        #training_corpus = [(doc['result_entities']) for doc in unwrapped_docs]\n",
    "        #train on whole data-set\n",
    "        training_corpus = [ sp(doc['title'], doc['abstract']) for doc in unwrapped_docs]\n",
    "\n",
    "        # Re-training is really long on this model\n",
    "        %time wiki_model = mm_utils.online_training(wiki_model, training_corpus, epochs=7)\n",
    "        # re-infer vectors\n",
    "        my_doc_vecs2 = [mm_utils.infer_vector(doc['result_entities'], wiki_model, index2word_set, verbose=False)\n",
    "                       for doc in unwrapped_docs]\n",
    "        # missing new titles here\n",
    "        \"\"\"\n",
    "    doc_vecs = my_doc_vecs\n",
    "    #if k==2:\n",
    "    #    doc_vecs = my_doc_vecs2\n",
    "    # TODO: change to 2 to cluster both models\n",
    "    for i in range(1):\n",
    "        # apply dbscan clustering to these vectors\n",
    "        urls_cluster_list = my_dbscan.apply_dbscan(doc_vecs = doc_vecs, titles = titles, \n",
    "                                                       urls = urls, subset_length = len(doc_vecs),\n",
    "                                                     eps = eps, eps_increment = eps_increment,\n",
    "                                                   n_iterations = 3, verbose = False, min_samples = min_s)\n",
    "        # get clusters as list of titles\n",
    "        titles_clusters = utils.getDocTitleFromUrl(unwrapped_docs, urls_cluster_list)\n",
    "        clusters_found.append(titles_clusters)\n",
    "        #data = utils.plot_clusters(titles_clusters)\n",
    "\n",
    "        # evaluate clustering\n",
    "        expected_clusters = []\n",
    "        for docs in cdocs:\n",
    "            expected_clusters.append([doc['title'] for doc in docs])\n",
    "        precision, recall = me.compute_clustering_accuracy(titles_clusters, expected_clusters)\n",
    "        print('Precision score: %s, Recall score: %s'%(precision, recall))\n",
    "\n",
    "        models_precision.append(precision)\n",
    "        models_recall.append(recall)\n",
    "        # last check: we want to penalize models that simply cluster all docs together (that's not a valid result)\n",
    "        # that's way we have precision score\n",
    "        print(\"#Number of clusters found: {0}, against number of pre-computed clusters: {1}#\\n\".format(\n",
    "            len(urls_cluster_list), len(cdocs)))\n",
    "        # change inferred docs for next model\n",
    "        #doc_vecs = wiki_doc_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~D4nt3_/18.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "# using my api-key\n",
    "tls.set_credentials_file(username='D4nt3_', api_key='4O71urldgOueVtcApOdX')\n",
    "\n",
    "title_dist_tuples = utils.choose_eps(2, my_doc_vecs, titles)\n",
    "trace = go.Scatter(\n",
    "    x =[x for (x, y) in title_dist_tuples],  # list of x\n",
    "    y = [y for (x, y) in title_dist_tuples],\n",
    "    mode = 'lines',\n",
    "    name = 'lines'\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "py.iplot(data, filename = 'eps-wiki-model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Marte, il sottosuolo può avere ospitato la vita - Spazio & Astronomia - ANSA.it', 'NASA, dalla CO2 al glucosio su Marte']\n",
      "['Decolla Facebook Dating, il Tinder di Menlo Park: primi test in Colombia - Repubblica.it', \"iPhone Xs Max tira 3-4 volte più dell'Xs - Hi-tech - ANSA.it\", 'Apple conferma \"per errore\" iPhone XS, XS Max e XR', 'Aspettando iPhone Xs: il giorno di iOS 12', 'iPhone XS: perché Apple ha nascosto il notch?', 'Apple iPhone Xs e Xs Max: è troppo caro?', 'Apple lancia iPhone Xs e la versione Max: sempre più grandi, gli smartphone sono la nuova Tv - Corriere.it', 'Problemi per iPhone XS e XS Max: «Non si caricano se il cavo è collegato mentre lo schermo è spento» - Corriere.it', 'iPhone XS e XS Max: novità, scheda tecnica e prezzo', 'iPhone XS appiana le rughe, protestano gli utenti', 'iPhone XS: proteste per la carica troppo lenta', 'FIFA 19, ecco i requisiti PC', 'YouTube Kids, lo streaming dei bambini in Italia', 'Social e bambini: YouTube assume nuovi moderatori e Facebook lancia Messenger Kids - Corriere.it', 'Samsung lancia il suo primo smartphone con tre fotocamere - La Stampa', 'Samsung, i nuovi Galaxy J6+ e J4+ - Tlc - ANSA.it', 'SmartThings: Samsung presenta un Tracker LTE', 'Samsung, in arrivo uno smartphone con quattro fotocamere (e il primo con schermo pieghevole) - Corriere.it', 'Samsung Galaxy Note 9: la nuova S Pen', 'Notizie Samsung Galaxy Note 9', \"Samsung Bixby 2.0 supporterà anche l'italiano\", 'Galaxy Note 9, Samsung regala una Micro SD', 'Samsung Galaxy Note 9', \"Huawei Mate 20 Lite: un midrange dall'ottima autonomia. La recensione\", 'Huawei P Smart Plus: per lanciarlo alleanza tra i cinesi e Amazon. Con Emis Killa come testimonial - Corriere.it', 'Huawei P20 Pro: 3 fotocamere | Arrivano anche P20 e P20 Lite - Corriere.it', 'Huawei MateBook X Pro', 'Huawei Mate 20 lite']\n",
      "['Elon Musk denuciato per truffa, Tesla crolla in borsa - Wired', 'Tesla, Elon Musk lascia la presidenza']\n",
      "[\"L'equinozio d'autunno non è il 21 settembre: quest'anno arriva il 23 - Repubblica.it\", \"E' l'equinozio d'autunno - Spazio & Astronomia - ANSA.it\"]\n",
      "['FIFA 19 sui campi della Champions League', 'FIFA 19, annunciata la disponibilità della demo', \"FIFA 19: L'ora dei campioni, trailer di lancio\", 'FIFA 19: novità, uscita e prezzo']\n",
      "['A spasso attorno alla Luna, SpaceX annuncia il primo turista spaziale - Corriere.it', 'SpaceX, i viaggi sulla Luna possono attendere', 'SpaceX porterà un uomo in orbita attorno alla Luna']\n",
      "['LetsApp, così Samsung fa avvicinare gli studenti al digitale - Corriere.it', 'Huawei: recensioni e novità']\n",
      "['SpaceX: ecco come saranno le basi umane su Marte', 'Elon Musk contro la NASA per terraformare Marte', 'SpaceX manderà Yusaku Maezawa sulla Luna']\n",
      "['Facebook: Tinder nel mirino, novità di Instagram e Whatsapp - Corriere.it', 'Facebook Dating: ecco come funziona l’anti-Tinder di Zuckerberg - Corriere.it']\n",
      "['Fifa 19 contro Pes 2019: qual è il migliore quest’anno? Ecco la sfida giocata da noi - Corriere.it', 'I miglioramenti di FIFA 19 per Nintendo Switch']\n"
     ]
    }
   ],
   "source": [
    "for cluster in clusters_found[1]:\n",
    "    print(cluster)\n",
    "#print(unwrapped_docs[29])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize results graphically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~D4nt3_/22.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "# using my api-key\n",
    "tls.set_credentials_file(username='D4nt3_', api_key='4O71urldgOueVtcApOdX')\n",
    "graph_name = 'model_eval_entities_results'\n",
    "\n",
    "model_descr = [\"DBSCAN Eps=0.11,incr=.04\", \"K-means with K=10\"]\n",
    "# plot test-accuracy results, this time input values directly (messed up with data structure naming)\n",
    "# Precision score: 58.75, Recall score: 60.925925925925924\n",
    "#Number of clusters found: 10, against number of pre-computed clusters: 10#\n",
    "# Precision score: 54.871212121212125, Recall score: 67.5\n",
    "trace0 = go.Bar(\n",
    "    x = model_descr,\n",
    "    y = [58.75, 54.871212121212125],\n",
    "    name='Precision',\n",
    "    marker=dict(\n",
    "        color='rgb(49,130,189)'\n",
    "    ),\n",
    "    width = [0.2, 0.2]\n",
    ")\n",
    "\n",
    "trace2 = go.Bar(\n",
    "    x = model_descr,\n",
    "    y = [60.925925925925924, 67.5],\n",
    "    name='Recall',\n",
    "    marker=dict(\n",
    "        color='rgb(155, 244, 66)',\n",
    "    ),\n",
    "    width = [0.2, 0.2]\n",
    "    \n",
    ")\n",
    "# precision, recall\n",
    "data = [trace0, trace2]\n",
    "layout = go.Layout(\n",
    "    title = 'Clustering Accuracy Results (set-1)',\n",
    "    xaxis=dict(\n",
    "        tickfont=dict(\n",
    "            size=13,\n",
    "            color='rgb(107, 107, 107)',\n",
    "            \n",
    "        ),\n",
    "        tickangle = 0\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Clustering accuracy (%)',\n",
    "        titlefont=dict(\n",
    "            size=16,\n",
    "            color='rgb(107, 107, 107)'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            size=14,\n",
    "            color='rgb(107, 107, 107)'\n",
    "        )\n",
    "    ),\n",
    "   \n",
    "    barmode='group',\n",
    "    bargap=0.2,\n",
    "    bargroupgap=0.02\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename=graph_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs:  54\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~D4nt3_/20.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "#titles = [doc['headline'] for doc in unwrapped_docs]\n",
    "#titles = [doc['title'] for doc in unwrapped_docs]\n",
    "# let's try Kmeans with K between 2 and 20\n",
    "model = wiki_model\n",
    "def run_KMeans_K_times(doc_vecs, K):\n",
    "    sse = []\n",
    "    kmeans_results = []\n",
    "    # run k means with k between 1 and K\n",
    "    for i in range(1, K+1):\n",
    "        kmeans = KMeans(n_clusters=i, init='k-means++', # smart-way of choosing starting point\n",
    "               n_init= 50, # run k-means 100 times and get the best result in terms of SSE\n",
    "                n_jobs = -1 # use every core\n",
    "        )\n",
    "        # compute sse for this kmeans result\n",
    "        kmeans.fit(doc_vecs)\n",
    "        sse.append(kmeans.inertia_)\n",
    "        \n",
    "        # get 'visual' clustering, by saving a list of titles\n",
    "        clusters = {label: [] for label in kmeans.labels_}\n",
    "        for label, list_ in clusters.items():\n",
    "            for i, l in enumerate(kmeans.labels_):\n",
    "                if label==l:\n",
    "                    clusters[label].append(titles[i])\n",
    "        \n",
    "        kmeans_results.append(clusters)\n",
    "    return kmeans_results, sse\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "# using my api-key\n",
    "tls.set_credentials_file(username='D4nt3_', api_key='4O71urldgOueVtcApOdX')\n",
    "graph_name = 'kmeans_choosing_k_WIKI'\n",
    "my_doc_vecs = []\n",
    "unknown_i = []\n",
    "index2word_set = set(model.wv.index2word)\n",
    "print(\"Number of docs: \", len(unwrapped_docs))\n",
    "for i, doc in enumerate(unwrapped_docs):\n",
    "    dvec = mm_utils.infer_vector(doc['result_entities'], wiki_model, index2word_set, verbose=False)\n",
    "    if len(dvec) > 0:\n",
    "        my_doc_vecs.append(dvec)\n",
    "    else:\n",
    "        unknown_i.append(i)\n",
    "        \n",
    "# get titles and urls, ONLY from valid docs\n",
    "titles = [doc['title'] for j, doc in enumerate(unwrapped_docs) if not(j in unknown_i)]\n",
    "## kmeans\n",
    "kmeans_results, sse = run_KMeans_K_times(my_doc_vecs, 23)\n",
    "K = 23 # max_k\n",
    "# plot results of K and SSE\n",
    "trace = go.Scatter(\n",
    "    x = [k for k in range(1, K+1)],\n",
    "    y = sse,\n",
    "    mode = 'lines+markers'\n",
    ")\n",
    "layout = dict(title = 'Choosing K in Kmeans',\n",
    "              xaxis = dict(title = 'K value'),\n",
    "              yaxis = dict(title = 'SSE'),\n",
    "              )\n",
    "\n",
    "data = [trace]\n",
    "fig = dict(data=data, layout=layout)\n",
    "py.iplot(fig, filename = graph_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Precision) over each cluster:  [50.0, 100.0, 27.27272727272727, 100.0, 49.31818181818182, 40.909090909090914, 53.03030303030303, 50.0, 45.45454545454546, 32.72727272727273]\n",
      "Accuracy (Recall) over each cluster:  [100.0, 100.0, 100.0, 100.0, 25.0, 50.0, 33.33333333333333, 100.0, 33.333333333333336, 33.333333333333336]\n",
      "Precision score: 54.871212121212125, Recall score: 67.5\n",
      "#Number of clusters found: 10, against number of pre-computed clusters: 10#\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import my_dbscan\n",
    "import model_evaluation as me\n",
    "\n",
    "clusters = []\n",
    "models_precision_kmeans = []\n",
    "models_recall_kmeans = []\n",
    "expected_clusters = []\n",
    "for docs in cdocs:\n",
    "    expected_clusters.append([doc['title'] for doc in docs])\n",
    "    #expected_clusters.append([doc['headline'] for doc in docs])\n",
    "        \n",
    "\n",
    "# K already chosen before, with value 4\n",
    "cluster_results, sse = run_KMeans_K_times(my_doc_vecs, 10)\n",
    "#cluster_results, sse = run_KMeans_K_times(model, 10)\n",
    "cluster_results = cluster_results[-1] # get 4-means results\n",
    "clist = []\n",
    "for key, cluster in cluster_results.items():\n",
    "    clist.append(cluster)\n",
    "clusters.append(clist)\n",
    "    \n",
    "# evaluate clustering\n",
    "precision, recall = me.compute_clustering_accuracy(clist, expected_clusters)\n",
    "print('Precision score: %s, Recall score: %s'%(precision, recall))\n",
    "\n",
    "models_precision_kmeans.append(precision)\n",
    "models_recall_kmeans.append(recall)\n",
    "# last check: we want to penalize models that simply cluster all docs together (that's not a valid result)\n",
    "# that's way we have precision score\n",
    "print(\"#Number of clusters found: {0}, against number of pre-computed clusters: {1}#\\n\".format(\n",
    "    len(clist), len(cdocs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Marte, il sottosuolo può avere ospitato la vita - Spazio & Astronomia - ANSA.it', 'SpaceX: ecco come saranno le basi umane su Marte', 'NASA, dalla CO2 al glucosio su Marte', 'Elon Musk contro la NASA per terraformare Marte', 'A spasso attorno alla Luna, SpaceX annuncia il primo turista spaziale - Corriere.it', 'SpaceX, i viaggi sulla Luna possono attendere', 'SpaceX porterà un uomo in orbita attorno alla Luna', 'SpaceX manderà Yusaku Maezawa sulla Luna'], ['Decolla Facebook Dating, il Tinder di Menlo Park: primi test in Colombia - Repubblica.it', 'Facebook: Tinder nel mirino, novità di Instagram e Whatsapp - Corriere.it', 'Facebook Dating: ecco come funziona l’anti-Tinder di Zuckerberg - Corriere.it'], ['Tesla sotto indagine per colpa dei tweet di Elon Musk: crollo in Borsa - Corriere.it', 'Elon Musk denuciato per truffa, Tesla crolla in borsa - Wired', 'Tesla, Elon Musk lascia la presidenza', 'Fifa 19 contro Pes 2019: qual è il migliore quest’anno? Ecco la sfida giocata da noi - Corriere.it', 'Fifa 19, la prova in anteprima - Corriere.it', 'FIFA 19 sui campi della Champions League', 'FIFA 19, annunciata la disponibilità della demo', 'I miglioramenti di FIFA 19 per Nintendo Switch', \"FIFA 19: L'ora dei campioni, trailer di lancio\", 'FIFA 19: novità, uscita e prezzo', \"FIFA 19 si mostra nel trailer L'Ora dei Campioni\"], [\"L'equinozio d'autunno non è il 21 settembre: quest'anno arriva il 23 - Repubblica.it\", \"E' l'equinozio d'autunno - Spazio & Astronomia - ANSA.it\"], [\"iPhone Xs Max tira 3-4 volte più dell'Xs - Hi-tech - ANSA.it\", 'iPhone XS: perché Apple ha nascosto il notch?', 'Apple lancia iPhone Xs e la versione Max: sempre più grandi, gli smartphone sono la nuova Tv - Corriere.it', 'iPhone XS appiana le rughe, protestano gli utenti'], ['Apple conferma \"per errore\" iPhone XS, XS Max e XR', 'iPhone XS: proteste per la carica troppo lenta', 'Samsung lancia il suo primo smartphone con tre fotocamere - La Stampa', 'Samsung, in arrivo uno smartphone con quattro fotocamere (e il primo con schermo pieghevole) - Corriere.it', 'Samsung Galaxy Note 9: la nuova S Pen', 'Notizie Samsung Galaxy Note 9', 'Galaxy Note 9, Samsung regala una Micro SD', \"Huawei Mate 20 Lite: un midrange dall'ottima autonomia. La recensione\", 'Huawei P Smart Plus: per lanciarlo alleanza tra i cinesi e Amazon. Con Emis Killa come testimonial - Corriere.it', 'Huawei P20 Pro: 3 fotocamere | Arrivano anche P20 e P20 Lite - Corriere.it'], ['Aspettando iPhone Xs: il giorno di iOS 12', 'Apple iPhone Xs e Xs Max: è troppo caro?', 'iPhone XS e XS Max: novità, scheda tecnica e prezzo', 'FIFA 19, ecco i requisiti PC', 'YouTube Kids, lo streaming dei bambini in Italia', 'Samsung, i nuovi Galaxy J6+ e J4+ - Tlc - ANSA.it', 'SmartThings: Samsung presenta un Tracker LTE', \"Samsung Bixby 2.0 supporterà anche l'italiano\", 'Samsung Galaxy Note 9', 'Huawei MateBook X Pro', 'Huawei Mate 20 lite'], ['Problemi per iPhone XS e XS Max: «Non si caricano se il cavo è collegato mentre lo schermo è spento» - Corriere.it', 'Social e bambini: YouTube assume nuovi moderatori e Facebook lancia Messenger Kids - Corriere.it'], ['YouTube e i video con bambini «abusati» Google sotto accusa, ritirata la pubblicità - Corriere.it'], ['LetsApp, così Samsung fa avvicinare gli studenti al digitale - Corriere.it', 'Huawei: recensioni e novità']]\n"
     ]
    }
   ],
   "source": [
    "print(clist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silhoutte score\n",
    "Silhouette coefficient combines ideas of both cohesion and separation, \n",
    "but for individual points, as well as clusters and \n",
    "clusterings; the Silhouette Coefficient is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample.\n",
    "The Silhoutte score is merely the average of each silhoutte coefficient, computed over each sample.\n",
    "It's a measure of the goodness of clustering, by assuming the fact that a cluster X is defined good if both every sample inside it is close to each other, and far from any other relatively-near cluster Y.\n",
    "This is not necessarily true for every shape of cluster.\n",
    "\n",
    "    The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2057951\n",
      "0.23791973\n",
      "0.24932371\n",
      "0.24040304\n",
      "0.13758796\n",
      "0.0041218703\n",
      "0.12820731\n",
      "0.228402\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "s_scores = []\n",
    "for model in models:\n",
    "    # get clusters for each model\n",
    "    doc_vecs = [model.docvecs[j] for j in range(len(model.docvecs))]\n",
    "    titles = [doc['title'] for doc in unwrapped_docs]\n",
    "    urls = [doc['url'] for doc in unwrapped_docs]\n",
    "    # apply dbscan clustering to these vectors\n",
    "    urls_cluster_list = my_dbscan.apply_dbscan(doc_vecs = doc_vecs, titles = titles, \n",
    "                                               urls = urls, subset_length = len(titles),\n",
    "                                                 eps = 0.27, eps_increment = 0.1, n_iterations = 3, verbose = False)\n",
    "    \n",
    "    # get cluster labels, mantaining original docs ordering\n",
    "    labels = []\n",
    "    for doc in unwrapped_docs:\n",
    "        noise = True\n",
    "        for i, url_list in enumerate(urls_cluster_list):\n",
    "            if(doc['url'] in url_list):\n",
    "                labels.append(i) # keep cluster id\n",
    "                noise = False\n",
    "        if noise:\n",
    "            labels.append(-1)\n",
    "    # make sure they have the same size\n",
    "    assert len(labels) == len(doc_vecs)\n",
    "    ss = silhouette_score(doc_vecs, labels , metric='cosine')\n",
    "    s_scores.append(ss)\n",
    "    print(ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions: \n",
    "A really low silhoutte score helps us identify models that tend to have very few cluster, hence not really recognizing differences between docs. This is fundamental, since test rules used so far prevent us from recognizing these kinds of models.\n",
    "Since silhoutte score is an average of silhoutte coefficients, smaller changes of values here may mean greater differences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
