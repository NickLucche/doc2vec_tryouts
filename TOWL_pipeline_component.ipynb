{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True # change this is if you don't need to display print()/log in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed libraries\n",
    "import json\n",
    "import random\n",
    "#import numpy as np\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import  gensim\n",
    "from collections import Counter\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load documents from JSON\n",
    "change this block to load from a pre-defined filename of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents:  96\n",
      "Number of documents:  604\n"
     ]
    }
   ],
   "source": [
    "filenames = ['blockchain.json', 'industria_4.0.json']\n",
    "\n",
    "# load multiple files, assuming same data format\n",
    "docs = []\n",
    "for filename in filenames:\n",
    "    with open(filename, 'r') as outfile:\n",
    "        json_data = json.load(outfile)\n",
    "\n",
    "    ## let's now retrieve the meaningful part of the json document\n",
    "    # response{}--->docs[] \n",
    "    ## -that's the way I was given JSON docs so far, change this part if format changes-\n",
    "\n",
    "    docs = docs + json_data['response']['docs']\n",
    "    if verbose:\n",
    "        print(\"Number of documents: \",len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This part can be ignored if we assume data is \"clean\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New length after removing docs:  412\n"
     ]
    }
   ],
   "source": [
    "## many documents have a failed abstract, let's remove them\n",
    "to_check = ' Questo sito web utilizza cookie tecnici e, previo Suo consenso, cookie di profilazione,'\n",
    "docs = [doc for i, doc in enumerate(docs) if not(to_check.strip() in doc['abstract'][0].strip())]\n",
    "\n",
    "if verbose:\n",
    "    print(\"New length after removing docs: \", len(docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Length:  362\n",
      "['L’annuncio è arrivato alla London Fintech News ed è stato rilanciato dalla testata The Fintech Times: il primo comunicatore crypto in versione Mobile per la Blockchain che con qualche forzatura si può...']\n"
     ]
    }
   ],
   "source": [
    "# List->String\n",
    "## Adjust data format: title, abstract and url came in as list, but they're more useful as strings\n",
    "for i, dictionary in enumerate(docs):\n",
    "    for field in ['title', 'abstract', 'url']:\n",
    "        if isinstance(dictionary[field], list):\n",
    "            # re-format data to hold string instead of single-list item\n",
    "            docs[i][field] = dictionary[field][0]\n",
    "            \n",
    "# remove duplicates (of a particular doc)\n",
    "# TODO: remove all duplicates\n",
    "docs = [doc for doc in docs\n",
    "            if not(\"Industry 4.0 (o industria 4.0): cos'è, notizie, normative, casi studio - I4T\" in doc['title'])]\n",
    "\n",
    "if verbose:\n",
    "    print(\"New Length: \", len(docs))\n",
    "\n",
    "# double check to be sure\n",
    "for doc in docs:\n",
    "    if to_check.strip() in doc['abstract'].strip():\n",
    "        print(\"cookie doc found\")\n",
    "if verbose:\n",
    "    print([d['abstract'][:200]+'...' for d in docs[:1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer Vectors from documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and prepara data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9031\n",
      "362\n",
      "['\"Polizza & IoT\", la nuova abbinata delle assicurazioni: settore auto in pole position - CorCom', 'blockchain e digital transformation nella PA: focus su standard e governance - Blockchain 4innovation', 'Infocert, la ricerca sull’identità digitale punta sulla blockchain - Blockchain 4innovation', \"Tech Data entra nell'alleanza globale per l'IoT\", 'Industria 4.0, così finisce il \"diritto pesante\" del lavoro - CorCom']\n",
      "['https://www.corrierecomunicazioni.it/industria-4-0/iot/polizza-iot-la-nuova-abbinata-delle-assicurazioni-settore-auto-in-pole-position/', 'https://www.blockchain4innovation.it/mercati/pubblica-amministrazione/blockchain-e-digital-transformation-nella-pa-focus-su-standard-e-governance/', 'https://www.blockchain4innovation.it/mercati/industria4-0/infocert-la-ricerca-sullidentita-digitale-punta-sulla-blockchain/', 'https://www.internet4things.it/industry-4-0/m2m/tech-data/', 'https://www.corrierecomunicazioni.it/industria-4-0/industria-40-cosi-finisce-il-diritto-pesante-del-lavoro/']\n"
     ]
    }
   ],
   "source": [
    "# shuffle documents\n",
    "random.shuffle(docs)\n",
    "\n",
    "## !Change this if you want to rename model or change dir in the filesystem ##\n",
    "MODEL_NAME = 'TestModels/d2v_TA_abstract&title0.model'\n",
    "model = Doc2Vec.load(MODEL_NAME)\n",
    "\n",
    "# print out dimension of the vocabulary of the model (number of known words)\n",
    "if verbose:\n",
    "    print(len(model.wv.vocab))\n",
    "    \n",
    "# infer vectors from data\n",
    "# preprocess data first (remove capitals, strange unicode chars..)\n",
    "## title + abstract may change in future versions to flattened_entities, with a newer model!\n",
    "test_corpus = [gensim.utils.simple_preprocess(d['title']+d['abstract']) for d in docs]\n",
    "if verbose: \n",
    "    print(len(test_corpus))\n",
    "    \n",
    "# list of vectors of docs\n",
    "inferred_vectors = [model.infer_vector(doc) for doc in test_corpus]\n",
    "\n",
    "# get docs titles, needed to return results correctly after clustering\n",
    "titles = [doc['title'] for doc in docs]\n",
    "# same thing for urls\n",
    "urls = [doc['url'] for doc in docs]\n",
    "if verbose: \n",
    "    print(titles[:5])\n",
    "    print(urls[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN \n",
    "density-based algorithm used to clusterize docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my function for performing dbscan and printing out cluster results\n",
    "def perform_dbscan(eps = 0.4, min_samples = 4, metric = 'euclidean', algorithm = 'auto', data = None, verbose = True\n",
    "                  , titles = None, urls = None, print_noise = True):\n",
    "    \"\"\"perform DBSCAN over given data, using given parametrs. Returns dbscan object and clusters dictionary.\"\"\"\n",
    "    \n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples, metric=metric, algorithm=algorithm).fit(data)\n",
    "\n",
    "    # labels will print out the number of the cluster each example belongs to;\n",
    "    # -1 if the vector is considered noise (not belonging to any cluster)\n",
    "    #print(\"Labels: \", db.labels_)\n",
    "\n",
    "    # create data structure containing clusters\n",
    "    clusters_to_ret = {label:[] for label in db.labels_ if label!=-1}\n",
    "    \n",
    "    for i, label in enumerate(db.labels_):\n",
    "        if label != -1: #ignore noise points\n",
    "            clusters_to_ret[label].append(urls[i])\n",
    "        \n",
    "    \n",
    "    \n",
    "    # only do this if you need to print out the result (messy for large number of docs)\n",
    "    if verbose:\n",
    "        print(\"##Clusters##\")\n",
    "        clusters = {label: [] for label in db.labels_ if label!=-1}\n",
    "        noise = []\n",
    "        for i, label in enumerate(db.labels_):\n",
    "            if label != -1: \n",
    "                clusters[label].append(titles[i])\n",
    "            else: # save noise points\n",
    "                noise.append(titles[i])\n",
    "                \n",
    "        for label, list_ in clusters.items():\n",
    "            print(\"Cluster: {}\".format(list_))\n",
    "        if print_noise:\n",
    "            print(\"Noise: \", noise)\n",
    "\n",
    "        print(\"DBSCAN finished.\\n\")\n",
    "    return db, clusters_to_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative/incremental DBSCAN \n",
    "that's the way I thought was best to use DBSCAN in our case\n",
    "TODO: heuristic to find out how many times to apply DBSCAN, right now is only based on eps size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply DBSCAN to SUBSET, change here to apply to 'docs' instead of 'subset' to clusterize all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Clusters##\n",
      "Cluster: ['blockchain e digital transformation nella PA: focus su standard e governance - Blockchain 4innovation', 'Guida completa al Cloud computing: costi, implementazione, compliance e ROI', \"Che cos'è TrustedChain e perché può cambiare la logica di gestione delle transazioni grazie alla Blockchain - Blockchain 4innovation\"]\n",
      "Cluster: ['Industria 4.0, il futuro del lavoro passa da formazione e governance - CorCom', 'Industria 4.0, Di Maio: \"Incentivi confermati\". Meno tasse per chi assume - CorCom', 'Industria 4.0 in crescita del 30%, Italia pronta alla svolta? - CorCom', 'Dall’IoT alla Servitizzazione. HBR: Gli Smart connected product cambiano la competizione - Industry4Business', 'AI servizi il 34 per cento del mercato IoT italiano - Industry4Business', 'Lavoro 4.0, Falciasecca: \"Nuovo patto accademia-impresa\" - CorCom']\n",
      "DBSCAN finished.\n",
      "\n",
      "##Clusters##\n",
      "Cluster: ['\"Polizza & IoT\", la nuova abbinata delle assicurazioni: settore auto in pole position - CorCom', 'Apre al Politecnico di Torino un nuovo centro di compentenza sull’Industria 4.0']\n",
      "DBSCAN finished.\n",
      "\n",
      "##Clusters##\n",
      "Cluster: ['Infocert, la ricerca sull’identità digitale punta sulla blockchain - Blockchain 4innovation', 'Industria 4.0, così finisce il \"diritto pesante\" del lavoro - CorCom', 'Service Strategy: a settembre un evento a Milano - Industry4Business', 'Industria 4.0, Purassanta: \"Informatica leva di sostenibilità\" - CorCom', 'MecSpe 2018, in mostra a Parma la via italiana a Industria 4.0', 'Industria 4.0, La Rosa (Samsung): \"Dal Governo spinta all\\'eccellenza\" - CorCom', 'Contro lo scetticismo sulle donazioni alle ONG, la soluzione arriva dalla Blockchain - Blockchain 4innovation']\n",
      "Noise:  [\"Tech Data entra nell'alleanza globale per l'IoT\", 'Dai videogame all’industria 4.0: Ford testa il body tracking - CorCom']\n",
      "DBSCAN finished.\n",
      "\n",
      "Number of cluster found:  4\n",
      "Length of cluster 0: 3\n",
      "Length of cluster 1: 6\n",
      "Length of cluster 2: 2\n",
      "Length of cluster 3: 7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "subset_length = 20\n",
    "# subset of docs vectors \n",
    "subset = inferred_vectors[:subset_length]\n",
    "subset_titles = titles[:subset_length]\n",
    "sub_urls = urls[:subset_length]\n",
    "\n",
    "eps = 0.27\n",
    "eps_increment = 0.1\n",
    "noise_bool = False\n",
    "# this will contain all clusters found, each one as a list, \n",
    "# mantaining the order dbscan returned (first clusters will contain articles more related to each other)\n",
    "final_clusters = []\n",
    "# starting eps will be the sum of eps + eps_increment \n",
    "for i in range(3):\n",
    "    if i==2: \n",
    "        noise_bool = True\n",
    "    eps = eps + eps_increment\n",
    "   \n",
    "    db, clusters = perform_dbscan(eps = eps, min_samples = 2, metric = 'cosine', algorithm = 'auto',\n",
    "                        data = subset, verbose = True, titles = subset_titles, urls = sub_urls, print_noise = noise_bool)\n",
    "\n",
    "    # TODO: ignore noise/'other' documents or return them?\n",
    "    for label, list_ in clusters.items():\n",
    "        final_clusters.append(list_)\n",
    "        \n",
    "    # let's try and find other clusters in the noise data, with higher eps\n",
    "    subset = [subset[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "    subset_titles = [subset_titles[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "    sub_urls = [sub_urls[i] for i, label in enumerate(db.labels_) if label==-1]\n",
    "    if subset is None:\n",
    "        break\n",
    "\n",
    "if verbose:\n",
    "    print(\"Number of cluster found: \", len(final_clusters))\n",
    "    for i, cluster in enumerate(final_clusters):\n",
    "        print(\"Length of cluster {0}: {1}\".format(i, len(cluster)))\n",
    "# final clusters composition:\n",
    "#[[cluster0_urls], [cluster1_urls], ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Save clusters to JSON using agreed format and Cluster entities as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ta_id: [152109]\n",
      "urls first cluster:['https://www.blockchain4innovation.it/mercati/pubblica-amministrazione/blockchain-e-digital-transformation-nella-pa-focus-su-standard-e-governance/', 'https://www.internet4things.it/industry-4-0/guida-completa-al-cloud-computing-costi-implementazione-compliance-roi/'] .. \n",
      "Second cluster main entities:  [('Lavoro', 5), ('Internet delle cose', 5), ('Italia', 5), ('Servizio', 5)]\n"
     ]
    }
   ],
   "source": [
    "#- sorgente_dati (vale sempre “cluster”)\n",
    "#- ta_id\n",
    "print('ta_id:', docs[0]['ta_id'])\n",
    "#- data inizio ?\n",
    "#- data fine ?\n",
    "#- documents (lista degli url)\n",
    "print('urls first cluster:{} .. '.format( final_clusters[0][:2]))\n",
    "#- entities (lista di coppie “nome entity”, numero di occorrenze)\n",
    "cluster1_docs = [doc for doc in docs if doc['url'] in final_clusters[1]]\n",
    "\n",
    "print(\"Second cluster main entities: \", getClusterEntites(cluster1_docs, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportClusterResults():\n",
    "    \"\"\"This method will save the result of the clustering operation to disk, as a json file,\n",
    "    using the agreed format: core results are the arrays 'documents':[__, __] containing the urls of each \n",
    "    document in a cluster, and 'entities':[{}, {}] \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Cluster Entities\n",
    "Each cluster will be represented by a few meaningful entities, which summarize the cluster: \n",
    "these entities are chosen based on the most 'popular' among the documents which form a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# efficient way of getting most common elements in a list (O(n))\n",
    "def mostCommons(lst, n):\n",
    "    \"\"\"given a list, returns the n most common elements; in case of ties, it may not return the first occurence. \"\"\"\n",
    "    data = Counter(lst)\n",
    "    item_count_list = data.most_common(n)\n",
    "\n",
    "    return item_count_list\n",
    "\n",
    "def getClusterEntites(cluster_docs = None, n_entities = 3):\n",
    "    \"\"\"given all documents belonging to a cluster (as a list of dictionaries, each dictionary \n",
    "    representing a doc with its attributes), returns the most common 'n_entities' in the cluster.\n",
    "    \"\"\"\n",
    "    \n",
    "    # get list of flattened_entities from documents\n",
    "    entities_field_name = 'result_entities'\n",
    "    # we're expecting flattened_entities as a list of strings\n",
    "    f_entities = [entity for doc in cluster_docs for entity in doc[entities_field_name]]\n",
    "    \n",
    "    # get the 'n_entities' most 'frequent' entity in the cluster\n",
    "    return mostCommons(f_entities, n_entities)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tecnologia', 241),\n",
       " ('Azienda', 205),\n",
       " ('Industria 4.0', 190),\n",
       " ('Produzione', 156)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to use getClusterEntities, example:\n",
    "\n",
    "## convert flattened_entites from string to list of strings\n",
    "for doc in docs:\n",
    "    if isinstance(doc['result_entities'], str):\n",
    "        doc['flattened_entities'] = doc['flattened_entities'].split()\n",
    "getClusterEntites(docs, 4) # print out 4 - most common in whole dataset\n",
    "\n",
    "# more realistic case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
